{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "okiGyk63qa2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2251,
     "status": "ok",
     "timestamp": 1755533903841,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "okiGyk63qa2d",
    "outputId": "e3499313-7d78-4aa5-fff0-65086481002b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/ChiLit_Topic_Modeling\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/ChiLit_Topic_Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jj4_qB8CqxGm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12948,
     "status": "ok",
     "timestamp": 1755533919947,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "jj4_qB8CqxGm",
    "outputId": "e28bc808-58b2-4891-c608-f6a1be644036"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tonazzog/OCTIS.git\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbde3299-672a-4cd5-99fa-d01a99b32270",
   "metadata": {
    "id": "bbde3299-672a-4cd5-99fa-d01a99b32270"
   },
   "outputs": [],
   "source": [
    "import octis\n",
    "from octis.models.LDA import LDA\n",
    "from octis.models.ProdLDA import ProdLDA\n",
    "from octis.models.ETM import ETM\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real\n",
    "from skopt.space.space import Integer\n",
    "from skopt.space.space import Categorical\n",
    "import optuna\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.io import show\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b9f45-a4d7-4906-9d44-b4fe6d2f76cd",
   "metadata": {
    "id": "FKVQr53xrY_3"
   },
   "source": [
    "Note: OCTIS extension for multi-objective optimization doesn't work. OPTUNA was used for multi-objective optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "i8iugN8Trnp8",
   "metadata": {
    "id": "i8iugN8Trnp8"
   },
   "outputs": [],
   "source": [
    "chunk_size = 200\n",
    "octis_folder = f\"./octis_{chunk_size}/\"\n",
    "optuna_folder = f\"./optuna_{chunk_size}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f36b5d-4bee-4909-b630-bc3ebb8f9bcf",
   "metadata": {
    "id": "28f36b5d-4bee-4909-b630-bc3ebb8f9bcf"
   },
   "source": [
    "### Create OCTIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cdb5d3a-ee2a-436d-b8a7-04bd392bdeb6",
   "metadata": {
    "id": "6cdb5d3a-ee2a-436d-b8a7-04bd392bdeb6"
   },
   "outputs": [],
   "source": [
    "def prepare_octis_corpus(output_folder, docs):\n",
    "  # Write to docs.tsv\n",
    "  with open(os.path.join(output_folder, \"corpus.tsv\"), \"w\", encoding=\"utf-8\") as f:\n",
    "      for doc in docs:\n",
    "          f.write(f\"{doc}\\n\")\n",
    "  # Tokenize and create vocabulary\n",
    "  vocab = set()\n",
    "  for doc in docs:\n",
    "      vocab.update(doc.split())\n",
    "\n",
    "  vocab = {w for w in vocab if w.isalpha() and len(w) > 2}\n",
    "  vocab = sorted(vocab)  # sorting is optional but nice for consistency\n",
    "\n",
    "  # Save vocab.json\n",
    "  with open(os.path.join(output_folder, \"vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "      json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a7c3cd-3ad8-45fc-a9f0-a6c968963703",
   "metadata": {
    "id": "10a7c3cd-3ad8-45fc-a9f0-a6c968963703"
   },
   "outputs": [],
   "source": [
    "df_chilit = pd.read_csv(f\"./data/ChiLit_Chunks_{chunk_size}.csv\")\n",
    "#df_chilit = pd.read_csv(f\"./data/ChiLit_Paragraphs.csv\")\n",
    "df_chilit = df_chilit.fillna(\"\")\n",
    "#df_chilit = df_chilit[df_chilit['tokens'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d82df91-f3d8-4c26-a2af-a0e824e6acf7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1754575425746,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "1d82df91-f3d8-4c26-a2af-a0e824e6acf7",
    "outputId": "96a1d149-c211-4156-9ae9-63288252792a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>chapter_num</th>\n",
       "      <th>paragraph_num</th>\n",
       "      <th>paragraph_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alice</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Alice was beginning to get very tired of sitti...</td>\n",
       "      <td>begin get tired sit sister bank have do peep b...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alice</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>So she was considering in her own mind (as wel...</td>\n",
       "      <td>consider own mind hot day make feel sleepy stu...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alice</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>There was nothing so VERY remarkable in that; ...</td>\n",
       "      <td>be remarkable think way hear say dear dear lat...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>alice</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Either the well was very deep, or she fell ver...</td>\n",
       "      <td>deep fall have plenty time go look wonder go h...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alice</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Down, down, down. Would the fall NEVER come to...</td>\n",
       "      <td>fall come end wonder many mile fall time say g...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book_id  chapter_num  paragraph_num  \\\n",
       "1   alice            1              2   \n",
       "2   alice            1              3   \n",
       "3   alice            1              4   \n",
       "6   alice            1              7   \n",
       "8   alice            1              9   \n",
       "\n",
       "                                      paragraph_text  \\\n",
       "1  Alice was beginning to get very tired of sitti...   \n",
       "2  So she was considering in her own mind (as wel...   \n",
       "3  There was nothing so VERY remarkable in that; ...   \n",
       "6  Either the well was very deep, or she fell ver...   \n",
       "8  Down, down, down. Would the fall NEVER come to...   \n",
       "\n",
       "                                              tokens  num_tokens  \n",
       "1  begin get tired sit sister bank have do peep b...          20  \n",
       "2  consider own mind hot day make feel sleepy stu...          23  \n",
       "3  be remarkable think way hear say dear dear lat...          41  \n",
       "6  deep fall have plenty time go look wonder go h...          46  \n",
       "8  fall come end wonder many mile fall time say g...          43  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chilit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8088ce-6471-4460-923c-384902175954",
   "metadata": {
    "id": "fb8088ce-6471-4460-923c-384902175954"
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for _, row in df_chilit.iterrows():\n",
    "  docs.append(row[\"tokens\"])\n",
    "\n",
    "prepare_octis_corpus(octis_folder, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e381db2-c397-4871-953a-9a7ac0cdb78b",
   "metadata": {
    "id": "5e381db2-c397-4871-953a-9a7ac0cdb78b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aplRtrBcrfMU",
   "metadata": {
    "id": "aplRtrBcrfMU"
   },
   "source": [
    "#### Load OCTIS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ce3914-c7a9-48c5-bf40-2d11e96ea6bd",
   "metadata": {
    "id": "86ce3914-c7a9-48c5-bf40-2d11e96ea6bd"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.load_custom_dataset_from_folder(octis_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a564a-6b51-4027-b348-b1a7498f469f",
   "metadata": {
    "id": "959a564a-6b51-4027-b348-b1a7498f469f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da7da774-ba86-4147-8275-7ae101a1728d",
   "metadata": {
    "id": "da7da774-ba86-4147-8275-7ae101a1728d"
   },
   "source": [
    "### Custom metric for OCTIS optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e38a6-8d6c-4d63-9eaa-7929c615642f",
   "metadata": {
    "id": "039e38a6-8d6c-4d63-9eaa-7929c615642f"
   },
   "outputs": [],
   "source": [
    "from octis.evaluation_metrics.metrics import AbstractMetric\n",
    "\n",
    "class CoherenceDiversityCombination(AbstractMetric):\n",
    "    def __init__(self, dataset, alpha=0.7):\n",
    "        \"\"\"\n",
    "        alpha: weight for coherence (between 0 and 1).\n",
    "        (1 - alpha) will be the weight for diversity.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        # Initialize base metrics\n",
    "        self.c_v = Coherence(texts=dataset.get_corpus(), measure=\"c_npmi\")\n",
    "        self.diversity = TopicDiversity(topk=10)\n",
    "\n",
    "    def score(self, model_output, **kwargs):\n",
    "        # Compute individual scores\n",
    "        coherence_score = self.c_v.score(model_output)\n",
    "        diversity_score = self.diversity.score(model_output)\n",
    "\n",
    "        # Weighted combination\n",
    "        combined_score = (self.alpha * coherence_score + (1 - self.alpha) * diversity_score)\n",
    "\n",
    "        return combined_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbefoU6jz3mF",
   "metadata": {
    "id": "dbefoU6jz3mF"
   },
   "source": [
    "## LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BZKQMI0Oy33T",
   "metadata": {
    "id": "BZKQMI0Oy33T"
   },
   "source": [
    "### OCTIS LDA model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc876f-3cb1-4921-b7fd-0e22ed744737",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 73,
     "status": "error",
     "timestamp": 1755551775091,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "f1bc876f-3cb1-4921-b7fd-0e22ed744737",
    "outputId": "05d66cc0-a992-4b37-ed5d-0b33a9de7280"
   },
   "outputs": [],
   "source": [
    "# Define evaluation metric\n",
    "eval_metric = CoherenceDiversityCombination(dataset) # Initialize metric\n",
    "\n",
    "# Istantiate model\n",
    "model = LDA()\n",
    "\n",
    "# Define the search space.\n",
    "search_space = {\n",
    "    \"num_topics\": Integer(10, 50),\n",
    "    \"alpha\": Real(low=0.01, high=2.0),\n",
    "    \"eta\": Real(low=0.0, high=2.0),\n",
    "    \"iterations\": Integer(50, 200)\n",
    "}\n",
    "\n",
    "# Initialize an optimizer object and start the optimization.\n",
    "optimizer = Optimizer()\n",
    "optResult = optimizer.optimize(\n",
    "    model, dataset,\n",
    "    eval_metric,\n",
    "    search_space,\n",
    "    save_path=octis_folder,\n",
    "    save_name = 'OCTIS_LDA',\n",
    "    number_of_call=50, # number of optimization iterations\n",
    "    model_runs=5 # number of runs of the topic model\n",
    ")\n",
    "\n",
    "#save the results of th optimization in a csv file\n",
    "optResult.save_to_csv(\"OCTIS_LDA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bc3ad-8fdf-4a75-86bb-0a1a502e2a50",
   "metadata": {
    "id": "4a5bc3ad-8fdf-4a75-86bb-0a1a502e2a50"
   },
   "source": [
    "#### Train model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "67a063b2-ce4a-4c38-ad7f-3b100111ff98",
   "metadata": {
    "executionInfo": {
     "elapsed": 96,
     "status": "aborted",
     "timestamp": 1755551775172,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "67a063b2-ce4a-4c38-ad7f-3b100111ff98"
   },
   "outputs": [],
   "source": [
    "results = json.load(open(f\"{octis_folder}/OCTIS_LDA.json\",'r'))\n",
    "best_iter = results['f_val'].index(max(results['f_val']))\n",
    "model = LDA(\n",
    "    num_topics=results['x_iters']['num_topics'][best_iter],\n",
    "    alpha= results['x_iters']['alpha'][best_iter],\n",
    "    eta= results['x_iters']['eta'][best_iter],\n",
    "    passes=20)\n",
    "\n",
    "output = model.train_model(dataset)\n",
    "\n",
    "pickle.dump(output, open(octis_folder + \"OCTIS_LDA_output.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54540cd5-cba4-400e-8052-1e67c44b2f66",
   "metadata": {
    "executionInfo": {
     "elapsed": 97,
     "status": "aborted",
     "timestamp": 1755551775178,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "54540cd5-cba4-400e-8052-1e67c44b2f66"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee5a3a-8389-4c32-873d-d3cdbdfae1f4",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1755551775180,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "e9ee5a3a-8389-4c32-873d-d3cdbdfae1f4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "417aca10-8f69-43a6-8cdc-3b923cbc3ed8",
   "metadata": {
    "id": "417aca10-8f69-43a6-8cdc-3b923cbc3ed8"
   },
   "source": [
    "### Optuna LDA multi-objective optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jFsePI4HnAvT",
   "metadata": {
    "id": "jFsePI4HnAvT"
   },
   "outputs": [],
   "source": [
    "optuna.delete_study(study_name=\"LDA_Study\", storage=f\"sqlite:///{optuna_folder}LDA_Study.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vc8Nz53h1vUd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 12373224,
     "status": "ok",
     "timestamp": 1754667851410,
     "user": {
      "displayName": "Giovanna Tonazzo",
      "userId": "15393083850265173052"
     },
     "user_tz": -120
    },
    "id": "Vc8Nz53h1vUd",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "549d4cd6-aa1c-44f9-d2a7-b38f85b3ffd5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 23:35:10,519] A new study created in RDB with name: LDA_Study\n",
      "[I 2025-08-12 23:40:50,226] Trial 0 finished with values: [-0.01527873111567863, 0.74] and parameters: {'num_topics': 25, 'alpha': 0.040399725923978266, 'eta': 0.012447161904934178, 'passes': 10, 'iterations': 195}.\n",
      "[I 2025-08-12 23:46:25,126] Trial 1 finished with values: [-0.1409386265621207, 0.9129032258064517] and parameters: {'num_topics': 31, 'alpha': 0.19711802101251766, 'eta': 0.13859233231342827, 'passes': 26, 'iterations': 153}.\n",
      "[I 2025-08-12 23:49:15,992] Trial 2 finished with values: [-0.06718309715057955, 0.8076923076923077] and parameters: {'num_topics': 13, 'alpha': 0.02329181016923864, 'eta': 0.1422413420782142, 'passes': 12, 'iterations': 176}.\n",
      "[I 2025-08-12 23:56:14,521] Trial 3 finished with values: [-0.061582172735959735, 0.8377777777777777] and parameters: {'num_topics': 45, 'alpha': 0.18177054258786077, 'eta': 0.03887815152676666, 'passes': 21, 'iterations': 126}.\n",
      "[I 2025-08-12 23:59:36,816] Trial 4 finished with values: [-0.0651524573402052, 0.8352941176470589] and parameters: {'num_topics': 17, 'alpha': 0.03488432271769642, 'eta': 0.09842721994617461, 'passes': 16, 'iterations': 110}.\n",
      "[I 2025-08-13 00:04:17,918] Trial 5 finished with values: [-0.1682649040165453, 0.9116279069767442] and parameters: {'num_topics': 43, 'alpha': 0.018769088455652963, 'eta': 0.09005865272318636, 'passes': 16, 'iterations': 174}.\n",
      "[I 2025-08-13 00:08:42,833] Trial 6 finished with values: [-0.07286701071202992, 0.8291666666666667] and parameters: {'num_topics': 24, 'alpha': 0.04415071017209227, 'eta': 0.0525488294017917, 'passes': 25, 'iterations': 59}.\n",
      "[I 2025-08-13 00:14:11,850] Trial 7 finished with values: [-0.022006837349015353, 0.8214285714285714] and parameters: {'num_topics': 14, 'alpha': 0.08263819021129706, 'eta': 0.11478421778029362, 'passes': 29, 'iterations': 139}.\n",
      "[I 2025-08-13 00:20:23,705] Trial 8 finished with values: [-0.13664213517637258, 0.9292682926829269] and parameters: {'num_topics': 41, 'alpha': 0.07279918300069405, 'eta': 0.16491169911038941, 'passes': 28, 'iterations': 110}.\n",
      "[I 2025-08-13 00:25:34,067] Trial 9 finished with values: [-0.04307426987868325, 0.8285714285714286] and parameters: {'num_topics': 14, 'alpha': 0.050157636083670895, 'eta': 0.12230635570122862, 'passes': 30, 'iterations': 132}.\n",
      "[I 2025-08-13 00:27:32,135] Trial 10 finished with values: [-0.16319937839281654, 0.8744186046511628] and parameters: {'num_topics': 43, 'alpha': 0.19134557918682032, 'eta': 0.15452322200315252, 'passes': 6, 'iterations': 91}.\n",
      "[I 2025-08-13 00:29:53,483] Trial 11 finished with values: [-0.16661348810511312, 0.9046511627906977] and parameters: {'num_topics': 43, 'alpha': 0.1053741428073114, 'eta': 0.13181907566652198, 'passes': 8, 'iterations': 90}.\n",
      "[I 2025-08-13 00:34:45,586] Trial 12 finished with values: [-0.16864003446991516, 0.9102040816326531] and parameters: {'num_topics': 49, 'alpha': 0.04855128504272087, 'eta': 0.14619142435982355, 'passes': 13, 'iterations': 141}.\n",
      "[I 2025-08-13 00:38:46,672] Trial 13 finished with values: [-0.08528632973939572, 0.8588235294117647] and parameters: {'num_topics': 17, 'alpha': 0.10709030189611563, 'eta': 0.12293504368006376, 'passes': 21, 'iterations': 144}.\n",
      "[I 2025-08-13 00:41:54,480] Trial 14 finished with values: [-0.09772353285137095, 0.92] and parameters: {'num_topics': 25, 'alpha': 0.1382267705701357, 'eta': 0.16743713252427922, 'passes': 17, 'iterations': 75}.\n",
      "[I 2025-08-13 00:45:02,277] Trial 15 finished with values: [-0.16980055616037826, 0.9102564102564102] and parameters: {'num_topics': 39, 'alpha': 0.0386258347749478, 'eta': 0.14182090284019774, 'passes': 13, 'iterations': 198}.\n",
      "[I 2025-08-13 00:51:12,143] Trial 16 finished with values: [-0.174556297460616, 0.9357142857142857] and parameters: {'num_topics': 42, 'alpha': 0.17640734041742767, 'eta': 0.1972758279551476, 'passes': 28, 'iterations': 141}.\n",
      "[I 2025-08-13 01:02:09,170] Trial 17 finished with values: [-0.005499611785714111, 0.7384615384615385] and parameters: {'num_topics': 26, 'alpha': 0.14095468189787447, 'eta': 0.012163847475525317, 'passes': 17, 'iterations': 146}.\n",
      "[I 2025-08-13 01:06:29,334] Trial 18 finished with values: [-0.11165341290192236, 0.921875] and parameters: {'num_topics': 32, 'alpha': 0.17692343783020337, 'eta': 0.18930604048057686, 'passes': 24, 'iterations': 80}.\n",
      "[I 2025-08-13 01:09:49,654] Trial 19 finished with values: [-0.07359561928111354, 0.8894736842105263] and parameters: {'num_topics': 19, 'alpha': 0.025343088805806286, 'eta': 0.1668207071554553, 'passes': 21, 'iterations': 60}.\n",
      "[I 2025-08-13 01:17:10,788] Trial 20 finished with values: [-0.16888893113636258, 0.9333333333333333] and parameters: {'num_topics': 45, 'alpha': 0.11247684874979444, 'eta': 0.17219089432189805, 'passes': 29, 'iterations': 184}.\n",
      "[I 2025-08-13 01:21:45,104] Trial 21 finished with values: [-0.07545532890400337, 0.8357142857142857] and parameters: {'num_topics': 14, 'alpha': 0.1660106581379266, 'eta': 0.17845696386360338, 'passes': 25, 'iterations': 139}.\n",
      "[I 2025-08-13 01:26:22,208] Trial 22 finished with values: [-0.17442752710278983, 0.9363636363636364] and parameters: {'num_topics': 44, 'alpha': 0.10909420919293025, 'eta': 0.14381496534167293, 'passes': 18, 'iterations': 156}.\n",
      "[I 2025-08-13 01:28:37,254] Trial 23 finished with values: [-0.09110871738248902, 0.8521739130434782] and parameters: {'num_topics': 23, 'alpha': 0.1789586666771088, 'eta': 0.10258405895264941, 'passes': 9, 'iterations': 177}.\n",
      "[I 2025-08-13 01:31:49,614] Trial 24 finished with values: [-0.018909999802533762, 0.7375] and parameters: {'num_topics': 24, 'alpha': 0.059774143917903186, 'eta': 0.017046745770376116, 'passes': 10, 'iterations': 123}.\n",
      "[I 2025-08-13 01:34:09,067] Trial 25 finished with values: [-0.1529837007734722, 0.88] and parameters: {'num_topics': 45, 'alpha': 0.019159641023973207, 'eta': 0.10374734431983271, 'passes': 6, 'iterations': 141}.\n",
      "[I 2025-08-13 01:36:31,948] Trial 26 finished with values: [-0.11255115853298954, 0.8826086956521739] and parameters: {'num_topics': 23, 'alpha': 0.10619057085436688, 'eta': 0.10492563915821744, 'passes': 11, 'iterations': 114}.\n",
      "[I 2025-08-13 01:41:46,136] Trial 27 finished with values: [-0.085247002574323, 0.8705882352941177] and parameters: {'num_topics': 17, 'alpha': 0.17350202374684792, 'eta': 0.13430320309356472, 'passes': 29, 'iterations': 160}.\n",
      "[I 2025-08-13 01:47:18,042] Trial 28 finished with values: [-0.12693884274870787, 0.9209302325581395] and parameters: {'num_topics': 43, 'alpha': 0.17783040591389107, 'eta': 0.18165461789249546, 'passes': 26, 'iterations': 60}.\n",
      "[I 2025-08-13 01:49:40,654] Trial 29 finished with values: [-0.033147796516368434, 0.7678571428571429] and parameters: {'num_topics': 28, 'alpha': 0.06929183673159899, 'eta': 0.02768813504300214, 'passes': 10, 'iterations': 114}.\n",
      "[I 2025-08-13 01:55:43,175] Trial 30 finished with values: [-0.060793354879482384, 0.8064516129032258] and parameters: {'num_topics': 31, 'alpha': 0.04532373131222612, 'eta': 0.036533978144051515, 'passes': 29, 'iterations': 197}.\n",
      "[I 2025-08-13 01:59:10,991] Trial 31 finished with values: [-0.1178086076461351, 0.8783783783783784] and parameters: {'num_topics': 37, 'alpha': 0.16014337333388742, 'eta': 0.054436342543563394, 'passes': 15, 'iterations': 101}.\n",
      "[I 2025-08-13 02:01:26,396] Trial 32 finished with values: [-0.023154677352655896, 0.7666666666666667] and parameters: {'num_topics': 18, 'alpha': 0.1570609361287365, 'eta': 0.04145297808348225, 'passes': 9, 'iterations': 136}.\n",
      "[I 2025-08-13 02:05:45,662] Trial 33 finished with values: [-0.13900961646361215, 0.9162162162162162] and parameters: {'num_topics': 37, 'alpha': 0.13254797657775672, 'eta': 0.07004871514338175, 'passes': 19, 'iterations': 163}.\n",
      "[I 2025-08-13 02:08:13,794] Trial 34 finished with values: [-0.019529375869714576, 0.8] and parameters: {'num_topics': 12, 'alpha': 0.07612248575652163, 'eta': 0.07681205684970394, 'passes': 11, 'iterations': 155}.\n",
      "[I 2025-08-13 02:13:17,610] Trial 35 finished with values: [-0.036535216782967714, 0.8214285714285714] and parameters: {'num_topics': 14, 'alpha': 0.12171112963615421, 'eta': 0.10978879103355163, 'passes': 28, 'iterations': 187}.\n",
      "[I 2025-08-13 02:17:18,826] Trial 36 finished with values: [-0.11612664762824641, 0.8340909090909091] and parameters: {'num_topics': 44, 'alpha': 0.17092167868649483, 'eta': 0.04894585364027787, 'passes': 14, 'iterations': 103}.\n",
      "[I 2025-08-13 02:20:09,029] Trial 37 finished with values: [-0.05734362808770434, 0.8833333333333333] and parameters: {'num_topics': 18, 'alpha': 0.19333173612299615, 'eta': 0.1720540816088222, 'passes': 16, 'iterations': 64}.\n",
      "[I 2025-08-13 02:27:10,431] Trial 38 finished with values: [-0.17012470093072915, 0.926530612244898] and parameters: {'num_topics': 49, 'alpha': 0.10047423965201158, 'eta': 0.16657452075228424, 'passes': 20, 'iterations': 166}.\n",
      "[I 2025-08-13 02:30:25,273] Trial 39 finished with values: [-0.05825615386842538, 0.8235294117647058] and parameters: {'num_topics': 17, 'alpha': 0.02622939468266993, 'eta': 0.1106788796450622, 'passes': 20, 'iterations': 57}.\n",
      "[I 2025-08-13 02:35:28,158] Trial 40 finished with values: [-0.13523037218242664, 0.9029411764705882] and parameters: {'num_topics': 34, 'alpha': 0.022669947443866377, 'eta': 0.09489926273336277, 'passes': 25, 'iterations': 158}.\n",
      "[I 2025-08-13 02:40:49,588] Trial 41 finished with values: [-0.13756361476484413, 0.9125] and parameters: {'num_topics': 32, 'alpha': 0.05249167290861897, 'eta': 0.11543443490595817, 'passes': 29, 'iterations': 174}.\n",
      "[I 2025-08-13 02:44:53,839] Trial 42 finished with values: [-0.011150477715279293, 0.7862068965517242] and parameters: {'num_topics': 29, 'alpha': 0.04527755597405852, 'eta': 0.022557245754183858, 'passes': 24, 'iterations': 54}.\n",
      "[I 2025-08-13 02:48:49,641] Trial 43 finished with values: [-0.014116639180241154, 0.79375] and parameters: {'num_topics': 16, 'alpha': 0.03896883188044942, 'eta': 0.06863843362649057, 'passes': 23, 'iterations': 76}.\n",
      "[I 2025-08-13 02:53:10,435] Trial 44 finished with values: [-0.024354380803940427, 0.7862068965517242] and parameters: {'num_topics': 29, 'alpha': 0.17740890070930765, 'eta': 0.026364160825645175, 'passes': 18, 'iterations': 148}.\n",
      "[I 2025-08-13 02:54:57,367] Trial 45 finished with values: [-0.1272995420216748, 0.8904761904761904] and parameters: {'num_topics': 21, 'alpha': 0.15853273979054205, 'eta': 0.15986969389859382, 'passes': 7, 'iterations': 148}.\n",
      "[I 2025-08-13 02:59:58,449] Trial 46 finished with values: [-0.13920284743748063, 0.9333333333333333] and parameters: {'num_topics': 33, 'alpha': 0.10406686068273367, 'eta': 0.1551649494183491, 'passes': 27, 'iterations': 112}.\n",
      "[I 2025-08-13 03:04:35,895] Trial 47 finished with values: [-0.1644454126737264, 0.925] and parameters: {'num_topics': 44, 'alpha': 0.10568432347351848, 'eta': 0.09145924933041565, 'passes': 19, 'iterations': 83}.\n",
      "[I 2025-08-13 03:08:35,575] Trial 48 finished with values: [-0.06807966239534646, 0.8153846153846154] and parameters: {'num_topics': 39, 'alpha': 0.11345969536144203, 'eta': 0.02764819052930039, 'passes': 16, 'iterations': 149}.\n",
      "[I 2025-08-13 03:12:13,123] Trial 49 finished with values: [-0.09010304084795367, 0.895] and parameters: {'num_topics': 20, 'alpha': 0.08032524910257355, 'eta': 0.1746142510917802, 'passes': 21, 'iterations': 101}.\n"
     ]
    }
   ],
   "source": [
    "def objectiveLDA(trial) -> Tuple[float, float]:\n",
    "\n",
    "    # Define hyperparameters to optimize\n",
    "    num_topics = trial.suggest_int(\"num_topics\", 10, 50)\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.01, 0.2)\n",
    "    eta = trial.suggest_float(\"eta\", 0.01, 0.2)\n",
    "    passes = trial.suggest_int(\"passes\", 5, 30)\n",
    "    iterations = trial.suggest_int(\"iterations\", 50, 200)\n",
    "\n",
    "    # Train LDA model\n",
    "    model = LDA(\n",
    "        num_topics=num_topics,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        passes=passes,\n",
    "        iterations=iterations,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    output = model.train_model(dataset)\n",
    "\n",
    "    # Compute coherence score\n",
    "    coherence_metrics = Coherence(texts=dataset.get_corpus(), #list of our documents\n",
    "                    measure='c_npmi')\n",
    "    coherence = coherence_metrics.score(output)\n",
    "\n",
    "    # Compute diversity score\n",
    "    diverisity_metric = TopicDiversity(topk=10) # Initialize metric\n",
    "    diversity = diverisity_metric.score(output)\n",
    "\n",
    "    return coherence, diversity  # Optuna will maximize these\n",
    "\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(\n",
    "    directions=[\"maximize\",\"maximize\"],\n",
    "    storage=f\"sqlite:///{optuna_folder}LDA_Study.db\",\n",
    "    study_name=\"LDA_Study\")\n",
    "study.optimize(objectiveLDA, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "WqKa-3tB2uzy",
   "metadata": {
    "id": "WqKa-3tB2uzy"
   },
   "outputs": [],
   "source": [
    "def train_final_LDA_model(params):\n",
    "    \"\"\"Train final LDA model with selected parameters\"\"\"\n",
    "    print(f\"\\nTraining final model with parameters: {params}\")\n",
    "\n",
    "    model = LDA(\n",
    "        num_topics=params[\"num_topics\"],\n",
    "        alpha=params[\"alpha\"],\n",
    "        eta=params[\"eta\"],\n",
    "        passes=params[\"passes\"],\n",
    "        iterations=params[\"iterations\"],\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    output = model.train_model(dataset)\n",
    "\n",
    "    # Calculate final metrics\n",
    "    coherence_metrics = Coherence(texts=dataset.get_corpus(), #list of our documents\n",
    "                    measure='c_npmi')\n",
    "    coherence = coherence_metrics.score(output)\n",
    "\n",
    "    diverisity_metric = TopicDiversity(topk=10) # Initialize metric\n",
    "    diversity = diverisity_metric.score(output)\n",
    "\n",
    "    print(f\"Final model metrics:\")\n",
    "    print(f\"  Coherence: {coherence:.4f}\")\n",
    "    print(f\"  Diversity: {diversity:.4f}\")\n",
    "\n",
    "    return model, output, coherence, diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5eac4919-665b-4316-9c01-0ea27110bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_study = optuna.load_study(\n",
    "    storage=f\"sqlite:///{optuna_folder}LDA_Study.db\",\n",
    "    study_name=\"LDA_Study\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "H0UGSqrBGhsz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120107,
     "status": "ok",
     "timestamp": 1754668151607,
     "user": {
      "displayName": "Giovanna Tonazzo",
      "userId": "15393083850265173052"
     },
     "user_tz": -120
    },
    "id": "H0UGSqrBGhsz",
    "outputId": "e4337b53-bbba-4318-b795-21674dd4748d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model with parameters: {'num_topics': 14, 'alpha': 0.08263819021129706, 'eta': 0.11478421778029362, 'passes': 29, 'iterations': 139}\n",
      "Final model metrics:\n",
      "  Coherence: -0.0220\n",
      "  Diversity: 0.8214\n",
      "\n",
      "Final model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "if LDA_study.best_trials:\n",
    "    # Get balanced solution\n",
    "    pareto_trials = LDA_study.best_trials\n",
    "\n",
    "    # Pick the first Pareto optimal solution\n",
    "    selected_params = pareto_trials[0].params\n",
    "    final_model, final_output, final_coherence, final_diversity = train_final_LDA_model(selected_params)\n",
    "\n",
    "    print(f\"\\nFinal model trained successfully!\")\n",
    "\n",
    "pickle.dump(final_output, open(optuna_folder + \"Optuna_LDA_output.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mc06e7vmX-GH",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755547894276,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "mc06e7vmX-GH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pkwbv989k2MW",
   "metadata": {
    "id": "pkwbv989k2MW"
   },
   "source": [
    "## ProdLDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zWh7dvzb0CNy",
   "metadata": {
    "id": "zWh7dvzb0CNy"
   },
   "source": [
    "### OCTIS ProdLDA optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf494d5-6457-4718-a46e-b615d7480603",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 46,
     "status": "error",
     "timestamp": 1755551798904,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "faf494d5-6457-4718-a46e-b615d7480603",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f4b84f86-5d19-41ad-dd31-ccf46ea6f381"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CoherenceDiversityCombination' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3625580042.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meval_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceDiversityCombination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Initialize metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProdLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_partitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CoherenceDiversityCombination' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize metric\n",
    "eval_metric = CoherenceDiversityCombination(dataset) # Initialize metric\n",
    "\n",
    "# Initialize model\n",
    "model = ProdLDA(use_partitions=False)\n",
    "\n",
    "# Define the search space.\n",
    "search_space = {\n",
    "    \"num_topics\": Integer(10,50),\n",
    "    \"dropout\" : Real(low=0, high=0.60),\n",
    "    \"num_neurons\" : Categorical([50, 100, 200, 300]),\n",
    "    \"num_layers\": Integer(1,3),\n",
    "    \"activation\" : Categorical([\"softplus\",\"relu\", \"sigmoid\"])\n",
    "}\n",
    "\n",
    "# Initialize an optimizer object and start the optimization.\n",
    "optimizer=Optimizer()\n",
    "optResult=optimizer.optimize(\n",
    "    model, dataset,\n",
    "    eval_metric,\n",
    "    search_space,\n",
    "    save_name='OCTIS_ProdLDA',\n",
    "    save_path=octis_folder,\n",
    "    number_of_call=50, # number of optimization iterations\n",
    "    model_runs=5  # number of runs of the topic model\n",
    ")\n",
    "\n",
    "#save the results of th optimization in a csv file\n",
    "optResult.save_to_csv(\"OCTIS_ProdLDA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VmAWzDfP0TmW",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1755551798985,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "VmAWzDfP0TmW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Xc5R_7Q60WpN",
   "metadata": {
    "id": "Xc5R_7Q60WpN"
   },
   "source": [
    "### Train ProdLDA model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9588916-aed0-442a-89fd-bf993224022b",
   "metadata": {
    "id": "b9588916-aed0-442a-89fd-bf993224022b",
    "outputId": "0348ff69-dabc-4cec-fdac-6045cb2de582",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [5843/584300]\tTrain Loss: 2257.5753472370784\tTime: 0:00:04.272165\n",
      "Epoch: [2/100]\tSamples: [11686/584300]\tTrain Loss: 2216.680636819699\tTime: 0:00:04.100038\n",
      "Epoch: [3/100]\tSamples: [17529/584300]\tTrain Loss: 2188.057128989817\tTime: 0:00:04.197414\n",
      "Epoch: [4/100]\tSamples: [23372/584300]\tTrain Loss: 2171.4995895195107\tTime: 0:00:03.960664\n",
      "Epoch: [5/100]\tSamples: [29215/584300]\tTrain Loss: 2159.9728153613296\tTime: 0:00:03.670171\n",
      "Epoch: [6/100]\tSamples: [35058/584300]\tTrain Loss: 2152.416890403046\tTime: 0:00:04.057952\n",
      "Epoch: [7/100]\tSamples: [40901/584300]\tTrain Loss: 2146.714863137515\tTime: 0:00:03.619083\n",
      "Epoch: [8/100]\tSamples: [46744/584300]\tTrain Loss: 2141.3794129995294\tTime: 0:00:03.998018\n",
      "Epoch: [9/100]\tSamples: [52587/584300]\tTrain Loss: 2139.1175999860943\tTime: 0:00:04.149957\n",
      "Epoch: [10/100]\tSamples: [58430/584300]\tTrain Loss: 2137.014820752396\tTime: 0:00:04.178687\n",
      "Epoch: [11/100]\tSamples: [64273/584300]\tTrain Loss: 2133.2107302541503\tTime: 0:00:04.289923\n",
      "Epoch: [12/100]\tSamples: [70116/584300]\tTrain Loss: 2131.6667981452165\tTime: 0:00:04.203596\n",
      "Epoch: [13/100]\tSamples: [75959/584300]\tTrain Loss: 2130.60145954561\tTime: 0:00:06.155934\n",
      "Epoch: [14/100]\tSamples: [81802/584300]\tTrain Loss: 2128.1353743261166\tTime: 0:00:05.686578\n",
      "Epoch: [15/100]\tSamples: [87645/584300]\tTrain Loss: 2125.966353970563\tTime: 0:00:04.535578\n",
      "Epoch: [16/100]\tSamples: [93488/584300]\tTrain Loss: 2126.8583441190312\tTime: 0:00:04.055727\n",
      "Epoch: [17/100]\tSamples: [99331/584300]\tTrain Loss: 2125.638387413358\tTime: 0:00:03.660432\n",
      "Epoch: [18/100]\tSamples: [105174/584300]\tTrain Loss: 2122.871995603714\tTime: 0:00:03.965322\n",
      "Epoch: [19/100]\tSamples: [111017/584300]\tTrain Loss: 2124.401732040476\tTime: 0:00:03.792903\n",
      "Epoch: [20/100]\tSamples: [116860/584300]\tTrain Loss: 2123.243642900693\tTime: 0:00:03.667378\n",
      "Epoch: [21/100]\tSamples: [122703/584300]\tTrain Loss: 2122.606730275543\tTime: 0:00:03.593728\n",
      "Epoch: [22/100]\tSamples: [128546/584300]\tTrain Loss: 2121.267105805023\tTime: 0:00:03.614359\n",
      "Epoch: [23/100]\tSamples: [134389/584300]\tTrain Loss: 2122.5705738971847\tTime: 0:00:03.775732\n",
      "Epoch: [24/100]\tSamples: [140232/584300]\tTrain Loss: 2120.0892881974155\tTime: 0:00:03.694808\n",
      "Epoch: [25/100]\tSamples: [146075/584300]\tTrain Loss: 2120.388514435008\tTime: 0:00:03.558196\n",
      "Epoch: [26/100]\tSamples: [151918/584300]\tTrain Loss: 2120.458487319228\tTime: 0:00:03.682561\n",
      "Epoch: [27/100]\tSamples: [157761/584300]\tTrain Loss: 2118.932202528667\tTime: 0:00:03.610093\n",
      "Epoch: [28/100]\tSamples: [163604/584300]\tTrain Loss: 2119.464129086086\tTime: 0:00:03.599592\n",
      "Epoch: [29/100]\tSamples: [169447/584300]\tTrain Loss: 2119.385827592846\tTime: 0:00:03.577675\n",
      "Epoch: [30/100]\tSamples: [175290/584300]\tTrain Loss: 2118.91798479484\tTime: 0:00:03.606305\n",
      "Epoch: [31/100]\tSamples: [181133/584300]\tTrain Loss: 2117.997895451823\tTime: 0:00:03.794649\n",
      "Epoch: [32/100]\tSamples: [186976/584300]\tTrain Loss: 2118.471153383322\tTime: 0:00:04.047616\n",
      "Epoch: [33/100]\tSamples: [192819/584300]\tTrain Loss: 2117.21871256204\tTime: 0:00:03.925154\n",
      "Epoch: [34/100]\tSamples: [198662/584300]\tTrain Loss: 2116.7193917935992\tTime: 0:00:03.954652\n",
      "Epoch: [35/100]\tSamples: [204505/584300]\tTrain Loss: 2116.6232832021224\tTime: 0:00:04.302898\n",
      "Epoch: [36/100]\tSamples: [210348/584300]\tTrain Loss: 2116.527934066404\tTime: 0:00:04.423754\n",
      "Epoch: [37/100]\tSamples: [216191/584300]\tTrain Loss: 2116.5620092953104\tTime: 0:00:04.236320\n",
      "Epoch: [38/100]\tSamples: [222034/584300]\tTrain Loss: 2117.3529684291034\tTime: 0:00:04.037215\n",
      "Epoch: [39/100]\tSamples: [227877/584300]\tTrain Loss: 2117.068744785427\tTime: 0:00:04.110940\n",
      "Epoch: [40/100]\tSamples: [233720/584300]\tTrain Loss: 2117.114169733014\tTime: 0:00:04.015002\n",
      "Epoch: [41/100]\tSamples: [239563/584300]\tTrain Loss: 2116.2784956358037\tTime: 0:00:04.201782\n",
      "Epoch: [42/100]\tSamples: [245406/584300]\tTrain Loss: 2116.224398452208\tTime: 0:00:04.189079\n",
      "Epoch: [43/100]\tSamples: [251249/584300]\tTrain Loss: 2115.8565785180986\tTime: 0:00:03.951939\n",
      "Epoch: [44/100]\tSamples: [257092/584300]\tTrain Loss: 2116.92192981987\tTime: 0:00:04.351504\n",
      "Epoch: [45/100]\tSamples: [262935/584300]\tTrain Loss: 2114.4082716498374\tTime: 0:00:04.160645\n",
      "Epoch: [46/100]\tSamples: [268778/584300]\tTrain Loss: 2115.530404971761\tTime: 0:00:04.296995\n",
      "Epoch: [47/100]\tSamples: [274621/584300]\tTrain Loss: 2115.77733572758\tTime: 0:00:04.298472\n",
      "Epoch: [48/100]\tSamples: [280464/584300]\tTrain Loss: 2114.2441222402877\tTime: 0:00:04.241984\n",
      "Epoch: [49/100]\tSamples: [286307/584300]\tTrain Loss: 2115.79291794134\tTime: 0:00:03.973784\n",
      "Epoch: [50/100]\tSamples: [292150/584300]\tTrain Loss: 2115.5166010610988\tTime: 0:00:04.257969\n",
      "Epoch: [51/100]\tSamples: [297993/584300]\tTrain Loss: 2114.6207815441553\tTime: 0:00:03.939059\n",
      "Epoch: [52/100]\tSamples: [303836/584300]\tTrain Loss: 2115.701563970777\tTime: 0:00:04.263291\n",
      "Epoch: [53/100]\tSamples: [309679/584300]\tTrain Loss: 2115.3566420289235\tTime: 0:00:04.141379\n",
      "Epoch: [54/100]\tSamples: [315522/584300]\tTrain Loss: 2114.8826714391153\tTime: 0:00:04.285505\n",
      "Epoch: [55/100]\tSamples: [321365/584300]\tTrain Loss: 2114.473386958754\tTime: 0:00:04.014552\n",
      "Epoch: [56/100]\tSamples: [327208/584300]\tTrain Loss: 2116.28898361287\tTime: 0:00:04.181113\n",
      "Epoch: [57/100]\tSamples: [333051/584300]\tTrain Loss: 2115.0174768419474\tTime: 0:00:04.329160\n",
      "Epoch: [58/100]\tSamples: [338894/584300]\tTrain Loss: 2114.751441361458\tTime: 0:00:04.253145\n",
      "Epoch: [59/100]\tSamples: [344737/584300]\tTrain Loss: 2114.143269055922\tTime: 0:00:04.065229\n",
      "Epoch: [60/100]\tSamples: [350580/584300]\tTrain Loss: 2113.640812858335\tTime: 0:00:04.115011\n",
      "Epoch: [61/100]\tSamples: [356423/584300]\tTrain Loss: 2113.664773821239\tTime: 0:00:03.810678\n",
      "Epoch: [62/100]\tSamples: [362266/584300]\tTrain Loss: 2113.8822569474155\tTime: 0:00:04.017210\n",
      "Epoch: [63/100]\tSamples: [368109/584300]\tTrain Loss: 2114.0041703213246\tTime: 0:00:03.971453\n",
      "Epoch: [64/100]\tSamples: [373952/584300]\tTrain Loss: 2114.3222445661477\tTime: 0:00:03.881559\n",
      "Epoch: [65/100]\tSamples: [379795/584300]\tTrain Loss: 2113.6410468455842\tTime: 0:00:03.552274\n",
      "Epoch: [66/100]\tSamples: [385638/584300]\tTrain Loss: 2113.693651859062\tTime: 0:00:03.890278\n",
      "Epoch: [67/100]\tSamples: [391481/584300]\tTrain Loss: 2113.016507466199\tTime: 0:00:04.055805\n",
      "Epoch: [68/100]\tSamples: [397324/584300]\tTrain Loss: 2112.7021302199214\tTime: 0:00:04.148732\n",
      "Epoch: [69/100]\tSamples: [403167/584300]\tTrain Loss: 2112.4825271692625\tTime: 0:00:04.106906\n",
      "Epoch: [70/100]\tSamples: [409010/584300]\tTrain Loss: 2112.979745395131\tTime: 0:00:04.167682\n",
      "Epoch: [71/100]\tSamples: [414853/584300]\tTrain Loss: 2114.397604505391\tTime: 0:00:03.953616\n",
      "Epoch: [72/100]\tSamples: [420696/584300]\tTrain Loss: 2113.2615596386704\tTime: 0:00:03.600703\n",
      "Epoch: [73/100]\tSamples: [426539/584300]\tTrain Loss: 2112.9348338824234\tTime: 0:00:03.768962\n",
      "Epoch: [74/100]\tSamples: [432382/584300]\tTrain Loss: 2113.7175566650267\tTime: 0:00:03.716523\n",
      "Epoch: [75/100]\tSamples: [438225/584300]\tTrain Loss: 2112.325910143548\tTime: 0:00:03.682041\n",
      "Epoch: [76/100]\tSamples: [444068/584300]\tTrain Loss: 2112.004509937104\tTime: 0:00:03.718461\n",
      "Epoch: [77/100]\tSamples: [449911/584300]\tTrain Loss: 2112.1668255551513\tTime: 0:00:03.731160\n",
      "Epoch: [78/100]\tSamples: [455754/584300]\tTrain Loss: 2111.6909295845458\tTime: 0:00:03.713374\n",
      "Epoch: [79/100]\tSamples: [461597/584300]\tTrain Loss: 2112.018015681157\tTime: 0:00:03.713995\n",
      "Epoch: [80/100]\tSamples: [467440/584300]\tTrain Loss: 2112.349974060842\tTime: 0:00:03.894529\n",
      "Epoch: [81/100]\tSamples: [473283/584300]\tTrain Loss: 2112.410214412545\tTime: 0:00:03.994777\n",
      "Epoch: [82/100]\tSamples: [479126/584300]\tTrain Loss: 2112.0076694335103\tTime: 0:00:03.739684\n",
      "Epoch: [83/100]\tSamples: [484969/584300]\tTrain Loss: 2113.1368705074447\tTime: 0:00:03.896611\n",
      "Epoch: [84/100]\tSamples: [490812/584300]\tTrain Loss: 2113.5061572073423\tTime: 0:00:04.137243\n",
      "Epoch: [85/100]\tSamples: [496655/584300]\tTrain Loss: 2113.2150423048947\tTime: 0:00:04.165952\n",
      "Epoch: [86/100]\tSamples: [502498/584300]\tTrain Loss: 2112.3128122058447\tTime: 0:00:04.255185\n",
      "Epoch: [87/100]\tSamples: [508341/584300]\tTrain Loss: 2111.297525484554\tTime: 0:00:04.237581\n",
      "Epoch: [88/100]\tSamples: [514184/584300]\tTrain Loss: 2111.564902046252\tTime: 0:00:03.982156\n",
      "Epoch: [89/100]\tSamples: [520027/584300]\tTrain Loss: 2111.526514098066\tTime: 0:00:04.153090\n",
      "Epoch: [90/100]\tSamples: [525870/584300]\tTrain Loss: 2112.0303722135463\tTime: 0:00:04.233708\n",
      "Epoch: [91/100]\tSamples: [531713/584300]\tTrain Loss: 2111.7970862570596\tTime: 0:00:04.158536\n",
      "Epoch: [92/100]\tSamples: [537556/584300]\tTrain Loss: 2111.783472210337\tTime: 0:00:04.324056\n",
      "Epoch: [93/100]\tSamples: [543399/584300]\tTrain Loss: 2111.5321846119286\tTime: 0:00:03.677153\n",
      "Epoch: [94/100]\tSamples: [549242/584300]\tTrain Loss: 2111.752345889312\tTime: 0:00:03.778534\n",
      "Epoch: [95/100]\tSamples: [555085/584300]\tTrain Loss: 2111.3683173241484\tTime: 0:00:04.160656\n",
      "Epoch: [96/100]\tSamples: [560928/584300]\tTrain Loss: 2111.3225153228223\tTime: 0:00:04.608057\n",
      "Epoch: [97/100]\tSamples: [566771/584300]\tTrain Loss: 2112.036767419348\tTime: 0:00:04.364146\n",
      "Epoch: [98/100]\tSamples: [572614/584300]\tTrain Loss: 2111.8848154308575\tTime: 0:00:03.956166\n",
      "Epoch: [99/100]\tSamples: [578457/584300]\tTrain Loss: 2112.0364665785983\tTime: 0:00:04.157533\n",
      "Epoch: [100/100]\tSamples: [584300/584300]\tTrain Loss: 2112.162521526827\tTime: 0:00:04.287766\n"
     ]
    }
   ],
   "source": [
    "results = json.load(open(f\"{octis_folder}/OCTIS_ProdLDA.json\",'r'))\n",
    "best_iter = results['f_val'].index(max(results['f_val']))\n",
    "\n",
    "model = ProdLDA(\n",
    "    num_topics=results['x_iters']['num_topics'][best_iter],\n",
    "    dropout=results['x_iters']['dropout'][best_iter],\n",
    "    num_neurons=results['x_iters']['num_neurons'][best_iter],\n",
    "    num_layers=results['x_iters']['num_layers'][best_iter],\n",
    "    activation=results['x_iters']['activation'][best_iter],\n",
    "    use_partitions=False\n",
    ")\n",
    "\n",
    "output = model.train_model(dataset)\n",
    "\n",
    "pickle.dump(output, open(octis_folder + \"OCTIS_ProdLDA_output.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546ea2b-a3c8-41fe-936b-691be1c68b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "063c3da8-ee14-471a-9a0a-76348a8c3d4f",
   "metadata": {
    "id": "063c3da8-ee14-471a-9a0a-76348a8c3d4f"
   },
   "source": [
    "### Optuna ProdLDA multi-objective optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZcxyqvLhk4Tj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3221604,
     "status": "ok",
     "timestamp": 1754645898555,
     "user": {
      "displayName": "Giovanna Tonazzo",
      "userId": "15393083850265173052"
     },
     "user_tz": -120
    },
    "id": "ZcxyqvLhk4Tj",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "36d40a5e-5bad-4938-d81e-fe9b8454d887"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 22:03:02,934] A new study created in RDB with name: ProdLDA_Study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 422.110667349298\tTime: 0:00:24.608389\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 410.1606030605864\tTime: 0:00:21.659467\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 406.31787976255123\tTime: 0:00:21.464043\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 404.4543563738316\tTime: 0:00:23.315591\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 403.264892229169\tTime: 0:00:22.471757\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 402.3959376895382\tTime: 0:00:21.106510\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 402.0111157540623\tTime: 0:00:22.316645\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 401.4959139825707\tTime: 0:00:22.660983\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 401.0708883141033\tTime: 0:00:22.463995\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 400.4433144515088\tTime: 0:00:22.273739\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 400.4027887974404\tTime: 0:00:21.884302\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 400.20842082945444\tTime: 0:00:21.790925\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 400.051686001032\tTime: 0:00:22.595815\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 399.81519412384574\tTime: 0:00:21.899002\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 399.6550301233497\tTime: 0:00:21.974267\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 399.4339774913841\tTime: 0:00:21.250934\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 399.53877150744285\tTime: 0:00:22.034369\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 399.3945675414222\tTime: 0:00:22.037591\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 399.2299416060732\tTime: 0:00:22.286131\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 399.06062629003515\tTime: 0:00:22.681348\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 399.11046528541806\tTime: 0:00:21.702456\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 399.00945733157255\tTime: 0:00:20.867219\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 398.877475199882\tTime: 0:00:21.711108\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 398.8145801817098\tTime: 0:00:21.865231\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 398.94762413135675\tTime: 0:00:22.215944\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 398.65739108606266\tTime: 0:00:22.017391\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 398.63039074548163\tTime: 0:00:22.325448\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 398.5282519171274\tTime: 0:00:22.275471\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 398.6308937930795\tTime: 0:00:22.387944\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 398.6730528917358\tTime: 0:00:21.951976\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 398.6011861785221\tTime: 0:00:22.691844\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 398.66480287439816\tTime: 0:00:22.230529\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 398.3003845421462\tTime: 0:00:21.492797\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 398.37479628317067\tTime: 0:00:22.162227\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 398.3701717715621\tTime: 0:00:21.832657\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 398.46685498535413\tTime: 0:00:21.768579\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 398.2697581081208\tTime: 0:00:22.180400\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 398.2504831387408\tTime: 0:00:21.794869\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 398.13945952375076\tTime: 0:00:21.117549\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 398.211942750355\tTime: 0:00:22.171759\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 398.0935631432544\tTime: 0:00:22.098004\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 398.1742629976186\tTime: 0:00:21.938227\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 398.1948356424669\tTime: 0:00:21.862167\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 398.2073388087833\tTime: 0:00:21.842996\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 398.15676954035956\tTime: 0:00:21.103544\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 398.2555258465452\tTime: 0:00:19.702010\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 398.1219191594613\tTime: 0:00:19.589245\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 398.1127739047337\tTime: 0:00:19.703862\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 398.0397414948775\tTime: 0:00:19.675190\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 398.0142077244602\tTime: 0:00:19.774743\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 398.0064027912952\tTime: 0:00:20.008801\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 398.0269510826892\tTime: 0:00:19.519299\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 397.80239313278133\tTime: 0:00:19.939042\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 398.0095829557262\tTime: 0:00:19.521160\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 397.8991110952428\tTime: 0:00:20.406483\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 397.96293576888445\tTime: 0:00:26.948034\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 397.8879509319402\tTime: 0:00:20.197023\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 397.8970914114807\tTime: 0:00:20.445221\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 397.72942926522654\tTime: 0:00:19.701756\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 397.82431932278735\tTime: 0:00:20.252286\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 398.06722041940543\tTime: 0:00:19.632549\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 397.7413477282451\tTime: 0:00:26.245232\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 397.65672399242084\tTime: 0:00:19.585522\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 397.59116489734964\tTime: 0:00:19.819708\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 397.7828116331199\tTime: 0:00:19.634963\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 397.5699972641851\tTime: 0:00:28.801586\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 397.65386765914155\tTime: 0:00:21.313505\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 397.495279470398\tTime: 0:00:22.086952\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 397.5964660162421\tTime: 0:00:21.224741\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 397.6428221003301\tTime: 0:00:21.460788\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 397.67169706918116\tTime: 0:00:20.979822\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 397.7222533711352\tTime: 0:00:21.173554\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 397.44184564068024\tTime: 0:00:21.144579\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 397.581834108236\tTime: 0:00:21.708432\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 397.4546553005459\tTime: 0:00:22.014883\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 397.5397541674895\tTime: 0:00:22.012283\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 397.58119963279546\tTime: 0:00:22.523757\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 397.63984910555604\tTime: 0:00:22.086387\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 397.42920652845953\tTime: 0:00:21.996148\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 397.5606198986309\tTime: 0:00:22.362263\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 397.4358680614327\tTime: 0:00:22.472609\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 397.4570392576215\tTime: 0:00:22.360565\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 397.5879917487705\tTime: 0:00:24.191042\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 397.43059493248325\tTime: 0:00:23.099717\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 397.51023131594167\tTime: 0:00:21.973318\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 397.74199810873205\tTime: 0:00:22.591989\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 397.38496846025635\tTime: 0:00:22.848438\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 397.5521863671787\tTime: 0:00:21.807458\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 397.51892803366906\tTime: 0:00:22.548879\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 397.36254476921596\tTime: 0:00:22.751258\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 397.33284799066\tTime: 0:00:22.997950\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 397.45306762429004\tTime: 0:00:23.589191\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 397.16828663053394\tTime: 0:00:22.824639\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 397.4625016309101\tTime: 0:00:22.187589\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 397.5905114680894\tTime: 0:00:22.813982\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 397.4086367854557\tTime: 0:00:22.854224\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 397.400507323227\tTime: 0:00:23.237162\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 397.38677318713326\tTime: 0:00:23.213218\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 397.3990075104143\tTime: 0:00:21.539763\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 397.2996161410728\tTime: 0:00:22.945675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 22:40:46,735] Trial 0 finished with values: [0.030310650198585733, 0.9842105263157894] and parameters: {'num_topics': 19, 'dropout': 0.20138708349811507, 'num_neurons': 300, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 424.3356337613664\tTime: 0:00:14.580425\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 415.51297521193015\tTime: 0:00:11.899327\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 413.02796595688284\tTime: 0:00:12.639376\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 411.6510305368074\tTime: 0:00:12.408113\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 410.8688356374713\tTime: 0:00:12.876275\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 410.2421379482505\tTime: 0:00:12.397923\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 409.52713239287124\tTime: 0:00:12.313817\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 409.19875280193287\tTime: 0:00:12.396397\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 408.9437831177593\tTime: 0:00:11.805877\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 408.59055525288215\tTime: 0:00:12.740643\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 408.29346191663376\tTime: 0:00:11.392872\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 408.2737452424443\tTime: 0:00:11.968659\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 408.132109298597\tTime: 0:00:11.644499\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 407.9853078879039\tTime: 0:00:11.725313\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 407.8234214406784\tTime: 0:00:11.620136\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 407.7299212196974\tTime: 0:00:11.504892\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 407.704475606581\tTime: 0:00:11.954450\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 407.72421553225854\tTime: 0:00:12.480011\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 407.54389939723916\tTime: 0:00:11.864371\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 407.4824002397291\tTime: 0:00:11.986991\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 407.37317947827336\tTime: 0:00:12.200373\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 407.44798050724535\tTime: 0:00:12.560201\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 407.4253573235796\tTime: 0:00:13.363355\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 407.2762322700975\tTime: 0:00:13.529071\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 407.4681462694181\tTime: 0:00:13.524416\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 407.3120393413709\tTime: 0:00:14.922483\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 407.16628784739856\tTime: 0:00:15.107162\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 407.2213374535117\tTime: 0:00:14.095544\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 407.23626781071334\tTime: 0:00:14.122872\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 407.0249267339366\tTime: 0:00:13.975101\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 407.1281636716693\tTime: 0:00:14.356404\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 407.1065210543143\tTime: 0:00:13.486640\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 407.17164164027827\tTime: 0:00:14.444173\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 406.9960281830075\tTime: 0:00:13.505872\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 407.1491906572068\tTime: 0:00:13.905084\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 406.97577378601517\tTime: 0:00:14.815685\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 407.0401638050807\tTime: 0:00:15.249963\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 406.98594107771714\tTime: 0:00:14.398258\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 407.0008512689362\tTime: 0:00:14.408057\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 406.8926054537633\tTime: 0:00:16.009659\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 407.14086177581714\tTime: 0:00:15.671635\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 407.01563624005587\tTime: 0:00:15.075716\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 406.94448800884396\tTime: 0:00:15.183136\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 406.9949527374017\tTime: 0:00:15.194373\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 406.8600011739614\tTime: 0:00:15.488665\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 406.9481080414739\tTime: 0:00:28.040148\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 406.8291425189127\tTime: 0:00:33.383822\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 406.9728258058753\tTime: 0:00:13.606693\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 406.9342081696546\tTime: 0:00:13.456875\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 407.0371177499436\tTime: 0:00:13.427019\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 406.86095386052574\tTime: 0:00:14.271219\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 406.9070148380492\tTime: 0:00:14.877698\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 406.878227512248\tTime: 0:00:13.383277\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 406.81760004678216\tTime: 0:00:13.073754\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 406.9726318598134\tTime: 0:00:12.612918\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 406.66109450080637\tTime: 0:00:13.098027\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 406.8270567171749\tTime: 0:00:13.071894\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 406.8488851260414\tTime: 0:00:12.929060\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 406.83079381535396\tTime: 0:00:12.931285\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 406.58101769816585\tTime: 0:00:13.001347\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 406.7047349727065\tTime: 0:00:13.098487\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 406.6864840605723\tTime: 0:00:12.645127\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 406.7166339309223\tTime: 0:00:12.913576\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 406.4640160631418\tTime: 0:00:13.127073\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 406.7437074787952\tTime: 0:00:12.976715\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 406.7953649021689\tTime: 0:00:13.026588\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 406.77152951888445\tTime: 0:00:13.669261\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 406.6178346847848\tTime: 0:00:12.836646\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 406.73710618664427\tTime: 0:00:12.981498\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 406.58853156530927\tTime: 0:00:21.531314\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 406.65252575809404\tTime: 0:00:12.785307\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 406.6023816649858\tTime: 0:00:12.815601\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 406.5620945866128\tTime: 0:00:12.820044\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 406.5511365606546\tTime: 0:00:13.060072\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 406.6864518831575\tTime: 0:00:13.063890\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 406.43945319846443\tTime: 0:00:13.144438\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 406.6413703699786\tTime: 0:00:12.908046\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 406.597500468703\tTime: 0:00:13.009699\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 406.60290987414663\tTime: 0:00:13.655165\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 406.50628348508144\tTime: 0:00:12.969314\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 406.5035393686997\tTime: 0:00:12.947991\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 406.41103543541476\tTime: 0:00:12.992617\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 406.497034755722\tTime: 0:00:12.990628\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 406.45946281201805\tTime: 0:00:12.880684\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 406.5227373106381\tTime: 0:00:13.081661\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 406.6240788664029\tTime: 0:00:12.944971\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 406.57998879226847\tTime: 0:00:13.023879\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 406.592028691969\tTime: 0:00:12.892670\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 406.5786985367063\tTime: 0:00:12.897516\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 406.5738566071546\tTime: 0:00:12.703644\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 406.4075035600857\tTime: 0:00:12.986891\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 406.6993436395095\tTime: 0:00:13.824890\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 406.6051259282964\tTime: 0:00:19.887269\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 406.4215531891197\tTime: 0:00:13.511452\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 406.58118464605434\tTime: 0:00:13.138603\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 406.3644628884211\tTime: 0:00:13.733256\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 406.5183208869917\tTime: 0:00:12.860154\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 406.54818182178826\tTime: 0:00:13.344491\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 406.48255580063284\tTime: 0:00:12.790527\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 406.3896387401969\tTime: 0:00:12.976882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 23:04:27,680] Trial 1 finished with values: [-0.003990636319932234, 0.915] and parameters: {'num_topics': 20, 'dropout': 0.45703562599863395, 'num_neurons': 50, 'num_layers': 1, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 424.81051605667693\tTime: 0:00:13.834156\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 415.0921675397178\tTime: 0:00:12.579149\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 413.5501647733417\tTime: 0:00:11.831077\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 412.9319410210026\tTime: 0:00:11.957262\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 412.09935871440797\tTime: 0:00:11.990128\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 411.84363348543405\tTime: 0:00:18.715786\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 411.10930278447773\tTime: 0:00:12.057881\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 410.9579253119593\tTime: 0:00:11.923338\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 410.9293189657503\tTime: 0:00:11.514043\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 410.366135416079\tTime: 0:00:11.779045\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 410.31104389851566\tTime: 0:00:12.497547\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 410.28417957730335\tTime: 0:00:11.795857\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 410.2460826201408\tTime: 0:00:11.964436\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 410.2013031559138\tTime: 0:00:11.732698\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 410.3228059557897\tTime: 0:00:12.122494\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 409.8362904597932\tTime: 0:00:12.012726\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 409.82992609038826\tTime: 0:00:18.989568\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 409.8263424226449\tTime: 0:00:11.793970\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 409.88456863016484\tTime: 0:00:11.706094\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 409.7997979287736\tTime: 0:00:11.682889\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 409.8360572102681\tTime: 0:00:12.227820\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 409.91167894697867\tTime: 0:00:11.805981\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 409.66574020689933\tTime: 0:00:12.371861\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 409.7592303999873\tTime: 0:00:11.817505\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 409.81920157108067\tTime: 0:00:11.996723\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 409.56360916577336\tTime: 0:00:12.074136\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 409.4484487694416\tTime: 0:00:12.084232\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 409.44481070129723\tTime: 0:00:11.697891\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 409.58475512693474\tTime: 0:00:11.794014\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 409.3549767220647\tTime: 0:00:11.842693\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 409.5721955032136\tTime: 0:00:12.125024\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 409.5862485483431\tTime: 0:00:11.887738\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 409.5488464911043\tTime: 0:00:18.352353\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 409.4990957632189\tTime: 0:00:11.764318\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 409.4123823687749\tTime: 0:00:12.086494\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 409.6297467563991\tTime: 0:00:11.873277\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 409.4305205864929\tTime: 0:00:12.497341\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 409.408258572269\tTime: 0:00:11.885952\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 409.54403574719777\tTime: 0:00:11.803424\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 409.57822810730977\tTime: 0:00:11.852800\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 409.3824606054408\tTime: 0:00:20.010655\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 409.4782004993817\tTime: 0:00:12.062314\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 409.52800845604827\tTime: 0:00:11.655855\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 409.3688236260391\tTime: 0:00:11.720810\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 409.4679372998829\tTime: 0:00:11.840563\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 409.5080109273913\tTime: 0:00:11.876067\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 409.35574894328784\tTime: 0:00:11.642198\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 409.58219606742034\tTime: 0:00:11.706932\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 409.406869415235\tTime: 0:00:18.546384\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 409.6527291810657\tTime: 0:00:12.394174\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 409.31920847673587\tTime: 0:00:11.862772\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 409.44253146774855\tTime: 0:00:11.808103\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 409.3807391872132\tTime: 0:00:11.883579\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 409.3297434211145\tTime: 0:00:11.916219\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 409.39429429052177\tTime: 0:00:12.090113\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 409.3775910532682\tTime: 0:00:11.945463\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 408.9998319501453\tTime: 0:00:11.619524\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 409.39920865598907\tTime: 0:00:11.777786\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 409.405766824527\tTime: 0:00:18.936499\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 409.24498539233525\tTime: 0:00:12.271881\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 409.27244929338985\tTime: 0:00:11.980870\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 409.4010619061136\tTime: 0:00:12.266979\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 409.3360325996883\tTime: 0:00:12.025139\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 409.46903371957984\tTime: 0:00:12.048212\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 409.2603564258444\tTime: 0:00:11.918794\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 409.2489286684016\tTime: 0:00:11.974572\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 409.33239038080427\tTime: 0:00:12.005836\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 409.0884854663874\tTime: 0:00:18.562978\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 409.45773981430546\tTime: 0:00:12.005469\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 409.18490486945666\tTime: 0:00:12.046867\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 409.2013276930292\tTime: 0:00:12.336274\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 409.1843831618968\tTime: 0:00:11.745110\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 408.9275033661396\tTime: 0:00:12.001000\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 409.2237816514096\tTime: 0:00:18.938391\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 409.06017514504816\tTime: 0:00:12.039615\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 409.13593018294404\tTime: 0:00:12.325553\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 409.3460333108238\tTime: 0:00:11.818995\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 409.3829568575801\tTime: 0:00:12.217402\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 409.2018127051127\tTime: 0:00:11.950098\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 409.36001759325865\tTime: 0:00:12.188549\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 409.0446308089784\tTime: 0:00:12.376990\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 409.09692917266136\tTime: 0:00:11.956906\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 409.263155787468\tTime: 0:00:12.583538\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 409.0289744396722\tTime: 0:00:18.829108\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 409.19591975687393\tTime: 0:00:11.867959\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 409.1931072451196\tTime: 0:00:12.143591\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 409.1684168462137\tTime: 0:00:13.043521\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 409.18098286133915\tTime: 0:00:12.355246\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 409.2609758410794\tTime: 0:00:12.663076\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 409.30806458580173\tTime: 0:00:12.149389\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 409.32943487056156\tTime: 0:00:12.153864\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 409.2304650767791\tTime: 0:00:12.345867\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 409.13187648985837\tTime: 0:00:19.098683\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 408.94044812706875\tTime: 0:00:11.931528\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 409.2621637607082\tTime: 0:00:11.990473\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 409.1164380527463\tTime: 0:00:12.072473\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 409.2531620921843\tTime: 0:00:12.181109\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 408.9667822188487\tTime: 0:00:12.185491\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 409.2714907664043\tTime: 0:00:12.326580\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 409.35579129552445\tTime: 0:00:18.941350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 23:26:33,602] Trial 2 finished with values: [-0.03284730826914241, 0.91] and parameters: {'num_topics': 10, 'dropout': 0.5582377178350553, 'num_neurons': 100, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 443.67042859581875\tTime: 0:00:13.099941\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 426.83665267610303\tTime: 0:00:11.554904\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 423.0755272982021\tTime: 0:00:11.855371\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 420.89350076461767\tTime: 0:00:11.554806\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 419.77932795044853\tTime: 0:00:11.660808\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 418.84737337526093\tTime: 0:00:11.983233\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 418.020652684331\tTime: 0:00:11.546026\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 417.65202293088936\tTime: 0:00:11.743834\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 417.393177117127\tTime: 0:00:11.494567\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 417.01337849484923\tTime: 0:00:11.571585\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 416.8597727730878\tTime: 0:00:11.496367\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 416.5671213159064\tTime: 0:00:12.068717\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 416.54069421817405\tTime: 0:00:17.744823\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 416.29971043265255\tTime: 0:00:11.434093\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 416.1528963860796\tTime: 0:00:11.917899\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 416.0933178822971\tTime: 0:00:11.413255\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 416.1057922505619\tTime: 0:00:11.878494\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 415.91904371808704\tTime: 0:00:11.344291\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 416.06881911532383\tTime: 0:00:12.039995\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 415.84428346186667\tTime: 0:00:11.528876\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 415.7406312665148\tTime: 0:00:11.494799\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 415.75141646743117\tTime: 0:00:11.729381\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 415.6689774164504\tTime: 0:00:11.627786\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 415.43691169694574\tTime: 0:00:11.719804\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 415.502753666756\tTime: 0:00:11.782397\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 415.40255922113613\tTime: 0:00:11.410446\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 415.62792682238774\tTime: 0:00:18.420218\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 415.4438942326906\tTime: 0:00:11.931978\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 415.66918337394446\tTime: 0:00:11.554623\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 415.3182349997414\tTime: 0:00:11.708184\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 415.55406352997113\tTime: 0:00:11.468604\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 415.3494140331142\tTime: 0:00:12.881533\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 415.3552922150051\tTime: 0:00:20.062932\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 415.29640732551906\tTime: 0:00:12.641618\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 415.34319714352716\tTime: 0:00:12.007570\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 415.17015020240916\tTime: 0:00:11.929296\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 415.2577419374271\tTime: 0:00:18.768071\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 415.3069388170701\tTime: 0:00:11.429218\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 415.0998066269606\tTime: 0:00:11.606974\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 415.06209579877566\tTime: 0:00:11.556905\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 415.08117043068955\tTime: 0:00:11.718256\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 414.998137530091\tTime: 0:00:11.665285\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 414.9042643302631\tTime: 0:00:11.542952\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 415.08618929929054\tTime: 0:00:11.822662\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 415.1324216692996\tTime: 0:00:11.446632\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 415.0678965859028\tTime: 0:00:18.963173\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 415.07358897628217\tTime: 0:00:11.472406\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 414.8867080609273\tTime: 0:00:11.402807\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 414.91209321282815\tTime: 0:00:11.976424\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 414.79741507366424\tTime: 0:00:11.794231\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 415.0196831376829\tTime: 0:00:11.826692\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 414.80526199174375\tTime: 0:00:11.770666\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 414.77957196251083\tTime: 0:00:11.632616\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 415.0647455133811\tTime: 0:00:11.321494\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 414.8442179683386\tTime: 0:00:11.487593\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 414.85345575149984\tTime: 0:00:11.825129\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 414.86580056526464\tTime: 0:00:18.437745\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 414.7965219667424\tTime: 0:00:12.272600\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 414.9000817805894\tTime: 0:00:11.506190\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 414.9121601389124\tTime: 0:00:11.973886\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 414.8378394203011\tTime: 0:00:11.675746\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 414.81281035043116\tTime: 0:00:11.774154\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 414.6746808264923\tTime: 0:00:15.177476\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 414.67002898612054\tTime: 0:00:15.419317\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 414.70398534179174\tTime: 0:00:11.678663\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 414.6103155282033\tTime: 0:00:11.791994\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 414.8073815503813\tTime: 0:00:11.483792\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 414.67402545042506\tTime: 0:00:11.541286\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 414.55635646463367\tTime: 0:00:11.855493\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 414.6143613973285\tTime: 0:00:11.750475\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 414.7392253779597\tTime: 0:00:18.306020\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 414.62740309455637\tTime: 0:00:12.306324\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 414.57854191056856\tTime: 0:00:11.811864\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 414.47000256831603\tTime: 0:00:11.405470\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 414.5479832842003\tTime: 0:00:11.977839\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 414.6464944396252\tTime: 0:00:11.601536\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 414.60146464539605\tTime: 0:00:11.397322\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 414.5338357638301\tTime: 0:00:11.339569\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 414.58621581994515\tTime: 0:00:19.898756\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 414.5290899257363\tTime: 0:00:11.568541\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 414.4012800954332\tTime: 0:00:11.615438\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 414.39959203005105\tTime: 0:00:11.911498\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 414.6463375196297\tTime: 0:00:11.675330\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 414.5471949375376\tTime: 0:00:11.620603\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 414.5760617518384\tTime: 0:00:12.448501\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 414.33139541546774\tTime: 0:00:11.541254\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 414.64387561680724\tTime: 0:00:11.588449\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 414.4265107883966\tTime: 0:00:11.976532\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 414.33945975149044\tTime: 0:00:11.781425\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 414.5463215558118\tTime: 0:00:18.248450\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 414.4477560734503\tTime: 0:00:11.575898\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 414.2460801590828\tTime: 0:00:11.753160\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 414.3622687266677\tTime: 0:00:11.899879\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 414.5141277217097\tTime: 0:00:11.521185\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 414.387036116283\tTime: 0:00:11.493798\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 414.38861956833483\tTime: 0:00:12.481982\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 414.41969567805893\tTime: 0:00:20.950209\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 414.41219837714164\tTime: 0:00:11.914881\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 414.30973127303844\tTime: 0:00:11.821828\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 414.32924096123196\tTime: 0:00:11.792705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 23:48:14,985] Trial 3 finished with values: [-0.013398629182610847, 0.7627906976744186] and parameters: {'num_topics': 43, 'dropout': 0.5880701441619546, 'num_neurons': 50, 'num_layers': 1, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 427.3327668859433\tTime: 0:00:25.133505\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 417.4972098581608\tTime: 0:00:24.052715\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 413.5679756336159\tTime: 0:00:23.097059\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 410.5752629732284\tTime: 0:00:29.399144\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 409.1527912584984\tTime: 0:00:22.180403\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 408.17424283163604\tTime: 0:00:22.472501\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 407.40872457543446\tTime: 0:00:22.024775\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 406.7685313992797\tTime: 0:00:28.893719\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 406.4794176945808\tTime: 0:00:22.247935\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 406.02086675964324\tTime: 0:00:21.846270\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 405.79956963809667\tTime: 0:00:21.969248\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 405.6174673259657\tTime: 0:00:29.484392\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 405.2585317533782\tTime: 0:00:21.869083\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 405.11048758186877\tTime: 0:00:21.899013\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 404.8401428603753\tTime: 0:00:22.463035\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 404.50009462216957\tTime: 0:00:21.295939\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 404.4062926460943\tTime: 0:00:28.794173\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 404.26901633789873\tTime: 0:00:21.908385\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 404.09220320669243\tTime: 0:00:21.726992\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 403.96071012762826\tTime: 0:00:23.515189\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 403.84051712484955\tTime: 0:00:31.909854\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 403.861767478948\tTime: 0:00:26.830307\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 403.7376000144578\tTime: 0:00:34.729340\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 403.64360445964275\tTime: 0:00:28.497398\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 403.5901013529795\tTime: 0:00:36.247648\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 403.23464942633103\tTime: 0:00:30.269203\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 403.34414626706723\tTime: 0:00:30.340944\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 403.2418442375099\tTime: 0:00:37.878933\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 403.25197883754606\tTime: 0:00:30.676535\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 403.0890541911744\tTime: 0:00:30.501753\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 403.17103328143804\tTime: 0:00:37.720430\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 402.9943488231588\tTime: 0:00:30.641734\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 402.8659812142669\tTime: 0:00:39.368575\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 402.7332656138334\tTime: 0:00:31.223107\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 402.744943627814\tTime: 0:00:38.188266\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 402.80489371461954\tTime: 0:00:31.592012\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 402.7517741289471\tTime: 0:00:31.386249\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 402.5577860087311\tTime: 0:00:39.000952\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 402.7319133175708\tTime: 0:00:32.057978\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 402.57884017610303\tTime: 0:00:31.368078\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 402.58102915861497\tTime: 0:00:32.161629\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 402.5790578879039\tTime: 0:00:39.518959\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 402.53131461195505\tTime: 0:00:32.144109\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 402.4560470104684\tTime: 0:00:39.198166\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 402.5432664645514\tTime: 0:00:32.403372\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 402.3127288049279\tTime: 0:00:32.204880\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 402.37452663202674\tTime: 0:00:39.673622\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 402.40337008464275\tTime: 0:00:32.715274\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 402.219525196532\tTime: 0:00:40.346182\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 402.0586356981823\tTime: 0:00:34.384648\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 402.211701603405\tTime: 0:00:39.714617\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 402.136259156605\tTime: 0:00:33.298457\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 402.11505089761766\tTime: 0:00:32.899125\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 402.02502521298806\tTime: 0:00:40.173923\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 402.0324084236652\tTime: 0:00:33.775472\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 402.1554558496483\tTime: 0:00:33.216716\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 402.17181593460845\tTime: 0:00:40.389436\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 402.0263112810784\tTime: 0:00:32.845485\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 401.78715701667466\tTime: 0:00:33.874653\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 401.9352675632852\tTime: 0:00:33.851528\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 401.8730459199626\tTime: 0:00:40.650197\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 401.67547820927837\tTime: 0:00:34.195710\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 401.8498448651899\tTime: 0:00:33.397084\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 401.70606295224695\tTime: 0:00:40.425082\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 401.84898173175026\tTime: 0:00:34.112787\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 401.64549444197604\tTime: 0:00:33.779781\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 401.7359041251152\tTime: 0:00:42.212746\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 401.6548194274007\tTime: 0:00:33.797418\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 401.82596437475314\tTime: 0:00:40.587011\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 401.71590898405174\tTime: 0:00:33.654828\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 401.64012121775795\tTime: 0:00:35.861942\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 401.6867709391221\tTime: 0:00:38.266301\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 401.6861820483525\tTime: 0:00:33.899558\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 401.6671073797064\tTime: 0:00:34.512874\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 401.51857404537395\tTime: 0:00:40.808380\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 401.72366734077616\tTime: 0:00:33.667521\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 401.5092570675708\tTime: 0:00:34.083317\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 401.6272425748044\tTime: 0:00:41.035830\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 401.6159811040703\tTime: 0:00:34.531905\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 401.6977594160813\tTime: 0:00:34.223165\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 401.33541252327353\tTime: 0:00:35.093819\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 401.46508809265214\tTime: 0:00:41.260403\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 401.56050757153673\tTime: 0:00:35.149170\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 401.4144927076281\tTime: 0:00:44.194485\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 401.27189445337774\tTime: 0:00:34.700279\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 401.54372392747825\tTime: 0:00:34.624642\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 401.3691927847363\tTime: 0:00:41.209457\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 401.3626245883054\tTime: 0:00:34.984972\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 401.28283315819414\tTime: 0:00:34.992089\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 401.1828571957515\tTime: 0:00:41.950820\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 401.2898597358572\tTime: 0:00:35.139069\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 401.3304876156624\tTime: 0:00:34.460295\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 401.42847181082146\tTime: 0:00:42.346264\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 401.301163742489\tTime: 0:00:36.219173\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 401.15914593060023\tTime: 0:00:42.962730\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 401.43604588205494\tTime: 0:00:34.273730\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 401.20284572501737\tTime: 0:00:41.743414\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 401.3468388481602\tTime: 0:00:34.642736\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 401.31323857451997\tTime: 0:00:42.309191\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 401.2114377192178\tTime: 0:00:35.636017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 00:44:53,327] Trial 4 finished with values: [0.009075721183529242, 0.9628571428571429] and parameters: {'num_topics': 35, 'dropout': 0.2750981101173346, 'num_neurons': 300, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 442.56136321160574\tTime: 0:00:15.622944\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 425.79693006158084\tTime: 0:00:13.829149\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 422.24221012704055\tTime: 0:00:13.288090\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 419.9251064058621\tTime: 0:00:13.388614\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 418.49935152958795\tTime: 0:00:13.409805\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 417.16149018015824\tTime: 0:00:13.399751\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 416.5575041624939\tTime: 0:00:13.585025\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 416.08118045858254\tTime: 0:00:13.350619\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 415.32176305492084\tTime: 0:00:13.626445\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 415.0676385421368\tTime: 0:00:13.282521\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 414.9851155947914\tTime: 0:00:14.075106\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 414.7522446685403\tTime: 0:00:13.291999\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 414.68355117737013\tTime: 0:00:20.232403\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 414.23007409914993\tTime: 0:00:13.472284\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 414.36984007971773\tTime: 0:00:13.367562\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 414.2474861578345\tTime: 0:00:15.339186\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 413.98861455806156\tTime: 0:00:13.633570\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 413.9135416740131\tTime: 0:00:20.460002\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 413.81550741285355\tTime: 0:00:13.264894\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 413.6883511587396\tTime: 0:00:13.471906\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 413.9213973343583\tTime: 0:00:13.357894\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 413.752368529548\tTime: 0:00:13.487306\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 413.66409871795776\tTime: 0:00:13.647039\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 413.6713131074038\tTime: 0:00:13.764812\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 413.6692983457576\tTime: 0:00:13.512756\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 413.5161327125884\tTime: 0:00:13.795552\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 413.64249297974027\tTime: 0:00:13.468669\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 413.6339337139378\tTime: 0:00:13.248226\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 413.60287560299594\tTime: 0:00:13.676775\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 413.4177698818222\tTime: 0:00:13.606552\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 413.5163456492021\tTime: 0:00:20.311027\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 413.3789362969467\tTime: 0:00:13.532373\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 413.359368314738\tTime: 0:00:13.372308\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 413.50733913202674\tTime: 0:00:14.435772\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 413.56696083288665\tTime: 0:00:20.968405\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 413.46450243431696\tTime: 0:00:14.099478\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 413.2109242396727\tTime: 0:00:14.648884\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 413.2800192506159\tTime: 0:00:15.918803\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 413.34696311322216\tTime: 0:00:21.014276\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 413.2063566997198\tTime: 0:00:13.612186\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 413.07906505068456\tTime: 0:00:13.382635\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 413.11236948311614\tTime: 0:00:13.474138\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 413.30767702426795\tTime: 0:00:13.619142\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 413.2542662991095\tTime: 0:00:13.484550\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 413.3240577159971\tTime: 0:00:13.557203\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 413.2188417937895\tTime: 0:00:13.401545\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 413.2057275871817\tTime: 0:00:13.799524\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 413.05030924111844\tTime: 0:00:13.391608\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 413.19792834750905\tTime: 0:00:20.252879\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 412.99071718315093\tTime: 0:00:13.932799\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 413.3053018829224\tTime: 0:00:13.454469\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 413.09102836372995\tTime: 0:00:13.505633\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 413.02645663042813\tTime: 0:00:13.246628\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 412.96065286211496\tTime: 0:00:13.717552\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 413.0347618195432\tTime: 0:00:13.305519\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 413.0702206694783\tTime: 0:00:20.192054\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 412.994022163627\tTime: 0:00:13.190379\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 412.99706899014046\tTime: 0:00:13.411954\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 412.9370788652275\tTime: 0:00:13.607596\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 413.03636903733405\tTime: 0:00:13.651483\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 412.9378606000926\tTime: 0:00:13.291109\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 412.85437629738163\tTime: 0:00:13.456065\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 413.01221948346875\tTime: 0:00:20.520531\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 413.0271095087052\tTime: 0:00:14.132373\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 413.0772554384239\tTime: 0:00:13.432159\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 412.9122050991358\tTime: 0:00:13.711495\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 412.96330937217897\tTime: 0:00:13.689558\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 412.84505678505604\tTime: 0:00:13.816831\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 412.791410790101\tTime: 0:00:20.343457\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 413.100173177671\tTime: 0:00:13.661169\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 412.65361659449525\tTime: 0:00:13.463524\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 412.67226201055064\tTime: 0:00:13.486242\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 412.92942552588767\tTime: 0:00:13.307188\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 412.5383793543829\tTime: 0:00:13.732927\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 412.6550705287792\tTime: 0:00:21.133606\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 412.7122424117134\tTime: 0:00:13.606694\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 412.5925067249328\tTime: 0:00:13.938718\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 412.8877966199315\tTime: 0:00:13.313793\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 412.73820073299856\tTime: 0:00:13.808466\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 412.67284619959753\tTime: 0:00:13.598777\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 412.5939016305574\tTime: 0:00:20.282441\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 412.7144493195432\tTime: 0:00:13.935365\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 412.7587878503371\tTime: 0:00:13.908932\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 412.7579906879443\tTime: 0:00:13.768620\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 412.74106059256985\tTime: 0:00:13.459367\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 412.58345601891034\tTime: 0:00:13.705090\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 412.7888646970562\tTime: 0:00:13.584528\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 412.57918949940756\tTime: 0:00:13.566434\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 412.5478892130139\tTime: 0:00:20.911544\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 412.45276553860583\tTime: 0:00:13.429681\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 412.848033820373\tTime: 0:00:13.509434\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 412.88590535197096\tTime: 0:00:13.416690\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 412.4573278625851\tTime: 0:00:20.096369\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 412.65570107387344\tTime: 0:00:13.746558\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 412.47782774092803\tTime: 0:00:13.654994\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 412.71800668702514\tTime: 0:00:13.769810\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 412.5116294540007\tTime: 0:00:13.822948\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 412.39927627998526\tTime: 0:00:13.688611\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 412.54422315492644\tTime: 0:00:13.863505\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 412.6103104958907\tTime: 0:00:20.181856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 01:10:07,005] Trial 5 finished with values: [-0.015170787215110804, 0.8131578947368421] and parameters: {'num_topics': 38, 'dropout': 0.5896432285558239, 'num_neurons': 100, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 421.18180856465807\tTime: 0:00:12.242895\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 408.79989681187936\tTime: 0:00:11.251981\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 404.80758607531925\tTime: 0:00:11.456262\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 403.40059867622995\tTime: 0:00:11.324232\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 402.4532883113998\tTime: 0:00:11.441377\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 401.74210848901913\tTime: 0:00:11.428178\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 401.34768350529885\tTime: 0:00:11.573401\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 401.0047350755567\tTime: 0:00:11.578340\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 400.68348652868286\tTime: 0:00:11.215528\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 400.42194879500727\tTime: 0:00:11.015650\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 400.21673501796056\tTime: 0:00:11.394150\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 399.9717398181256\tTime: 0:00:10.917014\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 399.7983838122038\tTime: 0:00:11.180708\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 399.74695669977854\tTime: 0:00:11.071240\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 399.6109471752638\tTime: 0:00:18.906285\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 399.489234230422\tTime: 0:00:10.991302\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 399.2931453951916\tTime: 0:00:11.150651\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 399.1650143417236\tTime: 0:00:11.149937\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 399.1033466935723\tTime: 0:00:11.035445\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 399.0169608953314\tTime: 0:00:12.548207\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 398.85548520602947\tTime: 0:00:12.518586\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 398.74771066509396\tTime: 0:00:13.137779\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 398.6425530736337\tTime: 0:00:13.323221\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 398.65196996304445\tTime: 0:00:13.649398\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 398.45944881704656\tTime: 0:00:13.093289\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 398.41184339707695\tTime: 0:00:13.384976\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 398.30866427584067\tTime: 0:00:13.136146\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 398.14764955739156\tTime: 0:00:13.223032\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 398.25993466662436\tTime: 0:00:15.097990\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 398.2015226308607\tTime: 0:00:18.209122\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 398.1530630401104\tTime: 0:00:13.101439\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 397.97593235896005\tTime: 0:00:13.543059\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 397.99055453293084\tTime: 0:00:13.334297\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 397.92285934972824\tTime: 0:00:13.852351\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 397.87318080063284\tTime: 0:00:12.998169\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 398.0506619437862\tTime: 0:00:12.992735\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 397.87897302915536\tTime: 0:00:13.003859\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 397.82967627463705\tTime: 0:00:13.261780\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 397.8466689984273\tTime: 0:00:13.737818\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 397.9715084419431\tTime: 0:00:13.156609\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 397.71021255312945\tTime: 0:00:13.171284\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 397.76670507386405\tTime: 0:00:20.403347\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 397.79053388208314\tTime: 0:00:13.023500\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 397.5271300787421\tTime: 0:00:13.253510\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 397.6110090690354\tTime: 0:00:13.207701\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 397.61700754890967\tTime: 0:00:13.146159\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 397.5120777706077\tTime: 0:00:13.868368\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 397.4605614723797\tTime: 0:00:13.307932\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 397.7524506260344\tTime: 0:00:20.339855\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 397.49148587808196\tTime: 0:00:13.641318\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 397.41695677765085\tTime: 0:00:13.008334\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 397.4736315268727\tTime: 0:00:13.260408\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 397.3942244258609\tTime: 0:00:13.590068\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 397.29771649816865\tTime: 0:00:14.021505\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 397.5028475910136\tTime: 0:00:13.321841\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 397.500200484395\tTime: 0:00:13.543389\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 397.2648712918101\tTime: 0:00:13.423560\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 397.3668971686226\tTime: 0:00:13.315249\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 397.3133973311259\tTime: 0:00:21.883868\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 397.3597788339022\tTime: 0:00:13.287045\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 397.36565433434185\tTime: 0:00:13.237052\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 397.25699395946174\tTime: 0:00:13.205916\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 397.23116372338825\tTime: 0:00:13.054189\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 397.28129591526084\tTime: 0:00:13.529909\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 397.1808285125983\tTime: 0:00:13.164345\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 397.08197435180875\tTime: 0:00:13.559407\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 397.4295367877478\tTime: 0:00:13.300714\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 397.3834496568918\tTime: 0:00:13.580250\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 397.2172674145932\tTime: 0:00:20.737603\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 397.2509211335971\tTime: 0:00:13.597919\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 397.19654692260497\tTime: 0:00:13.665124\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 397.1636853704135\tTime: 0:00:13.218189\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 397.1961172292278\tTime: 0:00:14.343619\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 397.21279864754945\tTime: 0:00:13.414239\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 397.2371954826436\tTime: 0:00:13.122288\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 397.0916581007156\tTime: 0:00:13.291903\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 397.11866097581907\tTime: 0:00:13.551303\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 397.03148762065797\tTime: 0:00:13.048088\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 397.15459918107746\tTime: 0:00:13.223358\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 397.00858483141974\tTime: 0:00:20.460040\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 396.9525058786227\tTime: 0:00:13.554635\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 397.1546979172544\tTime: 0:00:13.036654\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 397.0354798974907\tTime: 0:00:13.542679\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 397.157123418458\tTime: 0:00:13.507817\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 396.95705339952184\tTime: 0:00:13.091002\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 397.00922882050236\tTime: 0:00:13.932606\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 396.97355354439367\tTime: 0:00:13.112060\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 396.9310647374323\tTime: 0:00:13.698350\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 396.91766218298636\tTime: 0:00:13.467164\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 396.8830211022131\tTime: 0:00:13.516824\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 396.960614330028\tTime: 0:00:13.284534\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 396.94566141925026\tTime: 0:00:20.795706\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 397.0125916909393\tTime: 0:00:13.214750\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 396.942881305304\tTime: 0:00:13.391584\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 396.98461162888833\tTime: 0:00:13.083860\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 396.8687298119781\tTime: 0:00:13.153892\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 396.9170044193255\tTime: 0:00:13.391543\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 396.89025415162115\tTime: 0:00:13.846352\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 396.8782245002069\tTime: 0:00:13.067335\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 396.93692811624305\tTime: 0:00:13.406263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 01:33:27,805] Trial 6 finished with values: [0.04838178970449884, 0.9409090909090909] and parameters: {'num_topics': 22, 'dropout': 0.07548434734218092, 'num_neurons': 50, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 439.95748577435023\tTime: 0:00:13.979254\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 423.21297239089654\tTime: 0:00:11.853821\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 421.1395302699847\tTime: 0:00:18.673322\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 417.236539702522\tTime: 0:00:11.440985\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 414.0313911251458\tTime: 0:00:11.405575\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 412.34324618102573\tTime: 0:00:11.498718\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 411.3935707394752\tTime: 0:00:11.717772\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 410.581199559331\tTime: 0:00:11.161358\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 410.175739889533\tTime: 0:00:11.328492\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 409.9196312864971\tTime: 0:00:11.340007\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 409.55619950790594\tTime: 0:00:18.152387\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 409.2419420186494\tTime: 0:00:11.439487\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 409.08310070825576\tTime: 0:00:11.372445\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 408.88130589172874\tTime: 0:00:11.367804\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 408.81679348094394\tTime: 0:00:11.437465\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 408.57814097851076\tTime: 0:00:11.892140\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 408.2651873724658\tTime: 0:00:18.849032\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 408.34945730365604\tTime: 0:00:11.316305\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 408.15768358464044\tTime: 0:00:11.377083\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 408.0170072330127\tTime: 0:00:11.475445\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 407.79771300860887\tTime: 0:00:11.297139\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 407.8560085665388\tTime: 0:00:11.398146\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 407.791230545153\tTime: 0:00:11.269484\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 407.61505732575415\tTime: 0:00:11.274234\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 407.5725096003301\tTime: 0:00:11.441597\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 407.42711058863216\tTime: 0:00:11.281990\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 407.3788619142241\tTime: 0:00:19.157770\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 407.23152601316247\tTime: 0:00:11.459426\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 407.1963731792579\tTime: 0:00:11.391993\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 407.2445061104764\tTime: 0:00:11.992666\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 407.21862026183896\tTime: 0:00:16.853758\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 407.0039732495486\tTime: 0:00:12.849339\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 407.07798637264443\tTime: 0:00:11.562275\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 406.97776570022756\tTime: 0:00:11.447586\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 406.92986392479827\tTime: 0:00:11.529749\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 406.7821716111158\tTime: 0:00:11.723735\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 406.7205612284779\tTime: 0:00:11.764298\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 406.765326293679\tTime: 0:00:11.135529\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 406.55580434280495\tTime: 0:00:11.235574\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 406.5379196140591\tTime: 0:00:18.318174\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 406.6027196013057\tTime: 0:00:11.703520\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 406.44785561773585\tTime: 0:00:11.262041\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 406.29741635929156\tTime: 0:00:12.087195\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 406.3737425463002\tTime: 0:00:11.311225\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 406.36434523515663\tTime: 0:00:11.469703\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 406.30180325024213\tTime: 0:00:12.247339\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 406.2713843164635\tTime: 0:00:18.270980\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 406.14250767262376\tTime: 0:00:11.264801\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 406.0493426697792\tTime: 0:00:11.509253\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 406.334695143238\tTime: 0:00:11.366177\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 405.9850879721705\tTime: 0:00:11.260338\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 406.1027651125122\tTime: 0:00:11.467990\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 405.9823232490197\tTime: 0:00:11.417942\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 405.83706958667153\tTime: 0:00:11.529665\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 405.97503686444463\tTime: 0:00:11.834965\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 405.8129936808847\tTime: 0:00:20.427061\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 405.77017931635777\tTime: 0:00:11.744331\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 405.90835703295437\tTime: 0:00:11.283049\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 405.84628635900947\tTime: 0:00:11.380418\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 405.7256963177871\tTime: 0:00:11.398712\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 405.7802865141663\tTime: 0:00:11.243226\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 405.71498050401294\tTime: 0:00:11.347830\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 405.7184488693532\tTime: 0:00:11.440726\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 405.66450020099865\tTime: 0:00:18.356561\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 405.72806124757864\tTime: 0:00:11.752118\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 405.65340780862107\tTime: 0:00:11.410117\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 405.7549769938831\tTime: 0:00:11.359979\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 405.7226478750564\tTime: 0:00:11.495228\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 405.5796782949085\tTime: 0:00:11.859002\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 405.5302430232312\tTime: 0:00:19.919686\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 405.3843062519982\tTime: 0:00:12.136089\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 405.5005077125884\tTime: 0:00:11.344453\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 405.57881982645944\tTime: 0:00:11.275650\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 405.39776460266916\tTime: 0:00:11.655384\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 405.3865828775601\tTime: 0:00:11.587614\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 405.4330061815899\tTime: 0:00:11.382354\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 405.4168692815297\tTime: 0:00:11.436906\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 405.3527331554907\tTime: 0:00:11.492318\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 405.41628700255774\tTime: 0:00:11.513075\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 405.3818656171246\tTime: 0:00:18.314683\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 405.35564837050043\tTime: 0:00:11.345508\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 405.3170338565175\tTime: 0:00:12.320276\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 405.1913060445347\tTime: 0:00:11.427497\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 405.21257186289307\tTime: 0:00:11.563892\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 405.2879477509309\tTime: 0:00:11.477639\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 405.32974746165746\tTime: 0:00:11.449422\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 405.1700631838068\tTime: 0:00:11.483909\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 405.2268019940006\tTime: 0:00:21.885646\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 405.2318996531892\tTime: 0:00:11.800491\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 405.1732748277406\tTime: 0:00:11.465811\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 405.2035909113349\tTime: 0:00:11.572073\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 405.11186654571486\tTime: 0:00:11.586036\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 404.9836958581937\tTime: 0:00:12.556463\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 405.09742968573687\tTime: 0:00:18.651537\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 405.09678015009075\tTime: 0:00:11.396197\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 405.1992815988091\tTime: 0:00:11.317482\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 405.12724903967313\tTime: 0:00:11.534202\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 404.9683725398236\tTime: 0:00:11.844714\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 404.8698928271694\tTime: 0:00:11.472228\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 404.90866506925636\tTime: 0:00:11.578277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 01:55:07,892] Trial 7 finished with values: [0.01002298028311595, 0.9166666666666666] and parameters: {'num_topics': 42, 'dropout': 0.2764980570235083, 'num_neurons': 50, 'num_layers': 2, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 422.59135652928234\tTime: 0:00:12.440292\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 412.80337017280004\tTime: 0:00:11.205068\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 410.3204973364741\tTime: 0:00:11.151071\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 409.0354300518835\tTime: 0:00:11.095583\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 408.18617760375525\tTime: 0:00:11.322118\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 407.4643384984696\tTime: 0:00:11.417186\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 407.1449531194167\tTime: 0:00:11.130333\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 406.7592543493874\tTime: 0:00:10.981791\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 406.6010563301644\tTime: 0:00:17.962662\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 406.4462504202165\tTime: 0:00:11.421425\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 406.3183630482209\tTime: 0:00:11.291664\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 406.1765288973755\tTime: 0:00:11.565116\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 405.98880868778446\tTime: 0:00:10.980502\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 405.8682243033222\tTime: 0:00:11.308516\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 405.87415905281443\tTime: 0:00:11.129320\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 405.6967921835623\tTime: 0:00:11.090012\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 405.75057283879437\tTime: 0:00:11.137709\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 405.79851211780874\tTime: 0:00:11.119468\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 405.60245377031146\tTime: 0:00:11.339079\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 405.50879604161963\tTime: 0:00:19.193156\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 405.62652515803666\tTime: 0:00:12.612287\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 405.5382543546768\tTime: 0:00:13.878255\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 405.39571872884227\tTime: 0:00:13.977456\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 405.41394594870184\tTime: 0:00:14.293917\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 405.4734537062505\tTime: 0:00:14.716211\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 405.35291115977395\tTime: 0:00:13.933165\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 405.28908483318287\tTime: 0:00:13.786832\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 405.3341058116819\tTime: 0:00:14.039196\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 405.4146777277632\tTime: 0:00:21.043941\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 405.05323940613715\tTime: 0:00:13.966652\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 405.27533578381247\tTime: 0:00:14.148522\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 405.1497639735199\tTime: 0:00:14.010597\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 405.1637233882495\tTime: 0:00:13.767910\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 405.09351689740373\tTime: 0:00:14.062786\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 405.0127223474056\tTime: 0:00:13.884602\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 404.9789943191435\tTime: 0:00:22.190542\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 405.0656552967257\tTime: 0:00:14.200327\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 405.002446328366\tTime: 0:00:14.550445\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 404.89598746168093\tTime: 0:00:13.886050\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 405.0307275577959\tTime: 0:00:22.817762\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 404.9736611330329\tTime: 0:00:13.757705\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 404.9368780502426\tTime: 0:00:13.763578\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 405.04325360827835\tTime: 0:00:14.191124\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 404.8257280029903\tTime: 0:00:21.092161\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 404.8918250045254\tTime: 0:00:13.771801\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 405.00788978456706\tTime: 0:00:13.917031\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 404.7767984970943\tTime: 0:00:14.031780\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 404.87189825883456\tTime: 0:00:13.918092\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 404.8372743320027\tTime: 0:00:13.835500\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 404.9453057045414\tTime: 0:00:13.846629\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 404.91996228042956\tTime: 0:00:14.280002\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 404.74222026513013\tTime: 0:00:14.346882\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 404.71098620690873\tTime: 0:00:14.103105\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 404.66517596344175\tTime: 0:00:13.533969\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 404.6893537276433\tTime: 0:00:20.730843\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 404.7548265754885\tTime: 0:00:13.822487\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 404.6846736034708\tTime: 0:00:13.709063\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 404.80453660408676\tTime: 0:00:13.773030\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 404.57678548654604\tTime: 0:00:13.615930\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 404.47472265713157\tTime: 0:00:13.706523\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 404.5748217826699\tTime: 0:00:13.807146\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 404.6073402193001\tTime: 0:00:14.166434\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 404.564393986144\tTime: 0:00:13.922318\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 404.70666300960795\tTime: 0:00:13.866872\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 404.6478504091086\tTime: 0:00:14.345783\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 404.73546355900424\tTime: 0:00:13.727686\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 404.69503432698366\tTime: 0:00:13.864162\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 404.5737264649392\tTime: 0:00:20.971518\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 404.4029031815382\tTime: 0:00:14.238333\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 404.45139340695056\tTime: 0:00:13.712674\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 404.5193107465513\tTime: 0:00:14.249637\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 404.4956053218212\tTime: 0:00:13.946221\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 404.4689167274952\tTime: 0:00:13.710789\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 404.443253586239\tTime: 0:00:13.685342\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 404.4145236361478\tTime: 0:00:13.516405\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 404.4038450688332\tTime: 0:00:21.722320\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 404.485890534198\tTime: 0:00:14.231988\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 404.4218928885856\tTime: 0:00:14.144739\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 404.57492143715206\tTime: 0:00:13.725027\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 404.51297646082526\tTime: 0:00:13.953883\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 404.42002759029657\tTime: 0:00:13.847228\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 404.48797835620724\tTime: 0:00:13.933189\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 404.2543763488067\tTime: 0:00:13.698842\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 404.59806181031604\tTime: 0:00:13.973310\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 404.3215991190735\tTime: 0:00:13.994878\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 404.3980716839337\tTime: 0:00:20.890248\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 404.4730381180415\tTime: 0:00:13.746288\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 404.24686946078293\tTime: 0:00:14.117401\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 404.39922140206545\tTime: 0:00:13.593805\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 404.3594129811038\tTime: 0:00:13.956065\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 404.2603994025286\tTime: 0:00:14.363048\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 404.45217091761174\tTime: 0:00:13.904471\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 404.3713749717897\tTime: 0:00:14.159547\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 404.4123166181214\tTime: 0:00:13.873637\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 404.2216822587546\tTime: 0:00:13.515592\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 404.4580133223313\tTime: 0:00:22.365610\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 404.2599207818495\tTime: 0:00:14.332295\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 404.2624193801484\tTime: 0:00:16.211887\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 404.4489748848666\tTime: 0:00:14.955416\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 404.2353546480055\tTime: 0:00:14.230568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 02:19:40,191] Trial 8 finished with values: [-0.015580945667647072, 0.9238095238095239] and parameters: {'num_topics': 21, 'dropout': 0.35233262278556754, 'num_neurons': 50, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 442.5028497214817\tTime: 0:00:13.251888\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 425.2598549209993\tTime: 0:00:11.893321\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 421.70169846794363\tTime: 0:00:11.827840\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 416.621891830698\tTime: 0:00:11.893568\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 414.15668523994066\tTime: 0:00:12.196723\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 412.95479315652506\tTime: 0:00:12.037945\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 412.1842906334631\tTime: 0:00:11.977989\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 411.71998820602244\tTime: 0:00:12.330050\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 411.2066329993935\tTime: 0:00:11.813547\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 410.7130836160244\tTime: 0:00:11.868513\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 410.3043798677523\tTime: 0:00:12.303074\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 410.04113092740306\tTime: 0:00:12.152276\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 409.8345169552937\tTime: 0:00:11.662146\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 409.5083097806411\tTime: 0:00:11.918943\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 409.2185586986572\tTime: 0:00:11.980771\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 409.14117837072376\tTime: 0:00:12.503308\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 408.8817284590574\tTime: 0:00:19.103671\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 408.6495490166347\tTime: 0:00:11.754361\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 408.5873401193885\tTime: 0:00:12.298791\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 408.4115482537802\tTime: 0:00:11.820902\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 408.3064151994353\tTime: 0:00:12.060457\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 408.0165934814141\tTime: 0:00:12.418345\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 407.888799666354\tTime: 0:00:11.867414\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 407.6742135193335\tTime: 0:00:12.011897\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 407.4384515111337\tTime: 0:00:11.812520\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 407.4775746927424\tTime: 0:00:12.187638\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 407.3063113574814\tTime: 0:00:18.789532\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 407.1107942958112\tTime: 0:00:11.851145\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 407.047695781203\tTime: 0:00:16.254822\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 407.0510529948798\tTime: 0:00:12.033702\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 406.96193588143194\tTime: 0:00:11.749649\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 406.81275253393466\tTime: 0:00:11.762411\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 406.88664499172495\tTime: 0:00:11.958749\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 406.64240236138147\tTime: 0:00:11.755993\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 406.571066575512\tTime: 0:00:12.396104\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 406.60519766629994\tTime: 0:00:12.060020\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 406.5155065753592\tTime: 0:00:11.549707\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 406.5429507144855\tTime: 0:00:18.810948\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 406.4165906309947\tTime: 0:00:12.216613\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 406.40429121823985\tTime: 0:00:12.162216\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 406.314944050969\tTime: 0:00:11.780343\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 406.2995516025822\tTime: 0:00:12.029438\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 406.2417851353861\tTime: 0:00:11.999054\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 406.1517069604303\tTime: 0:00:11.532748\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 406.08495682349263\tTime: 0:00:12.245720\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 406.06148086487013\tTime: 0:00:12.167371\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 405.9069391696993\tTime: 0:00:12.273188\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 405.84981735276557\tTime: 0:00:12.399419\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 405.8668247694393\tTime: 0:00:11.862398\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 405.9539662410432\tTime: 0:00:12.210214\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 405.7098444963985\tTime: 0:00:12.272230\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 405.76873298564095\tTime: 0:00:11.914071\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 405.61988702348043\tTime: 0:00:11.832386\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 405.69041462729444\tTime: 0:00:11.955761\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 405.5995310986633\tTime: 0:00:18.858751\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 405.6605812416544\tTime: 0:00:11.750277\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 405.6776866231287\tTime: 0:00:11.855623\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 405.56664860911286\tTime: 0:00:11.917592\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 405.44559317080655\tTime: 0:00:12.123664\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 405.55825419746293\tTime: 0:00:12.218900\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 405.4563312810314\tTime: 0:00:12.459450\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 405.37707985111996\tTime: 0:00:11.619740\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 405.4565686078317\tTime: 0:00:12.003528\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 405.3553061365122\tTime: 0:00:21.086819\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 405.3148365357942\tTime: 0:00:12.064652\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 405.2581822096804\tTime: 0:00:11.854488\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 405.41262024655833\tTime: 0:00:12.765813\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 405.3790629863344\tTime: 0:00:17.570865\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 405.2764596792015\tTime: 0:00:13.463317\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 405.3292318883764\tTime: 0:00:11.891646\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 405.20053295496217\tTime: 0:00:12.135651\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 405.3004906247649\tTime: 0:00:11.857024\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 405.04851024446606\tTime: 0:00:12.235466\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 405.13680437277844\tTime: 0:00:12.714339\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 405.1704165109221\tTime: 0:00:11.882275\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 405.01331707859634\tTime: 0:00:18.770031\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 405.2573300959034\tTime: 0:00:12.199464\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 405.1337128798992\tTime: 0:00:11.782908\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 405.0847760275615\tTime: 0:00:11.781754\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 404.9355434589046\tTime: 0:00:12.022208\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 404.82486200443844\tTime: 0:00:12.236027\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 404.95376197323077\tTime: 0:00:12.274475\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 404.9865847729421\tTime: 0:00:12.006818\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 404.90307038655214\tTime: 0:00:11.908684\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 404.90554003987063\tTime: 0:00:11.848048\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 404.88616923615814\tTime: 0:00:12.400018\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 404.7290825131766\tTime: 0:00:12.464583\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 404.9451441930198\tTime: 0:00:11.931195\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 404.97281015795437\tTime: 0:00:11.947307\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 404.8750926753625\tTime: 0:00:12.032855\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 404.90783055020734\tTime: 0:00:19.327245\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 404.77621438151186\tTime: 0:00:12.464383\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 404.6909114671491\tTime: 0:00:11.947365\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 404.69032198866415\tTime: 0:00:11.781011\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 404.8322818371276\tTime: 0:00:12.285615\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 404.861643728137\tTime: 0:00:19.290131\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 404.68397227541044\tTime: 0:00:12.236284\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 404.8719174330475\tTime: 0:00:12.425810\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 404.88873391570047\tTime: 0:00:12.186549\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 404.68023683018083\tTime: 0:00:12.664369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 02:41:46,215] Trial 9 finished with values: [0.020191554537591486, 0.8551020408163266] and parameters: {'num_topics': 49, 'dropout': 0.20266326363726397, 'num_neurons': 50, 'num_layers': 2, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 430.34936297534415\tTime: 0:00:23.288000\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 418.6727022460717\tTime: 0:00:20.448128\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 417.82672476820505\tTime: 0:00:20.380069\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 416.3391943980149\tTime: 0:00:28.580712\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 414.1767175539993\tTime: 0:00:20.193216\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 412.32426730997986\tTime: 0:00:20.817417\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 411.02339650685514\tTime: 0:00:20.663075\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 409.78916266873307\tTime: 0:00:21.222836\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 408.99417434516755\tTime: 0:00:28.531959\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 408.0760168283472\tTime: 0:00:20.578467\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 407.58396005827785\tTime: 0:00:20.659950\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 407.1815143029343\tTime: 0:00:27.779175\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 406.863038523565\tTime: 0:00:20.406391\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 406.5620324357157\tTime: 0:00:20.750169\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 406.3283751830733\tTime: 0:00:27.084902\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 406.15631924021335\tTime: 0:00:20.498351\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 406.01585714755885\tTime: 0:00:21.078776\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 405.9750153393704\tTime: 0:00:20.479051\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 405.7177782126871\tTime: 0:00:20.880546\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 405.5315910218254\tTime: 0:00:27.517291\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 405.4923638697\tTime: 0:00:20.393260\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 405.2705333046528\tTime: 0:00:20.846068\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 405.2898745022051\tTime: 0:00:20.665540\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 405.13382399483044\tTime: 0:00:27.117524\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 405.15191655441305\tTime: 0:00:42.025100\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 404.93622318842625\tTime: 0:00:46.205746\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 405.0441721706208\tTime: 0:00:27.659731\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 404.7046317919323\tTime: 0:00:21.862056\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 404.65987840757356\tTime: 0:00:20.682712\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 404.4495016614713\tTime: 0:00:21.523524\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 404.560393554761\tTime: 0:00:21.042014\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 404.4480629343216\tTime: 0:00:27.763117\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 404.4490866977098\tTime: 0:00:20.874272\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 404.3023894962927\tTime: 0:00:21.386990\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 404.2325277004932\tTime: 0:00:20.603725\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 404.3595189167936\tTime: 0:00:20.800645\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 404.27058634596216\tTime: 0:00:27.741152\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 404.17627103727057\tTime: 0:00:20.492276\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 404.08437851159914\tTime: 0:00:20.763685\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 404.0746334933682\tTime: 0:00:27.829078\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 404.0885003429319\tTime: 0:00:20.877303\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 403.87827893734016\tTime: 0:00:20.754575\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 403.74654798049255\tTime: 0:00:21.183105\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 403.5711523819515\tTime: 0:00:21.357928\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 403.66058851463174\tTime: 0:00:27.629927\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 403.6187584263687\tTime: 0:00:21.087109\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 403.6546167758054\tTime: 0:00:20.886886\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 403.68884017463375\tTime: 0:00:20.676591\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 403.6344050616396\tTime: 0:00:21.094106\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 403.63649798942583\tTime: 0:00:28.238287\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 403.5208409001683\tTime: 0:00:21.181381\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 403.5353393600603\tTime: 0:00:21.060397\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 403.5633046557634\tTime: 0:00:21.026136\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 403.64588850511075\tTime: 0:00:27.528636\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 403.28323394332307\tTime: 0:00:21.680246\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 403.42539221917787\tTime: 0:00:24.876810\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 403.3922632381411\tTime: 0:00:24.663635\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 403.25576012457213\tTime: 0:00:21.018014\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 403.25556838244285\tTime: 0:00:20.692478\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 403.3932965335374\tTime: 0:00:28.287269\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 403.13926216159354\tTime: 0:00:21.061255\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 403.11698330565196\tTime: 0:00:27.725416\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 403.05749589774695\tTime: 0:00:21.181334\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 403.08300898793067\tTime: 0:00:20.702386\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 403.1603849447313\tTime: 0:00:28.453803\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 402.93985280816264\tTime: 0:00:20.873617\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 403.0668522157456\tTime: 0:00:20.955073\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 403.2206442800017\tTime: 0:00:27.286590\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 403.0489788372522\tTime: 0:00:21.437732\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 402.9643281399867\tTime: 0:00:21.541969\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 403.08390371106975\tTime: 0:00:21.004375\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 403.0172137782221\tTime: 0:00:21.318300\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 402.97007346147876\tTime: 0:00:27.965607\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 402.99001115336785\tTime: 0:00:21.042645\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 402.7090034022841\tTime: 0:00:20.986386\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 402.9010168356936\tTime: 0:00:21.300863\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 402.78813835436176\tTime: 0:00:27.819630\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 402.9001441518798\tTime: 0:00:21.170088\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 403.0490011337029\tTime: 0:00:21.287380\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 403.00256978531934\tTime: 0:00:28.490023\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 402.8501900010226\tTime: 0:00:21.271122\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 402.7381742123439\tTime: 0:00:20.788378\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 402.83690407133923\tTime: 0:00:21.871598\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 402.7744682057756\tTime: 0:00:28.147510\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 402.76438594913674\tTime: 0:00:20.863993\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 402.7509390589149\tTime: 0:00:21.244277\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 402.8919928707191\tTime: 0:00:20.873935\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 402.8081885202738\tTime: 0:00:27.789231\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 402.65890353475515\tTime: 0:00:21.437197\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 402.6302912379283\tTime: 0:00:21.290297\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 402.7305851911015\tTime: 0:00:20.955282\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 402.9020949994828\tTime: 0:00:27.994090\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 402.65854267753707\tTime: 0:00:21.454831\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 402.63015823060067\tTime: 0:00:21.458750\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 402.5353114803139\tTime: 0:00:21.951677\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 402.6844505654997\tTime: 0:00:28.138588\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 402.4751472079995\tTime: 0:00:21.469432\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 402.65559697479404\tTime: 0:00:21.047947\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 402.6946164981099\tTime: 0:00:21.424344\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 402.5177769564456\tTime: 0:00:28.029700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 03:21:43,587] Trial 10 finished with values: [-0.008300724428236402, 0.95] and parameters: {'num_topics': 30, 'dropout': 0.36432313165852914, 'num_neurons': 300, 'num_layers': 2, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 423.32456116764934\tTime: 0:00:13.338556\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 414.58229197521723\tTime: 0:00:11.826871\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 413.8061575964558\tTime: 0:00:11.763629\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 412.5765820268421\tTime: 0:00:11.594547\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 411.5521232979764\tTime: 0:00:11.855850\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 410.78718992873365\tTime: 0:00:11.725569\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 410.39319989109634\tTime: 0:00:12.773767\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 410.12753724058246\tTime: 0:00:18.645275\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 409.953929068048\tTime: 0:00:11.576901\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 409.6309241338839\tTime: 0:00:11.533083\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 409.43107781409856\tTime: 0:00:12.063200\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 409.22552639458974\tTime: 0:00:11.756637\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 409.014794888816\tTime: 0:00:12.311188\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 408.958599421453\tTime: 0:00:18.814440\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 408.7639204785884\tTime: 0:00:11.767576\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 408.45600877223916\tTime: 0:00:12.046980\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 408.26954289411015\tTime: 0:00:12.471286\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 408.44275362414663\tTime: 0:00:19.565251\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 408.3231356640948\tTime: 0:00:11.790397\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 408.1271345821109\tTime: 0:00:11.819477\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 408.27764396234153\tTime: 0:00:11.878661\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 408.20392657025786\tTime: 0:00:11.730057\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 408.3044711840231\tTime: 0:00:18.764187\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 408.1720157945559\tTime: 0:00:12.091011\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 407.9138432821786\tTime: 0:00:11.959849\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 408.0938594619819\tTime: 0:00:11.998473\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 408.08704137633526\tTime: 0:00:12.254775\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 407.90564564497055\tTime: 0:00:18.824759\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 408.0994452554916\tTime: 0:00:12.163408\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 408.1605214783509\tTime: 0:00:12.119757\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 407.91972844318906\tTime: 0:00:11.931752\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 407.8527132466336\tTime: 0:00:12.007344\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 407.75719444385675\tTime: 0:00:19.268460\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 407.8714724223393\tTime: 0:00:11.862359\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 407.83843382090197\tTime: 0:00:11.752581\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 407.8923308805034\tTime: 0:00:11.930598\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 407.90129265784856\tTime: 0:00:19.067086\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 407.7226792810948\tTime: 0:00:11.866293\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 407.82812735086134\tTime: 0:00:11.761351\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 407.9096395747527\tTime: 0:00:12.313407\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 407.85598476406756\tTime: 0:00:18.588828\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 407.6814476339756\tTime: 0:00:12.980307\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 407.94980778769843\tTime: 0:00:12.223288\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 407.85877516303225\tTime: 0:00:12.109525\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 407.75187547311083\tTime: 0:00:18.629244\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 407.63017211537556\tTime: 0:00:11.606657\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 407.6621585226952\tTime: 0:00:11.861224\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 407.52526808635184\tTime: 0:00:11.881966\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 407.6493019557991\tTime: 0:00:12.137041\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 407.6632612235998\tTime: 0:00:18.990895\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 407.6886867809303\tTime: 0:00:14.545768\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 407.6844950114722\tTime: 0:00:13.977539\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 407.7456138069614\tTime: 0:00:12.342750\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 407.7100587186395\tTime: 0:00:21.857242\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 407.8364668111271\tTime: 0:00:12.680163\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 407.68462261916517\tTime: 0:00:11.849481\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 407.7666367886881\tTime: 0:00:20.858111\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 407.5785723615695\tTime: 0:00:12.748542\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 407.5861272953222\tTime: 0:00:12.851557\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 407.6258842177274\tTime: 0:00:12.045615\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 407.53364391150416\tTime: 0:00:12.894763\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 407.63363486068795\tTime: 0:00:19.369519\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 407.6105123026452\tTime: 0:00:12.395542\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 407.5899440656619\tTime: 0:00:11.820399\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 407.4645987829003\tTime: 0:00:11.729876\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 407.65489718948646\tTime: 0:00:12.186206\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 407.41280218118794\tTime: 0:00:12.141189\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 407.4133265334669\tTime: 0:00:18.772793\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 407.66556833689486\tTime: 0:00:12.004308\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 407.47946603416744\tTime: 0:00:11.992441\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 407.56343277770725\tTime: 0:00:11.761499\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 407.43465689031586\tTime: 0:00:12.134741\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 407.39836267649093\tTime: 0:00:11.980861\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 407.5325769877708\tTime: 0:00:11.631700\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 407.66468514766933\tTime: 0:00:11.695521\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 407.5277117332666\tTime: 0:00:12.162694\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 407.44718205922527\tTime: 0:00:19.032894\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 407.4885241968282\tTime: 0:00:11.879937\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 407.5766198610171\tTime: 0:00:12.571472\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 407.5908537755421\tTime: 0:00:11.821102\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 407.50917779946445\tTime: 0:00:12.146049\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 407.40849441141484\tTime: 0:00:19.072024\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 407.5887073289278\tTime: 0:00:11.962564\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 407.44624358802565\tTime: 0:00:11.651173\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 407.499247503973\tTime: 0:00:11.811894\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 407.50277996701743\tTime: 0:00:12.476540\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 407.48000772404885\tTime: 0:00:12.088429\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 407.13535080581653\tTime: 0:00:11.791683\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 407.6481189582863\tTime: 0:00:11.904525\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 407.30075226828734\tTime: 0:00:12.057044\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 407.4342799811226\tTime: 0:00:12.534458\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 407.5353212878136\tTime: 0:00:11.892242\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 407.4820000423155\tTime: 0:00:12.068065\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 407.37828801019566\tTime: 0:00:11.727039\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 407.38867382180706\tTime: 0:00:11.924613\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 407.5102742191614\tTime: 0:00:20.247546\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 407.6027798421279\tTime: 0:00:11.689116\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 407.2449240128145\tTime: 0:00:11.936861\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 407.3779888630882\tTime: 0:00:11.676063\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 407.56104386178345\tTime: 0:00:12.174072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 03:44:42,684] Trial 11 finished with values: [-0.035445390846388966, 0.92] and parameters: {'num_topics': 10, 'dropout': 0.5026603148780894, 'num_neurons': 100, 'num_layers': 2, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 428.40807316644566\tTime: 0:00:16.486160\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 415.3848389792207\tTime: 0:00:19.890101\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 413.11383245352346\tTime: 0:00:12.611499\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 409.98623309877615\tTime: 0:00:12.749105\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 406.61909518725787\tTime: 0:00:12.899524\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 404.7256269673771\tTime: 0:00:12.495744\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 404.0100616498698\tTime: 0:00:12.778505\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 403.62845525194183\tTime: 0:00:12.597137\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 403.07132649262064\tTime: 0:00:13.306691\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 402.6869326343047\tTime: 0:00:19.417962\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 402.174008186287\tTime: 0:00:12.715840\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 402.03871964156417\tTime: 0:00:12.677070\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 401.7632015190678\tTime: 0:00:12.663726\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 401.35055060111523\tTime: 0:00:13.064866\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 401.3117735103767\tTime: 0:00:12.570724\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 401.0870692928139\tTime: 0:00:12.645233\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 400.9371733772004\tTime: 0:00:19.112404\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 400.8642550208521\tTime: 0:00:12.981220\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 400.5879955689202\tTime: 0:00:13.291997\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 400.65220468185794\tTime: 0:00:12.829997\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 400.47862332383585\tTime: 0:00:12.590836\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 400.29228516359643\tTime: 0:00:12.852971\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 400.1636787953481\tTime: 0:00:13.110174\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 400.26787032972004\tTime: 0:00:12.550089\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 400.11334450286336\tTime: 0:00:19.558278\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 399.84008225223096\tTime: 0:00:12.850161\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 399.92950692446215\tTime: 0:00:12.639656\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 399.7803561951662\tTime: 0:00:12.819221\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 399.7868391361407\tTime: 0:00:12.801879\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 399.47614716891644\tTime: 0:00:12.720878\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 399.48994059079496\tTime: 0:00:13.284639\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 399.3719266895641\tTime: 0:00:19.449050\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 399.44936274025804\tTime: 0:00:12.949781\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 399.3338666850034\tTime: 0:00:12.694991\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 399.31481284351963\tTime: 0:00:12.896018\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 399.1961304528229\tTime: 0:00:14.291388\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 399.1745458356254\tTime: 0:00:13.259187\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 398.9972904118944\tTime: 0:00:12.924040\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 398.9757222507264\tTime: 0:00:13.094060\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 399.1669067116504\tTime: 0:00:13.103131\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 399.11113877046427\tTime: 0:00:19.916794\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 398.9762644548588\tTime: 0:00:12.849511\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 398.90609487988274\tTime: 0:00:12.759704\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 398.913263537729\tTime: 0:00:12.790922\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 399.0511323731828\tTime: 0:00:13.311046\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 398.8191807806505\tTime: 0:00:20.499894\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 398.7942200326887\tTime: 0:00:14.096313\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 398.79303398640263\tTime: 0:00:13.569497\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 398.7028359175177\tTime: 0:00:13.205751\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 398.64243233486377\tTime: 0:00:12.964252\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 398.60161847988604\tTime: 0:00:12.935060\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 398.70205304395404\tTime: 0:00:12.831539\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 398.7588494135189\tTime: 0:00:13.327435\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 398.6251948643671\tTime: 0:00:19.582738\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 398.627663011665\tTime: 0:00:12.757559\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 398.50293835630123\tTime: 0:00:12.851820\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 398.38220263513927\tTime: 0:00:12.727420\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 398.5020780512417\tTime: 0:00:12.973986\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 398.44424340906636\tTime: 0:00:12.697325\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 398.5484709410028\tTime: 0:00:12.833671\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 398.44285048698094\tTime: 0:00:12.930454\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 398.3641904823615\tTime: 0:00:13.858103\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 398.42537308169716\tTime: 0:00:19.723170\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 398.2011934735387\tTime: 0:00:13.047847\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 398.49817819264604\tTime: 0:00:12.766989\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 398.4260845111149\tTime: 0:00:13.215157\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 398.2829433915524\tTime: 0:00:12.607520\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 398.44674656215915\tTime: 0:00:12.873238\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 398.2057250526593\tTime: 0:00:19.589233\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 398.3034498082285\tTime: 0:00:12.825369\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 398.2570522534769\tTime: 0:00:13.245443\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 398.2763952876396\tTime: 0:00:12.832826\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 398.21308927278454\tTime: 0:00:13.055180\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 398.22125080223145\tTime: 0:00:12.890503\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 398.21000424477404\tTime: 0:00:13.712831\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 398.12262893592964\tTime: 0:00:12.829152\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 398.11932737154893\tTime: 0:00:13.279649\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 398.079737176639\tTime: 0:00:12.941549\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 398.1921692514387\tTime: 0:00:20.920635\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 398.0999332061517\tTime: 0:00:13.429713\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 397.9691823380962\tTime: 0:00:12.787233\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 398.0991774042259\tTime: 0:00:12.979742\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 397.98800803583185\tTime: 0:00:13.549417\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 398.2627148173028\tTime: 0:00:13.066133\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 398.1247448580785\tTime: 0:00:12.849558\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 398.06318814119743\tTime: 0:00:12.891177\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 397.8646235183696\tTime: 0:00:20.584738\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 397.9669842827287\tTime: 0:00:12.925253\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 398.0028299228212\tTime: 0:00:12.953548\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 397.97313880102547\tTime: 0:00:12.870207\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 397.968314172344\tTime: 0:00:12.783006\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 398.0229785677847\tTime: 0:00:13.399279\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 398.0457032792753\tTime: 0:00:19.790286\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 397.9020095603654\tTime: 0:00:12.938368\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 398.12729726906315\tTime: 0:00:13.301666\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 397.8758698554338\tTime: 0:00:12.892907\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 398.02791816827937\tTime: 0:00:13.099175\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 397.8604083504946\tTime: 0:00:12.835314\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 397.8490040649331\tTime: 0:00:13.005316\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 397.9551539035465\tTime: 0:00:12.734868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 04:08:52,582] Trial 12 finished with values: [0.006328999413223143, 0.9695652173913043] and parameters: {'num_topics': 23, 'dropout': 0.1732542046092017, 'num_neurons': 100, 'num_layers': 2, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 437.06148350958915\tTime: 0:00:19.938168\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 420.1491689852037\tTime: 0:00:17.628829\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 414.93849735293014\tTime: 0:00:17.280214\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 411.97413957005097\tTime: 0:00:17.097738\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 410.16850004466636\tTime: 0:00:17.787802\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 408.998496256841\tTime: 0:00:17.342579\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 408.3043732192225\tTime: 0:00:17.215280\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 407.52309331920463\tTime: 0:00:24.103928\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 407.11603421884405\tTime: 0:00:17.127374\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 406.41575181427726\tTime: 0:00:17.707743\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 406.1478184888193\tTime: 0:00:17.547591\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 405.87181965190797\tTime: 0:00:17.183982\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 405.76963630411683\tTime: 0:00:23.877019\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 405.6277828688619\tTime: 0:00:18.468516\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 405.2011902411043\tTime: 0:00:17.544380\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 405.14921974911607\tTime: 0:00:17.278289\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 404.8064100936583\tTime: 0:00:24.314124\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 404.61351280984354\tTime: 0:00:17.689073\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 404.5583081203453\tTime: 0:00:17.451532\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 404.45778547244083\tTime: 0:00:17.620999\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 404.3944229267166\tTime: 0:00:23.824809\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 404.39275407227956\tTime: 0:00:17.324581\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 404.221979091733\tTime: 0:00:17.169707\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 404.1313896044323\tTime: 0:00:17.514771\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 404.13669230280976\tTime: 0:00:17.303741\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 404.080854901017\tTime: 0:00:17.220377\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 404.0993144153643\tTime: 0:00:23.886438\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 404.03241330904893\tTime: 0:00:17.323283\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 403.84445937241406\tTime: 0:00:17.093970\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 403.88337946164097\tTime: 0:00:17.328980\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 403.82348855042363\tTime: 0:00:24.280738\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 403.82737132854226\tTime: 0:00:18.319972\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 403.55987056157375\tTime: 0:00:26.471705\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 403.66993858815493\tTime: 0:00:17.294574\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 403.69868330459406\tTime: 0:00:17.398633\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 403.52764605607746\tTime: 0:00:24.174132\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 403.4016286473614\tTime: 0:00:17.738085\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 403.38428887766355\tTime: 0:00:24.506657\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 403.445926809458\tTime: 0:00:18.515145\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 403.17006171451845\tTime: 0:00:19.653491\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 403.5092261757833\tTime: 0:00:29.042095\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 403.1864351332703\tTime: 0:00:21.602677\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 403.1377696952226\tTime: 0:00:17.192102\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 403.09039693706274\tTime: 0:00:25.655914\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 403.18535325952803\tTime: 0:00:17.947298\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 403.1302534772178\tTime: 0:00:17.819014\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 403.3119037627887\tTime: 0:00:17.485306\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 403.4186542464784\tTime: 0:00:23.987645\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 403.26284055165314\tTime: 0:00:17.225758\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 403.20483113762884\tTime: 0:00:17.701256\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 402.8845897144526\tTime: 0:00:21.914437\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 403.0625570818523\tTime: 0:00:20.553425\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 402.9096639649392\tTime: 0:00:19.361087\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 402.97734375734643\tTime: 0:00:25.281826\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 403.004126459591\tTime: 0:00:17.520293\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 402.90158545028396\tTime: 0:00:25.311043\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 403.0973960757659\tTime: 0:00:18.896616\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 403.0713642900634\tTime: 0:00:19.077472\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 402.9691536502412\tTime: 0:00:28.809810\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 402.95304279331697\tTime: 0:00:17.381814\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 402.8250629296199\tTime: 0:00:25.306797\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 402.8903857998571\tTime: 0:00:20.658936\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 402.7347270782202\tTime: 0:00:21.416484\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 402.84507922843557\tTime: 0:00:25.485300\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 402.79080467192557\tTime: 0:00:17.546286\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 402.7582762074024\tTime: 0:00:17.391149\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 402.6387213900173\tTime: 0:00:17.804121\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 402.68113856917176\tTime: 0:00:17.457956\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 402.6080700882866\tTime: 0:00:24.362236\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 402.6191872381728\tTime: 0:00:18.152428\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 402.81657488757\tTime: 0:00:17.376655\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 402.5931478856353\tTime: 0:00:17.161812\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 402.65658734860455\tTime: 0:00:18.719642\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 402.5747900827738\tTime: 0:00:25.021539\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 402.6147317314564\tTime: 0:00:17.357243\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 402.55586285721336\tTime: 0:00:25.430583\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 402.58783115555417\tTime: 0:00:17.553390\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 402.71127407722815\tTime: 0:00:17.761693\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 402.53495734509\tTime: 0:00:17.800822\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 402.4744353377953\tTime: 0:00:24.490940\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 402.47346659925574\tTime: 0:00:17.547326\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 402.47688730455525\tTime: 0:00:17.888308\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 402.49165378100787\tTime: 0:00:17.563188\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 402.5874116369988\tTime: 0:00:25.523226\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 402.56852176809457\tTime: 0:00:19.886880\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 402.41435349255715\tTime: 0:00:25.591975\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 402.5776802464173\tTime: 0:00:17.787613\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 402.2858250333235\tTime: 0:00:17.575516\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 402.4529239278897\tTime: 0:00:17.570337\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 402.2373952323944\tTime: 0:00:17.594009\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 402.35860939057443\tTime: 0:00:17.715525\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 402.28883038917337\tTime: 0:00:24.580503\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 402.3322712582515\tTime: 0:00:17.492208\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 402.3465996480173\tTime: 0:00:17.644388\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 402.3739748407879\tTime: 0:00:25.167181\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 402.41231007978826\tTime: 0:00:17.621839\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 402.28664662263503\tTime: 0:00:18.131378\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 402.4179319075947\tTime: 0:00:24.866973\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 402.3333782568246\tTime: 0:00:17.737218\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 402.1504435928449\tTime: 0:00:17.775067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 04:42:59,654] Trial 13 finished with values: [0.031043014858498412, 0.9390243902439024] and parameters: {'num_topics': 41, 'dropout': 0.27961490715381193, 'num_neurons': 200, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 441.5523574290745\tTime: 0:00:15.219454\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 423.84146187725685\tTime: 0:00:13.035755\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 421.5638594957755\tTime: 0:00:13.117027\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 420.80898436030714\tTime: 0:00:12.909837\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 419.7556371451375\tTime: 0:00:13.341839\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 417.8882381778121\tTime: 0:00:20.044669\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 416.49231602599815\tTime: 0:00:12.828860\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 415.65997530714003\tTime: 0:00:12.921064\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 414.7894946177029\tTime: 0:00:14.251099\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 414.13530360492837\tTime: 0:00:13.570995\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 413.72889110163715\tTime: 0:00:12.869070\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 413.21903338899\tTime: 0:00:19.759742\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 412.73953973220165\tTime: 0:00:13.131172\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 412.60772212407375\tTime: 0:00:13.740722\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 412.2373842127318\tTime: 0:00:13.051739\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 412.23072829978656\tTime: 0:00:12.806849\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 412.0990715420005\tTime: 0:00:13.252400\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 411.74311601677107\tTime: 0:00:12.897524\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 411.87977834609853\tTime: 0:00:13.078583\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 411.57716096318313\tTime: 0:00:13.096200\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 411.5390037284661\tTime: 0:00:13.019384\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 411.5357231014444\tTime: 0:00:20.154543\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 411.3399701822623\tTime: 0:00:12.936304\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 411.15191761964707\tTime: 0:00:12.810650\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 411.08399568852025\tTime: 0:00:12.759654\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 411.0520362059096\tTime: 0:00:13.241163\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 410.9854291041925\tTime: 0:00:13.444948\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 410.9811097637737\tTime: 0:00:12.902789\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 410.9275629457821\tTime: 0:00:12.987355\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 410.88316601077634\tTime: 0:00:12.822884\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 410.90079618531604\tTime: 0:00:20.169741\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 410.8652233920696\tTime: 0:00:13.370736\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 410.7367138499234\tTime: 0:00:13.159987\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 410.63425750835734\tTime: 0:00:13.249107\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 410.6226121860425\tTime: 0:00:12.758524\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 410.6594034233831\tTime: 0:00:12.927147\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 410.7316937691595\tTime: 0:00:12.995179\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 410.81421925102734\tTime: 0:00:20.316923\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 410.3834780508891\tTime: 0:00:13.039128\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 410.6095827573723\tTime: 0:00:13.491491\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 410.41542394258255\tTime: 0:00:13.188112\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 410.52754291203547\tTime: 0:00:13.102305\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 410.5298895124549\tTime: 0:00:14.386609\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 410.59152825235793\tTime: 0:00:19.881052\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 410.32982155052235\tTime: 0:00:12.930264\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 410.42815304871453\tTime: 0:00:13.317242\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 410.405131210388\tTime: 0:00:13.052394\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 410.0984470577207\tTime: 0:00:13.113867\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 410.3794306757433\tTime: 0:00:13.192874\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 410.3468829268106\tTime: 0:00:13.190614\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 410.259311137382\tTime: 0:00:13.353530\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 410.4710371309735\tTime: 0:00:20.549405\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 410.1879320809707\tTime: 0:00:13.413055\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 410.27213732674153\tTime: 0:00:13.433193\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 410.2565324927241\tTime: 0:00:13.254569\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 410.26856335630123\tTime: 0:00:13.519187\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 410.27574016869784\tTime: 0:00:13.135637\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 410.1789603124647\tTime: 0:00:20.177562\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 409.9811729431726\tTime: 0:00:13.185192\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 410.1258527382245\tTime: 0:00:13.431050\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 410.0258814407724\tTime: 0:00:13.305052\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 410.06824903144513\tTime: 0:00:13.241951\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 410.0426759942968\tTime: 0:00:13.710688\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 410.0574246005299\tTime: 0:00:20.690708\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 410.0377833742971\tTime: 0:00:13.244194\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 410.0894052041605\tTime: 0:00:13.890729\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 409.9243435513522\tTime: 0:00:13.064499\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 409.90478027086624\tTime: 0:00:20.678657\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 409.83807366159584\tTime: 0:00:13.010825\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 409.9906595870477\tTime: 0:00:13.228972\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 409.9945862968879\tTime: 0:00:12.959963\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 409.7674228946861\tTime: 0:00:13.635405\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 409.77986534266154\tTime: 0:00:13.006963\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 409.90554823115315\tTime: 0:00:19.980995\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 409.7419354803162\tTime: 0:00:13.170314\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 409.6136082768539\tTime: 0:00:13.420411\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 410.0043165855032\tTime: 0:00:13.105682\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 409.71931637392566\tTime: 0:00:13.266643\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 409.84512962502583\tTime: 0:00:13.512801\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 409.77274256334397\tTime: 0:00:13.451826\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 409.7608181497076\tTime: 0:00:13.357813\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 409.89750472229275\tTime: 0:00:13.516043\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 409.87687426094794\tTime: 0:00:20.025828\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 409.73546014290883\tTime: 0:00:14.552398\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 409.76602460969826\tTime: 0:00:13.150936\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 409.73755185853224\tTime: 0:00:13.366103\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 409.77308020580614\tTime: 0:00:13.405333\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 409.71931725549865\tTime: 0:00:13.152304\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 409.686650530942\tTime: 0:00:13.389721\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 409.7225187246107\tTime: 0:00:20.298361\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 409.73341551797705\tTime: 0:00:13.734307\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 409.5715850873698\tTime: 0:00:13.523274\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 409.65167081593694\tTime: 0:00:13.263625\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 409.66572264890357\tTime: 0:00:13.304459\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 409.5243908477441\tTime: 0:00:20.412416\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 409.76667590849036\tTime: 0:00:13.400214\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 409.44909716638927\tTime: 0:00:13.590787\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 409.62242911271437\tTime: 0:00:13.455804\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 409.4031784159779\tTime: 0:00:13.318103\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 409.5203317550238\tTime: 0:00:13.729482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 05:07:36,904] Trial 14 finished with values: [-0.030735699460577456, 0.8729729729729729] and parameters: {'num_topics': 37, 'dropout': 0.4998873295356704, 'num_neurons': 100, 'num_layers': 2, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 420.5887863031765\tTime: 0:00:23.050427\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 410.74774721364156\tTime: 0:00:19.957590\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 408.37650473492863\tTime: 0:00:26.309171\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 407.5347933357782\tTime: 0:00:19.830797\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 406.60268151000525\tTime: 0:00:19.584435\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 406.27444686436235\tTime: 0:00:19.674198\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 405.93056995458136\tTime: 0:00:19.912480\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 405.78743265516863\tTime: 0:00:26.315792\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 405.40980409831536\tTime: 0:00:19.889575\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 405.1516013185981\tTime: 0:00:19.880597\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 405.1067452309839\tTime: 0:00:19.943028\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 404.9388497984724\tTime: 0:00:19.593866\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 404.59058361014723\tTime: 0:00:26.488313\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 404.6361550942578\tTime: 0:00:19.717321\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 404.48598897651726\tTime: 0:00:20.381221\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 404.7934810908467\tTime: 0:00:26.614115\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 404.58406228401464\tTime: 0:00:19.755019\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 404.3249540920856\tTime: 0:00:19.958557\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 404.1740754796933\tTime: 0:00:20.064206\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 404.2959026249718\tTime: 0:00:19.807963\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 404.02055776830383\tTime: 0:00:26.547861\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 404.143536909111\tTime: 0:00:20.226153\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 404.0800217777919\tTime: 0:00:19.881193\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 403.84216651121596\tTime: 0:00:19.941668\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 403.9467134664391\tTime: 0:00:19.690944\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 404.06989643427227\tTime: 0:00:26.371387\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 404.0553220121728\tTime: 0:00:20.073562\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 403.8471684830009\tTime: 0:00:19.760297\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 403.87233665774517\tTime: 0:00:20.050710\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 403.9675190669549\tTime: 0:00:19.506950\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 403.8333292927904\tTime: 0:00:26.526725\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 403.72369746118727\tTime: 0:00:20.103893\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 403.6646953224912\tTime: 0:00:26.585234\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 403.62697865388503\tTime: 0:00:20.021968\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 403.68085903706367\tTime: 0:00:19.813171\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 403.5412442095346\tTime: 0:00:27.907257\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 403.5315847406177\tTime: 0:00:23.069934\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 403.732856123171\tTime: 0:00:26.639737\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 403.5408144059608\tTime: 0:00:20.283484\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 403.78623911551193\tTime: 0:00:20.184296\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 403.66843209008033\tTime: 0:00:20.288182\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 403.4300421127426\tTime: 0:00:19.876745\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 403.6269421420696\tTime: 0:00:27.658023\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 403.58278117477244\tTime: 0:00:19.990759\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 403.498379374953\tTime: 0:00:19.986550\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 403.623850061475\tTime: 0:00:19.918349\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 403.58167138455156\tTime: 0:00:20.116843\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 403.4737136233591\tTime: 0:00:20.535868\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 403.7279532181529\tTime: 0:00:26.341946\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 403.5397336341848\tTime: 0:00:19.795469\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 403.5059241338839\tTime: 0:00:19.690023\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 403.7196913363119\tTime: 0:00:26.452416\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 403.46222389868024\tTime: 0:00:19.805512\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 403.614270301451\tTime: 0:00:20.130042\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 403.54333974530766\tTime: 0:00:20.053288\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 403.5750354539278\tTime: 0:00:19.723147\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 403.3841423528831\tTime: 0:00:26.618400\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 403.64009061982813\tTime: 0:00:19.737540\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 403.44189475164325\tTime: 0:00:20.207325\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 403.27451592091467\tTime: 0:00:20.210951\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 403.37716198433856\tTime: 0:00:26.474559\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 403.4478144776621\tTime: 0:00:20.876430\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 403.22375057596105\tTime: 0:00:20.337372\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 403.6003247714963\tTime: 0:00:19.796557\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 403.3188896778967\tTime: 0:00:20.188458\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 403.3437975682102\tTime: 0:00:26.601987\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 403.4872972675939\tTime: 0:00:20.006032\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 403.4893538304935\tTime: 0:00:20.205423\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 403.56040843130546\tTime: 0:00:20.070803\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 403.24877071990426\tTime: 0:00:19.964826\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 403.35901513455156\tTime: 0:00:28.169393\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 403.543250743166\tTime: 0:00:20.449727\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 403.5763804772131\tTime: 0:00:26.244075\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 403.2912312063327\tTime: 0:00:21.736035\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 403.29849062946664\tTime: 0:00:20.746180\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 403.1156857771125\tTime: 0:00:20.513641\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 403.3035712816426\tTime: 0:00:27.176849\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 403.3000009844232\tTime: 0:00:20.271076\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 403.2873559583169\tTime: 0:00:56.202735\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 403.1663360767885\tTime: 0:00:37.524956\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 403.41192384061395\tTime: 0:00:21.170376\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 403.35965060176176\tTime: 0:00:29.334969\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 403.44773638498646\tTime: 0:00:21.236598\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 403.28725850776726\tTime: 0:00:20.271305\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 403.4202761204205\tTime: 0:00:27.222910\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 403.38939105491374\tTime: 0:00:20.550414\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 403.22702686858213\tTime: 0:00:20.962840\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 403.37739236875143\tTime: 0:00:26.954783\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 403.3784196584316\tTime: 0:00:20.438385\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 403.1326611633002\tTime: 0:00:20.802500\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 403.35839296440093\tTime: 0:00:20.345480\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 403.09400157889723\tTime: 0:00:27.237401\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 403.2074081959255\tTime: 0:00:20.053597\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 403.34804917443626\tTime: 0:00:20.453920\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 403.0817456938097\tTime: 0:00:28.217217\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 403.1852837621892\tTime: 0:00:20.680733\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 403.1830220499041\tTime: 0:00:20.382084\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 403.13877869226286\tTime: 0:00:27.354853\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 403.12515680979885\tTime: 0:00:20.583831\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 403.252059795334\tTime: 0:00:20.483207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 05:46:13,925] Trial 15 finished with values: [0.0016312618392387013, 0.9833333333333333] and parameters: {'num_topics': 12, 'dropout': 0.3695435414172589, 'num_neurons': 300, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 421.68222580581653\tTime: 0:00:11.903947\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 410.2168683191459\tTime: 0:00:10.975296\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 406.689418449795\tTime: 0:00:10.754426\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 405.14957708004215\tTime: 0:00:10.846483\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 404.06052130938275\tTime: 0:00:11.231547\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 403.32143782794515\tTime: 0:00:10.735084\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 402.5401542547064\tTime: 0:00:11.173729\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 402.07042982267455\tTime: 0:00:10.721816\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 401.87728180480326\tTime: 0:00:10.602965\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 401.42161460537267\tTime: 0:00:11.010927\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 401.14608774061065\tTime: 0:00:10.637635\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 400.8928459395335\tTime: 0:00:17.448779\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 400.65208890193617\tTime: 0:00:10.653078\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 400.54030070602244\tTime: 0:00:10.607771\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 400.3096527395763\tTime: 0:00:10.793209\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 400.3087732235716\tTime: 0:00:10.867395\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 400.1127066113274\tTime: 0:00:10.646777\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 400.16239140489824\tTime: 0:00:10.955626\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 399.9839896056665\tTime: 0:00:10.985671\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 399.93209272502446\tTime: 0:00:11.001162\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 399.7587601542518\tTime: 0:00:10.801397\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 399.82369939330147\tTime: 0:00:11.350706\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 399.761298239675\tTime: 0:00:11.398800\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 399.5122040926733\tTime: 0:00:18.795785\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 399.58718099546047\tTime: 0:00:12.380190\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 399.612602512248\tTime: 0:00:12.295458\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 399.4199283398687\tTime: 0:00:12.233706\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 399.31266096053844\tTime: 0:00:12.819158\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 399.3212211079139\tTime: 0:00:12.584710\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 399.31860022809235\tTime: 0:00:20.083687\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 399.34914452889916\tTime: 0:00:13.059066\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 399.25681158404564\tTime: 0:00:13.189150\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 399.1712587070027\tTime: 0:00:12.750447\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 399.05778586180224\tTime: 0:00:21.434052\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 399.0566724350927\tTime: 0:00:12.663657\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 399.15961896798365\tTime: 0:00:12.320187\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 399.0545568435337\tTime: 0:00:12.863619\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 399.03041511362886\tTime: 0:00:12.665675\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 398.94348683576413\tTime: 0:00:12.689923\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 398.8743205276038\tTime: 0:00:12.563755\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 398.9879203560497\tTime: 0:00:12.678230\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 398.984159896186\tTime: 0:00:19.360857\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 398.7976344016823\tTime: 0:00:13.092643\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 398.7580350237202\tTime: 0:00:12.270795\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 398.71310488397324\tTime: 0:00:13.294746\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 398.74014026708136\tTime: 0:00:12.680686\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 398.77212527857705\tTime: 0:00:12.710312\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 398.78346230746445\tTime: 0:00:12.995304\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 398.58670909677556\tTime: 0:00:12.434190\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 398.6061470836977\tTime: 0:00:12.904677\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 398.86310633122224\tTime: 0:00:12.620363\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 398.7698177612277\tTime: 0:00:19.695091\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 398.63452043750704\tTime: 0:00:12.541780\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 398.81664706636013\tTime: 0:00:12.940596\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 398.6670865525441\tTime: 0:00:12.546841\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 398.5880107760546\tTime: 0:00:12.930360\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 398.6597579626613\tTime: 0:00:13.189522\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 398.71893123671765\tTime: 0:00:12.806021\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 398.69450971757925\tTime: 0:00:12.938249\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 398.52365649742814\tTime: 0:00:12.966557\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 398.531629149858\tTime: 0:00:19.739816\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 398.55305501044955\tTime: 0:00:13.118485\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 398.37037471701507\tTime: 0:00:13.310814\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 398.42200910929387\tTime: 0:00:13.265908\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 398.6014499892448\tTime: 0:00:13.091072\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 398.6299652763085\tTime: 0:00:12.987275\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 398.47346053844126\tTime: 0:00:13.014494\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 398.50903990675306\tTime: 0:00:20.197531\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 398.50660731623316\tTime: 0:00:13.109511\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 398.34284348582196\tTime: 0:00:13.300181\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 398.44803733197216\tTime: 0:00:13.475161\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 398.3746172136533\tTime: 0:00:12.776658\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 398.49103730434956\tTime: 0:00:13.115349\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 398.50418581884026\tTime: 0:00:12.974918\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 398.37502802667524\tTime: 0:00:13.090964\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 398.3167290893821\tTime: 0:00:12.783977\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 398.4708836270735\tTime: 0:00:13.977563\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 398.4262881544798\tTime: 0:00:20.251250\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 398.39685243173096\tTime: 0:00:13.276853\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 398.3330264357298\tTime: 0:00:12.790312\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 398.4601196941059\tTime: 0:00:13.277355\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 398.26128316110334\tTime: 0:00:12.888353\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 398.32966172868237\tTime: 0:00:12.858122\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 398.307744280942\tTime: 0:00:13.460576\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 398.28778532110414\tTime: 0:00:13.012326\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 398.30403653179775\tTime: 0:00:13.575787\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 398.1751871432568\tTime: 0:00:12.857980\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 398.1833074224333\tTime: 0:00:19.438406\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 398.2782216497875\tTime: 0:00:13.230544\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 398.36628205105603\tTime: 0:00:13.332456\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 398.12396657604097\tTime: 0:00:13.057281\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 398.29454301899966\tTime: 0:00:13.259701\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 398.22194731837243\tTime: 0:00:13.042380\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 398.33697514816305\tTime: 0:00:13.447495\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 398.1212519188906\tTime: 0:00:12.888504\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 398.151547432448\tTime: 0:00:13.990590\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 398.04325188186453\tTime: 0:00:13.261317\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 398.139964224298\tTime: 0:00:12.837261\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 398.29235587309813\tTime: 0:00:13.359530\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 398.0760197669239\tTime: 0:00:20.091776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 06:09:05,308] Trial 16 finished with values: [0.020866667832279653, 0.9590909090909091] and parameters: {'num_topics': 22, 'dropout': 0.13993250556253728, 'num_neurons': 50, 'num_layers': 1, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 432.19140588267794\tTime: 0:00:24.329027\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 417.22922742173984\tTime: 0:00:20.613610\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 413.33754739924206\tTime: 0:00:20.367936\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 411.6131586011552\tTime: 0:00:20.378588\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 410.4988810634591\tTime: 0:00:20.451509\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 409.7410350636848\tTime: 0:00:26.882241\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 409.1983689503522\tTime: 0:00:20.284119\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 408.880254395523\tTime: 0:00:20.771376\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 408.432766981447\tTime: 0:00:20.173341\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 408.30385463390036\tTime: 0:00:20.246970\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 407.9647775218277\tTime: 0:00:21.543051\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 407.6204079963961\tTime: 0:00:26.740675\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 407.7266862140788\tTime: 0:00:20.267696\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 407.49845967156114\tTime: 0:00:20.887758\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 407.17583616465197\tTime: 0:00:26.498598\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 407.20242293729547\tTime: 0:00:20.188562\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 406.9855339746483\tTime: 0:00:20.261106\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 407.15739817537894\tTime: 0:00:27.263228\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 406.8247115125795\tTime: 0:00:19.982632\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 406.83529747799594\tTime: 0:00:20.107206\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 406.8891113391447\tTime: 0:00:20.401249\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 406.62219784672857\tTime: 0:00:20.219315\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 406.5462919496516\tTime: 0:00:26.976349\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 406.7088257653229\tTime: 0:00:20.141887\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 406.5639049335176\tTime: 0:00:20.378153\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 406.2947459644526\tTime: 0:00:20.113645\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 406.2221631568307\tTime: 0:00:27.765080\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 406.1552048584664\tTime: 0:00:20.117861\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 406.37768626315307\tTime: 0:00:20.217608\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 406.2664057798277\tTime: 0:00:26.966003\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 406.21148598534\tTime: 0:00:20.467906\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 406.14048269942356\tTime: 0:00:20.013658\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 406.12882977354155\tTime: 0:00:27.022094\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 406.0809641793378\tTime: 0:00:20.142032\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 405.9230337542551\tTime: 0:00:20.562935\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 406.00873014403726\tTime: 0:00:20.374275\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 405.94420844000365\tTime: 0:00:20.150250\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 405.9900334498185\tTime: 0:00:27.890285\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 405.9562089627765\tTime: 0:00:20.469114\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 405.87227671078057\tTime: 0:00:20.691740\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 405.9606535232947\tTime: 0:00:20.323641\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 405.8443186145904\tTime: 0:00:20.686639\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 405.9111169074536\tTime: 0:00:20.222230\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 405.74923394978794\tTime: 0:00:27.064696\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 405.64466333902243\tTime: 0:00:20.124075\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 405.7768466162877\tTime: 0:00:20.268315\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 405.94245998687046\tTime: 0:00:26.831981\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 405.69496714377397\tTime: 0:00:19.634499\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 405.51449636615604\tTime: 0:00:20.691368\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 405.6699193037454\tTime: 0:00:25.121915\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 405.65465508749907\tTime: 0:00:21.708640\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 405.4626512779282\tTime: 0:00:20.464705\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 405.6364093178741\tTime: 0:00:20.266062\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 405.5610623380844\tTime: 0:00:21.794603\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 405.6249619454318\tTime: 0:00:26.203039\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 405.4740818637511\tTime: 0:00:20.187158\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 405.46205434280495\tTime: 0:00:20.280458\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 405.4956500983835\tTime: 0:00:26.995808\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 405.5617164652566\tTime: 0:00:20.075258\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 405.6604641393732\tTime: 0:00:20.316717\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 405.4514598555161\tTime: 0:00:20.164978\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 405.3129709803797\tTime: 0:00:26.753896\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 405.4482828133228\tTime: 0:00:20.177589\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 405.3846349685337\tTime: 0:00:26.935929\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 405.40565875836904\tTime: 0:00:21.072361\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 405.2787596297158\tTime: 0:00:27.438265\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 405.4416161378404\tTime: 0:00:20.368955\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 405.13373543347535\tTime: 0:00:21.478986\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 405.3553056589935\tTime: 0:00:26.977602\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 405.13743275067236\tTime: 0:00:20.233834\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 405.2489899377257\tTime: 0:00:20.314005\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 405.2740460425012\tTime: 0:00:27.327257\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 405.10401643193313\tTime: 0:00:20.197486\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 405.3347159336681\tTime: 0:00:20.247684\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 405.27547455809685\tTime: 0:00:20.341160\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 405.23665081745327\tTime: 0:00:27.185252\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 405.4350428356325\tTime: 0:00:20.087413\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 405.20308510882137\tTime: 0:00:20.346072\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 405.2324970658312\tTime: 0:00:26.847936\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 405.16663695030985\tTime: 0:00:20.721812\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 405.2499730018266\tTime: 0:00:20.435906\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 405.008150987303\tTime: 0:00:20.456971\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 405.1423691554648\tTime: 0:00:26.933970\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 405.070154955557\tTime: 0:00:20.526003\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 405.0265057046589\tTime: 0:00:20.383851\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 404.99548693391256\tTime: 0:00:27.376209\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 405.07739278015214\tTime: 0:00:20.380592\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 404.9840428673691\tTime: 0:00:26.901046\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 405.0553990763466\tTime: 0:00:20.183868\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 404.93862870730834\tTime: 0:00:20.274404\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 405.14353220738826\tTime: 0:00:20.450039\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 404.98138051688386\tTime: 0:00:29.053287\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 404.9735248565387\tTime: 0:00:20.927207\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 404.94908019610887\tTime: 0:00:20.302147\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 405.00851041196495\tTime: 0:00:26.958834\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 405.071428828554\tTime: 0:00:20.671106\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 404.7555943521144\tTime: 0:00:20.379768\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 404.94052904812446\tTime: 0:00:22.026584\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 404.92689981921876\tTime: 0:00:26.966837\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 404.86044309916406\tTime: 0:00:20.364963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 06:47:19,331] Trial 17 finished with values: [0.011328758028971585, 0.9171428571428571] and parameters: {'num_topics': 35, 'dropout': 0.40793422280208835, 'num_neurons': 300, 'num_layers': 1, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 423.35685046876176\tTime: 0:00:19.728600\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 414.2128232581293\tTime: 0:00:18.420701\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 411.4597703708014\tTime: 0:00:18.068877\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 410.2470768141009\tTime: 0:00:17.614308\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 409.37481031487437\tTime: 0:00:24.992334\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 408.8325133602389\tTime: 0:00:17.569494\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 408.2864040431289\tTime: 0:00:17.708524\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 407.98471572796774\tTime: 0:00:17.447431\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 407.92177365583626\tTime: 0:00:17.715764\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 407.59471150229444\tTime: 0:00:18.087488\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 407.23012104291263\tTime: 0:00:17.517229\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 407.1704013772521\tTime: 0:00:25.949432\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 407.10552399524187\tTime: 0:00:58.104860\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 406.89581750175137\tTime: 0:00:18.134874\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 406.7881491168989\tTime: 0:00:17.473843\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 406.5787187394211\tTime: 0:00:17.953781\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 406.6458490914509\tTime: 0:00:17.468549\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 406.48406578826734\tTime: 0:00:17.259292\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 406.757627149275\tTime: 0:00:17.275752\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 406.50625986627125\tTime: 0:00:24.225068\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 406.3434183081556\tTime: 0:00:18.497744\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 406.188287869144\tTime: 0:00:19.468603\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 406.19835513904167\tTime: 0:00:21.282607\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 406.2512299780077\tTime: 0:00:28.535921\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 406.1175141227996\tTime: 0:00:22.103639\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 406.04109415846216\tTime: 0:00:22.811984\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 406.1403171106268\tTime: 0:00:22.627058\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 405.9949990567169\tTime: 0:00:24.629798\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 406.10950683189697\tTime: 0:00:28.584631\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 405.9101701347749\tTime: 0:00:23.040103\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 405.9141211980695\tTime: 0:00:31.037520\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 405.8038814851449\tTime: 0:00:23.679494\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 405.8191225233676\tTime: 0:00:23.357782\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 405.6711639011721\tTime: 0:00:24.208075\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 405.608233509883\tTime: 0:00:31.673208\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 405.719514984978\tTime: 0:00:24.006385\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 405.72858133892134\tTime: 0:00:24.850158\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 405.6686416105986\tTime: 0:00:24.051928\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 405.7546144469834\tTime: 0:00:31.855570\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 405.6730248283283\tTime: 0:00:24.594913\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 405.6029874893036\tTime: 0:00:25.134804\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 405.39876313102997\tTime: 0:00:31.974192\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 405.3126666172986\tTime: 0:00:24.690971\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 405.6724133105182\tTime: 0:00:24.679299\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 405.65970727221327\tTime: 0:00:31.148537\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 405.54052264202727\tTime: 0:00:24.441561\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 405.56272182580824\tTime: 0:00:25.157290\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 405.4087561651339\tTime: 0:00:24.817774\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 405.5341379964431\tTime: 0:00:31.868086\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 405.434601828735\tTime: 0:00:26.091682\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 405.5457152907075\tTime: 0:00:25.234654\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 405.5011079903427\tTime: 0:00:24.825551\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 405.2987642476891\tTime: 0:00:32.226380\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 405.3550477254241\tTime: 0:00:25.270396\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 405.28271370505155\tTime: 0:00:25.432994\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 405.4888473668002\tTime: 0:00:25.502506\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 405.31848621131655\tTime: 0:00:32.628514\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 405.36857516937954\tTime: 0:00:25.388322\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 405.1622947991894\tTime: 0:00:25.629739\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 405.2584474896915\tTime: 0:00:25.621857\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 405.2126167863843\tTime: 0:00:25.422831\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 405.34538910134796\tTime: 0:00:32.971687\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 405.12059764478954\tTime: 0:00:25.086524\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 405.0290774367853\tTime: 0:00:34.042316\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 405.2705640862437\tTime: 0:00:24.966423\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 405.29782309503827\tTime: 0:00:25.637190\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 405.1951477193119\tTime: 0:00:32.756503\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 405.10904617326787\tTime: 0:00:25.657078\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 405.17815128556856\tTime: 0:00:25.415577\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 405.12787866646215\tTime: 0:00:25.610081\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 405.1566709511115\tTime: 0:00:25.797008\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 404.9390361776993\tTime: 0:00:32.891714\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 405.1113924798296\tTime: 0:00:25.697760\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 405.02598025041374\tTime: 0:00:25.964816\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 405.159807734804\tTime: 0:00:25.659681\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 405.0057590960703\tTime: 0:00:26.618693\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 404.995465886357\tTime: 0:00:32.861253\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 404.95126598291864\tTime: 0:00:26.004356\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 405.0506918438041\tTime: 0:00:25.912853\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 405.1448396168919\tTime: 0:00:26.118668\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 405.1379150445723\tTime: 0:00:25.934759\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 405.11815487944784\tTime: 0:00:25.723134\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 404.958068714502\tTime: 0:00:26.224885\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 405.01973368120207\tTime: 0:00:26.292045\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 405.0057690504988\tTime: 0:00:33.358591\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 404.87501135025246\tTime: 0:00:25.670186\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 405.0594094988904\tTime: 0:00:25.438455\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 405.0833950801761\tTime: 0:00:33.173004\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 405.10607435392455\tTime: 0:00:26.414517\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 404.91171384257694\tTime: 0:00:26.002789\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 404.82195016879183\tTime: 0:00:26.297061\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 405.0780391935605\tTime: 0:00:26.313569\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 404.90153373133415\tTime: 0:00:26.011524\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 405.04550521920606\tTime: 0:00:32.900412\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 404.8474053690147\tTime: 0:00:25.555802\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 404.917119280942\tTime: 0:00:25.514254\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 404.9161459508764\tTime: 0:00:26.080333\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 404.97384247994717\tTime: 0:00:26.266236\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 404.90615883065806\tTime: 0:00:32.808304\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 404.87454778977894\tTime: 0:00:25.988775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 07:30:50,330] Trial 18 finished with values: [-0.0056879246977228235, 0.9619047619047619] and parameters: {'num_topics': 21, 'dropout': 0.4448893746990935, 'num_neurons': 200, 'num_layers': 1, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 427.7054999576845\tTime: 0:00:18.855267\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 416.3579912242346\tTime: 0:00:16.001401\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 415.1818272980845\tTime: 0:00:15.855278\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 413.90716808482375\tTime: 0:00:16.011158\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 412.55599061183517\tTime: 0:00:15.828273\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 411.65619864837225\tTime: 0:00:17.081808\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 411.01781996986193\tTime: 0:00:16.098004\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 410.61935381873917\tTime: 0:00:23.939842\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 410.1668934880553\tTime: 0:00:16.430411\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 410.0585715270137\tTime: 0:00:16.051772\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 409.7865707338919\tTime: 0:00:17.019462\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 409.40825510107527\tTime: 0:00:23.781052\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 408.99264713012724\tTime: 0:00:16.241443\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 408.8446782229134\tTime: 0:00:15.727192\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 408.6307341549006\tTime: 0:00:16.375850\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 408.52021527719006\tTime: 0:00:15.944179\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 408.3036064343663\tTime: 0:00:23.110484\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 408.17781922640205\tTime: 0:00:16.728728\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 408.0976473975377\tTime: 0:00:16.529538\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 407.999757971477\tTime: 0:00:15.990396\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 407.9118508904475\tTime: 0:00:16.300190\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 407.88278476718244\tTime: 0:00:22.976924\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 408.00843599251016\tTime: 0:00:16.222177\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 407.82393267955877\tTime: 0:00:16.004397\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 407.7905802748627\tTime: 0:00:16.410842\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 407.6531890683184\tTime: 0:00:15.936106\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 407.7508120022051\tTime: 0:00:16.117771\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 407.4947019666131\tTime: 0:00:23.238415\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 407.6999697032743\tTime: 0:00:16.417954\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 407.4356381178064\tTime: 0:00:16.078852\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 407.3847321619579\tTime: 0:00:16.189718\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 407.500649866236\tTime: 0:00:16.492677\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 407.39886127949154\tTime: 0:00:23.214033\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 407.52989810779167\tTime: 0:00:16.075226\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 407.4980476096442\tTime: 0:00:16.218808\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 407.27779195641034\tTime: 0:00:23.278453\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 407.22284722075295\tTime: 0:00:16.373994\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 407.163951605462\tTime: 0:00:16.611499\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 407.2550654817738\tTime: 0:00:16.694017\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 407.3902744645326\tTime: 0:00:16.432048\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 407.3129264609428\tTime: 0:00:15.874966\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 407.3937733543383\tTime: 0:00:25.670389\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 407.30936788129793\tTime: 0:00:16.243716\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 407.1079782944971\tTime: 0:00:16.086470\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 407.15274391068135\tTime: 0:00:16.339795\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 407.12533661396037\tTime: 0:00:16.931815\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 407.1120156050177\tTime: 0:00:16.588649\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 407.2022826569905\tTime: 0:00:16.471921\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 406.98673835030655\tTime: 0:00:23.366456\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 406.8551186553191\tTime: 0:00:16.712447\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 406.94836593831104\tTime: 0:00:17.805317\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 407.07013126328235\tTime: 0:00:23.336150\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 406.8817809861158\tTime: 0:00:16.198206\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 407.1367797254664\tTime: 0:00:16.468137\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 407.0509729186649\tTime: 0:00:23.752167\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 406.78860735120224\tTime: 0:00:16.694045\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 406.93149556950794\tTime: 0:00:16.308448\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 406.92378298111555\tTime: 0:00:16.857643\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 406.92794789932907\tTime: 0:00:17.030733\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 406.995540342544\tTime: 0:00:23.294237\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 406.88649857714114\tTime: 0:00:16.561292\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 406.96542246594777\tTime: 0:00:16.224642\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 406.62522619694107\tTime: 0:00:16.737248\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 406.8820191210247\tTime: 0:00:16.097257\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 406.834761151017\tTime: 0:00:16.985456\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 406.70925068351295\tTime: 0:00:16.660955\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 406.668232964777\tTime: 0:00:23.259606\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 406.86930298136235\tTime: 0:00:16.829505\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 406.72944381118117\tTime: 0:00:16.570501\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 406.7401424710139\tTime: 0:00:16.430011\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 406.9048032652289\tTime: 0:00:17.453344\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 406.89713982453173\tTime: 0:00:23.501878\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 406.8851740504283\tTime: 0:00:16.979689\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 406.7132519597368\tTime: 0:00:16.693323\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 406.78044086290714\tTime: 0:00:24.542827\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 406.72348213698\tTime: 0:00:16.891851\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 406.8722188208197\tTime: 0:00:16.613432\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 406.735178884387\tTime: 0:00:16.893589\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 406.859851490211\tTime: 0:00:16.615049\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 406.6258786711639\tTime: 0:00:23.164481\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 406.7968731633896\tTime: 0:00:16.706812\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 406.59488506198045\tTime: 0:00:16.719183\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 406.93542371190426\tTime: 0:00:16.773179\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 406.84977408222375\tTime: 0:00:16.863193\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 406.74032087935143\tTime: 0:00:17.896741\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 406.73328824087395\tTime: 0:00:16.583523\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 406.7147899740465\tTime: 0:00:24.141871\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 406.77065194968685\tTime: 0:00:17.017829\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 406.7729521940589\tTime: 0:00:16.967677\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 406.7768145123373\tTime: 0:00:16.788394\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 406.5170163426004\tTime: 0:00:23.977396\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 406.7203697802062\tTime: 0:00:16.798524\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 406.46387839082365\tTime: 0:00:16.536559\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 406.7133969417645\tTime: 0:00:16.818903\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 406.60614675310785\tTime: 0:00:23.666850\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 406.7524372555104\tTime: 0:00:16.882298\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 406.70204566078013\tTime: 0:00:16.768969\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 406.6050937876138\tTime: 0:00:17.868009\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 406.7484201477046\tTime: 0:00:24.970262\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 406.58340246335007\tTime: 0:00:16.857349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 08:01:53,741] Trial 19 finished with values: [-0.021238737195587866, 0.8941176470588236] and parameters: {'num_topics': 17, 'dropout': 0.5038375802935597, 'num_neurons': 200, 'num_layers': 2, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 435.2601405873862\tTime: 0:00:23.708240\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 420.9563911912637\tTime: 0:00:20.975305\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 417.24252837489655\tTime: 0:00:21.168311\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 415.3661584104416\tTime: 0:00:27.427544\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 414.5777170153582\tTime: 0:00:20.567319\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 413.7791662185337\tTime: 0:00:20.432417\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 413.3115819519084\tTime: 0:00:20.268111\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 412.909660218254\tTime: 0:00:27.065842\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 412.5656077652477\tTime: 0:00:20.340288\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 412.29999947840264\tTime: 0:00:27.441452\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 412.07004060819133\tTime: 0:00:20.492393\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 412.01380040099815\tTime: 0:00:20.118405\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 411.66289794205596\tTime: 0:00:20.346461\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 411.4901557313412\tTime: 0:00:27.182920\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 411.45931536893244\tTime: 0:00:20.708230\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 411.5303212995679\tTime: 0:00:20.141799\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 411.274804305485\tTime: 0:00:26.891591\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 411.1183057753611\tTime: 0:00:20.108891\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 411.1451325195239\tTime: 0:00:20.336546\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 410.93771286314933\tTime: 0:00:20.278887\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 411.0884254459584\tTime: 0:00:27.294142\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 410.9400113811075\tTime: 0:00:20.698311\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 410.77129329405045\tTime: 0:00:20.387524\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 410.74867407746325\tTime: 0:00:27.234850\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 410.5245831775888\tTime: 0:00:20.367039\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 410.563649056952\tTime: 0:00:20.357565\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 410.5732543458611\tTime: 0:00:20.582089\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 410.5464505225965\tTime: 0:00:27.279022\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 410.40783771298806\tTime: 0:00:20.001253\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 410.5098206498486\tTime: 0:00:21.334912\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 410.3215424780077\tTime: 0:00:27.075007\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 410.301630719058\tTime: 0:00:20.039354\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 410.29771513907696\tTime: 0:00:26.974931\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 410.26968122760803\tTime: 0:00:20.300947\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 410.24355467280714\tTime: 0:00:20.365931\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 410.2515006576535\tTime: 0:00:20.500459\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 410.31170522520074\tTime: 0:00:27.018482\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 410.1701737110227\tTime: 0:00:20.272790\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 410.1552367052913\tTime: 0:00:20.232093\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 410.0785672557925\tTime: 0:00:20.023541\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 410.1238774637027\tTime: 0:00:27.697707\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 410.10055526610574\tTime: 0:00:20.875645\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 410.1146502226853\tTime: 0:00:20.056871\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 409.966801980777\tTime: 0:00:20.063549\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 409.97862134029657\tTime: 0:00:26.970672\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 410.04809976203404\tTime: 0:00:20.646898\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 409.9610905631136\tTime: 0:00:23.635454\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 409.74046038828004\tTime: 0:00:23.988018\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 409.7703220209885\tTime: 0:00:20.099259\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 409.86970901919716\tTime: 0:00:20.090982\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 409.96049682369244\tTime: 0:00:20.325527\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 409.7204707203157\tTime: 0:00:27.019147\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 409.92253415948477\tTime: 0:00:20.457315\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 409.7958611910404\tTime: 0:00:20.543650\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 409.8531513149543\tTime: 0:00:20.700047\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 409.7950941123265\tTime: 0:00:20.479179\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 409.8162512033472\tTime: 0:00:28.206147\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 409.71353171223814\tTime: 0:00:21.575246\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 409.7087827151743\tTime: 0:00:22.438019\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 409.6893245255374\tTime: 0:00:22.256625\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 409.71693190259913\tTime: 0:00:32.928622\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 409.6592786440702\tTime: 0:00:21.732363\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 409.7149583177589\tTime: 0:00:23.506434\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 409.7216314948422\tTime: 0:00:38.610193\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 409.58280097343294\tTime: 0:00:25.107559\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 409.5749081033603\tTime: 0:00:22.522065\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 409.62115156649645\tTime: 0:00:23.762725\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 409.62547542497697\tTime: 0:00:22.346011\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 409.5678674205291\tTime: 0:00:22.679817\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 409.5515368315326\tTime: 0:00:22.782830\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 409.57939196734185\tTime: 0:00:22.584340\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 409.4562614531027\tTime: 0:00:22.840079\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 409.38358079087675\tTime: 0:00:23.901145\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 409.5592611742317\tTime: 0:00:21.284133\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 409.4242119765807\tTime: 0:00:22.090318\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 409.4805246196894\tTime: 0:00:22.863639\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 409.5190662569703\tTime: 0:00:22.374484\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 409.45552684566127\tTime: 0:00:22.532385\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 409.34544221612174\tTime: 0:00:22.900693\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 409.3018682662515\tTime: 0:00:23.233476\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 409.3818147430156\tTime: 0:00:28.181917\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 409.19172130215384\tTime: 0:00:23.711424\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 409.33464353448477\tTime: 0:00:21.555309\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 409.46499023804824\tTime: 0:00:21.102193\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 409.45515092823763\tTime: 0:00:30.764563\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 409.3304336193156\tTime: 0:00:26.236615\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 409.3411410948784\tTime: 0:00:22.579107\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 409.2758781348736\tTime: 0:00:23.110338\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 409.1598857172831\tTime: 0:00:22.201629\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 409.44039273489807\tTime: 0:00:22.266103\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 409.4201008710529\tTime: 0:00:23.741807\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 409.215479180478\tTime: 0:00:22.826108\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 409.3639687300177\tTime: 0:00:23.122521\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 409.31176473137884\tTime: 0:00:23.813882\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 409.2327416656088\tTime: 0:00:21.393601\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 409.1468347635386\tTime: 0:00:23.073838\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 409.1404591540778\tTime: 0:00:23.463413\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 409.14410923365443\tTime: 0:00:23.008059\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 409.0182840078542\tTime: 0:00:24.396216\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 409.3500292241452\tTime: 0:00:22.008521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 08:41:17,853] Trial 20 finished with values: [-0.008877864617609412, 0.9027027027027027] and parameters: {'num_topics': 37, 'dropout': 0.5238555834109606, 'num_neurons': 300, 'num_layers': 1, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 432.0620901420155\tTime: 0:00:24.453382\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 419.89490562467086\tTime: 0:00:21.148468\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 416.0832329809392\tTime: 0:00:20.826301\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 411.77953611187513\tTime: 0:00:20.952808\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 408.8344380177772\tTime: 0:00:28.138004\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 406.8874654790703\tTime: 0:00:20.931118\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 405.5720126135466\tTime: 0:00:21.341248\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 404.4583929129758\tTime: 0:00:21.467269\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 403.5883648378141\tTime: 0:00:29.921883\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 402.9772540205606\tTime: 0:00:26.937856\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 402.3394104201342\tTime: 0:00:21.545024\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 401.93476991532197\tTime: 0:00:32.001130\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 401.561436308701\tTime: 0:00:21.478689\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 401.11936542611716\tTime: 0:00:21.015819\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 400.7065606736746\tTime: 0:00:28.185138\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 400.4261808964305\tTime: 0:00:21.531877\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 400.1359693394785\tTime: 0:00:22.018889\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 399.9795078721531\tTime: 0:00:21.524547\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 399.5336375200999\tTime: 0:00:21.573825\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 399.4851895367275\tTime: 0:00:28.713595\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 399.273590820239\tTime: 0:00:21.573912\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 399.1477630231842\tTime: 0:00:34.646270\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 398.8487618894813\tTime: 0:00:22.776448\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 398.73118737893066\tTime: 0:00:21.949881\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 398.5000766233873\tTime: 0:00:28.000049\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 398.4220700112959\tTime: 0:00:21.944119\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 398.25238702421507\tTime: 0:00:30.301579\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 398.27182818847325\tTime: 0:00:23.868643\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 398.1667429594641\tTime: 0:00:21.977213\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 397.88521258251524\tTime: 0:00:21.778883\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 397.96610869707035\tTime: 0:00:28.405328\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 397.86519859782874\tTime: 0:00:22.116660\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 397.6673776552979\tTime: 0:00:21.326777\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 397.66318177183246\tTime: 0:00:34.614825\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 397.536300201175\tTime: 0:00:22.112711\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 397.5372722456133\tTime: 0:00:21.692653\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 397.4013133013499\tTime: 0:00:28.464781\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 397.34308466950415\tTime: 0:00:21.933590\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 397.13944890814247\tTime: 0:00:21.626996\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 397.09545705593405\tTime: 0:00:21.690525\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 397.0075909313172\tTime: 0:00:21.647935\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 397.0765611629476\tTime: 0:00:28.458264\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 397.0364416201784\tTime: 0:00:21.635531\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 396.99430577301024\tTime: 0:00:21.877768\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 396.88579306160904\tTime: 0:00:21.317469\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 396.88797473441144\tTime: 0:00:39.615384\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 396.6787002851595\tTime: 0:00:28.316478\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 396.6510892348769\tTime: 0:00:28.972462\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 396.6811755217737\tTime: 0:00:21.858556\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 396.5648223571617\tTime: 0:00:22.133995\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 396.67109462422655\tTime: 0:00:28.621823\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 396.5440484932742\tTime: 0:00:21.592615\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 396.53717049739527\tTime: 0:00:21.519282\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 396.39519360518693\tTime: 0:00:21.698380\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 396.36604520177445\tTime: 0:00:28.574566\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 396.43246283875914\tTime: 0:00:22.242113\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 396.2616526687566\tTime: 0:00:21.432881\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 396.16535661244404\tTime: 0:00:22.830670\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 396.1994439184392\tTime: 0:00:34.407742\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 396.1691450255186\tTime: 0:00:21.771464\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 396.184453614273\tTime: 0:00:28.881266\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 396.0955712563708\tTime: 0:00:21.841836\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 396.06720282467745\tTime: 0:00:22.463733\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 395.907704632196\tTime: 0:00:29.173632\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 395.86346755576244\tTime: 0:00:21.946004\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 395.7958332745618\tTime: 0:00:29.619418\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 395.860886309994\tTime: 0:00:21.610301\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 395.8913911971409\tTime: 0:00:24.966985\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 395.8845604388823\tTime: 0:00:25.982194\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 395.83515110014434\tTime: 0:00:23.640684\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 395.7426646513908\tTime: 0:00:35.168344\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 395.80084141735784\tTime: 0:00:21.868444\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 395.76152891794555\tTime: 0:00:22.032897\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 395.6693655289673\tTime: 0:00:21.674477\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 395.6782698938704\tTime: 0:00:28.561749\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 395.5476506005863\tTime: 0:00:21.645793\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 395.63950430031315\tTime: 0:00:22.361955\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 395.5245943809124\tTime: 0:00:21.857459\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 395.68766687442405\tTime: 0:00:28.364564\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 395.48093319204656\tTime: 0:00:22.087072\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 395.4167531342859\tTime: 0:00:21.939348\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 395.57386659831536\tTime: 0:00:21.898549\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 395.4780554438896\tTime: 0:00:37.413067\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 395.3579273101914\tTime: 0:00:22.612976\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 395.34258463394735\tTime: 0:00:28.496164\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 395.43809260072265\tTime: 0:00:21.950118\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 395.2827826514072\tTime: 0:00:22.156966\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 395.39571479849593\tTime: 0:00:28.870827\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 395.41618264635287\tTime: 0:00:22.803104\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 395.1774042626406\tTime: 0:00:22.383709\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 395.15877622091983\tTime: 0:00:22.021443\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 395.1819618481649\tTime: 0:00:29.043889\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 395.2185418753056\tTime: 0:00:22.158453\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 395.0801928029555\tTime: 0:00:21.879718\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 395.25491550416575\tTime: 0:00:36.419559\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 395.17110652546216\tTime: 0:00:22.344364\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 395.154204640424\tTime: 0:00:22.200953\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 395.18106165192677\tTime: 0:00:28.966475\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 395.1895498041732\tTime: 0:00:22.598444\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 394.9684493468719\tTime: 0:00:21.930920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 09:23:35,945] Trial 21 finished with values: [0.06312547111192116, 0.9230769230769231] and parameters: {'num_topics': 39, 'dropout': 0.0867707153667109, 'num_neurons': 300, 'num_layers': 2, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 433.9391776334349\tTime: 0:00:15.501825\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 419.46182087288656\tTime: 0:00:14.699790\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 415.6911228977422\tTime: 0:00:14.477923\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 413.44710789689594\tTime: 0:00:14.292728\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 411.5307711589276\tTime: 0:00:14.093025\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 410.4545347087048\tTime: 0:00:27.028812\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 409.4623997357632\tTime: 0:00:17.386009\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 408.7069299425685\tTime: 0:00:13.945980\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 408.248344883409\tTime: 0:00:20.893218\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 407.9325338421185\tTime: 0:00:14.046491\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 407.4557632908885\tTime: 0:00:13.943867\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 407.42881275918245\tTime: 0:00:13.988192\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 406.95352347099976\tTime: 0:00:14.172039\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 406.7387164164762\tTime: 0:00:13.950685\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 406.71748119604774\tTime: 0:00:13.925242\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 406.63228796406\tTime: 0:00:20.875007\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 406.19446435330156\tTime: 0:00:13.606758\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 406.25710613962707\tTime: 0:00:13.761428\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 406.0033353580127\tTime: 0:00:13.572108\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 406.00316433284905\tTime: 0:00:14.438826\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 405.95955581797045\tTime: 0:00:14.413334\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 405.88856344152\tTime: 0:00:15.277314\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 405.7926821653079\tTime: 0:00:22.737786\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 405.7210897682286\tTime: 0:00:16.180486\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 405.6383369507212\tTime: 0:00:27.701929\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 405.59152733405267\tTime: 0:00:16.668166\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 405.41086393273247\tTime: 0:00:16.780804\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 405.5923523027275\tTime: 0:00:16.938240\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 405.4012530605276\tTime: 0:00:16.698339\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 405.41890075633086\tTime: 0:00:17.190542\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 405.1628728172252\tTime: 0:00:24.342460\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 405.32811181313707\tTime: 0:00:17.805432\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 405.1929802986064\tTime: 0:00:17.135196\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 405.27895563278133\tTime: 0:00:17.764937\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 405.08102603637724\tTime: 0:00:17.430925\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 405.2655081548442\tTime: 0:00:17.606166\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 405.0981031340508\tTime: 0:00:17.944481\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 404.98657386347605\tTime: 0:00:26.101728\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 404.9189341826408\tTime: 0:00:17.724012\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 404.85568807801803\tTime: 0:00:17.942318\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 404.88792499900086\tTime: 0:00:31.394988\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 404.96396518903276\tTime: 0:00:18.246660\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 404.79511626184836\tTime: 0:00:18.083193\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 404.8399207774416\tTime: 0:00:18.028352\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 404.8057652230027\tTime: 0:00:17.945513\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 404.83515370813114\tTime: 0:00:18.662368\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 404.803520480998\tTime: 0:00:17.589018\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 404.64483058076854\tTime: 0:00:24.597099\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 404.7266622646788\tTime: 0:00:17.754670\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 404.7699239907752\tTime: 0:00:18.049106\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 404.6845661250282\tTime: 0:00:17.626996\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 404.5362623302678\tTime: 0:00:18.226244\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 404.5637617881004\tTime: 0:00:18.270098\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 404.57122878494556\tTime: 0:00:17.914696\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 404.5309118432869\tTime: 0:00:24.955351\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 404.54524221659193\tTime: 0:00:21.240007\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 404.5510236825773\tTime: 0:00:29.145482\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 404.5516920251143\tTime: 0:00:18.430582\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 404.66689712454394\tTime: 0:00:18.320507\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 404.43526734289196\tTime: 0:00:18.729093\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 404.2580227551625\tTime: 0:00:24.683583\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 404.39856470363867\tTime: 0:00:18.450106\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 404.23395537124804\tTime: 0:00:18.235099\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 404.2339113293298\tTime: 0:00:18.092281\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 404.3163189742722\tTime: 0:00:18.202285\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 404.18921414525033\tTime: 0:00:18.050357\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 404.2832763322919\tTime: 0:00:17.973877\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 404.43884726395004\tTime: 0:00:25.458505\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 404.306066978097\tTime: 0:00:17.998816\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 404.11449333942204\tTime: 0:00:18.651941\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 404.2986846489929\tTime: 0:00:29.117912\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 404.21856799190596\tTime: 0:00:22.350575\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 404.14337638935905\tTime: 0:00:21.586722\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 404.258236352956\tTime: 0:00:24.952849\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 404.09544423639323\tTime: 0:00:18.058567\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 403.89929802545277\tTime: 0:00:18.265580\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 404.3241248624746\tTime: 0:00:17.967507\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 404.30163913073375\tTime: 0:00:18.237307\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 404.1574422907616\tTime: 0:00:18.391081\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 404.15358239680893\tTime: 0:00:18.125030\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 404.03816461789097\tTime: 0:00:25.112082\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 404.05062058331924\tTime: 0:00:18.541847\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 403.99193522319075\tTime: 0:00:18.061694\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 403.9286120911264\tTime: 0:00:18.245338\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 403.97609174001354\tTime: 0:00:18.984819\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 403.93416129935633\tTime: 0:00:29.455689\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 403.94261543757756\tTime: 0:00:20.242217\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 404.0736872716726\tTime: 0:00:17.986491\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 403.86478892350533\tTime: 0:00:18.462650\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 404.06773000533644\tTime: 0:00:18.142605\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 403.9034670209415\tTime: 0:00:25.311564\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 404.03642123380257\tTime: 0:00:18.135104\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 403.7353629862169\tTime: 0:00:18.111186\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 403.9582927442427\tTime: 0:00:18.359066\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 403.8536459876086\tTime: 0:00:18.345171\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 403.6981080414739\tTime: 0:00:17.837344\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 404.0152310470619\tTime: 0:00:25.340339\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 403.77643705216093\tTime: 0:00:18.201071\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 403.8922349359743\tTime: 0:00:18.846665\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 403.80908750434907\tTime: 0:00:18.323494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 09:56:15,756] Trial 22 finished with values: [0.041256692534933206, 0.9] and parameters: {'num_topics': 49, 'dropout': 0.22067237951632693, 'num_neurons': 100, 'num_layers': 1, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 436.5222726481983\tTime: 0:00:13.427136\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 422.34673989159\tTime: 0:00:12.487130\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 419.12147260600034\tTime: 0:00:12.482738\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 417.35657446294573\tTime: 0:00:12.208636\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 416.0051654301136\tTime: 0:00:12.103859\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 415.0810449534647\tTime: 0:00:11.524460\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 414.2584992086413\tTime: 0:00:12.004095\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 413.6412661239703\tTime: 0:00:12.021914\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 413.1942007997043\tTime: 0:00:12.416276\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 412.96772542850323\tTime: 0:00:11.875192\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 412.66846963039757\tTime: 0:00:18.997681\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 412.49439088153775\tTime: 0:00:11.871049\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 412.1334284991396\tTime: 0:00:12.179975\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 412.13328803517356\tTime: 0:00:11.785357\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 412.0115321136477\tTime: 0:00:11.853118\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 412.01683635477787\tTime: 0:00:12.631733\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 411.8094195635156\tTime: 0:00:11.693211\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 411.60607788021656\tTime: 0:00:12.229929\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 411.7816006823963\tTime: 0:00:11.672684\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 411.7046233067921\tTime: 0:00:21.337363\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 411.3951240343837\tTime: 0:00:16.207918\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 411.5665358045\tTime: 0:00:12.716205\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 411.5603930772422\tTime: 0:00:12.916764\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 411.34640426939933\tTime: 0:00:13.020076\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 411.3297839734729\tTime: 0:00:20.147408\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 411.26019899159803\tTime: 0:00:13.589132\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 411.20582591930435\tTime: 0:00:13.384350\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 410.9766639176282\tTime: 0:00:13.162385\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 411.1669985789043\tTime: 0:00:13.339349\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 411.0001839549011\tTime: 0:00:14.215495\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 411.0480056541154\tTime: 0:00:20.537979\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 411.0240519196546\tTime: 0:00:13.546247\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 411.13393738715865\tTime: 0:00:13.718722\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 411.0895581203453\tTime: 0:00:13.633605\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 410.96419542651677\tTime: 0:00:13.998358\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 410.93168613620657\tTime: 0:00:13.714147\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 410.87535446581376\tTime: 0:00:13.537829\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 410.9445618773156\tTime: 0:00:17.676276\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 410.8181347208117\tTime: 0:00:16.782524\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 410.8114886520984\tTime: 0:00:15.165501\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 410.71473098211936\tTime: 0:00:18.436728\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 410.8371712981574\tTime: 0:00:21.582609\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 410.6231810577583\tTime: 0:00:13.666976\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 410.66494433013384\tTime: 0:00:13.595457\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 410.5475375755802\tTime: 0:00:13.977570\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 410.7087732382645\tTime: 0:00:14.085508\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 410.669807637831\tTime: 0:00:13.824585\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 410.6783271593837\tTime: 0:00:20.474001\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 410.59220078236666\tTime: 0:00:13.648209\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 410.62251605785235\tTime: 0:00:13.970656\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 410.5799632266513\tTime: 0:00:14.040808\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 410.61062143403717\tTime: 0:00:13.683540\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 410.4041073368032\tTime: 0:00:14.025315\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 410.4572733519874\tTime: 0:00:22.410455\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 410.57582640857737\tTime: 0:00:14.864941\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 410.4238147177438\tTime: 0:00:14.281655\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 410.5983467053266\tTime: 0:00:13.896281\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 410.3212500161622\tTime: 0:00:21.712924\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 410.2669628972368\tTime: 0:00:14.389420\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 410.1938056346033\tTime: 0:00:19.870387\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 410.2516464110575\tTime: 0:00:14.294293\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 410.4421599949221\tTime: 0:00:14.304329\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 410.3608439944896\tTime: 0:00:13.975243\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 410.28907954374483\tTime: 0:00:13.953877\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 410.32419285379285\tTime: 0:00:21.329213\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 410.2108264585332\tTime: 0:00:14.061924\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 410.3073578581067\tTime: 0:00:13.799157\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 410.2626563396266\tTime: 0:00:14.022342\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 410.1691216638339\tTime: 0:00:14.629398\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 410.2782467011538\tTime: 0:00:13.889986\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 410.30937827651303\tTime: 0:00:21.084387\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 410.1769989961822\tTime: 0:00:14.134195\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 410.35147298360744\tTime: 0:00:14.445236\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 410.3265880509244\tTime: 0:00:13.988317\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 410.03629428728937\tTime: 0:00:14.374831\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 410.18651983774356\tTime: 0:00:14.420385\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 410.12782371507797\tTime: 0:00:14.355153\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 410.0103312642815\tTime: 0:00:14.793907\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 410.06815356443474\tTime: 0:00:25.196853\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 409.98550580104427\tTime: 0:00:19.320698\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 409.9796226603052\tTime: 0:00:14.506532\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 410.07462173906146\tTime: 0:00:14.157697\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 409.9866103018271\tTime: 0:00:14.137269\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 410.1210930153558\tTime: 0:00:14.096991\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 409.87346385903294\tTime: 0:00:14.093515\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 410.0425160989924\tTime: 0:00:13.919722\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 410.0330363607726\tTime: 0:00:21.366145\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 410.18300346340646\tTime: 0:00:14.265865\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 410.0249610418195\tTime: 0:00:14.180699\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 409.9366261407555\tTime: 0:00:14.414715\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 410.0565988237465\tTime: 0:00:14.257997\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 409.89287268058143\tTime: 0:00:21.263372\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 409.92805813268734\tTime: 0:00:14.384628\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 409.99633045235277\tTime: 0:00:14.202328\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 409.8044886318222\tTime: 0:00:14.466502\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 409.86674296681053\tTime: 0:00:14.418233\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 409.85201842017415\tTime: 0:00:14.297198\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 410.0017710434416\tTime: 0:00:22.446300\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 409.8349308170889\tTime: 0:00:19.120012\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 409.91362465207254\tTime: 0:00:14.115987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 10:22:04,862] Trial 23 finished with values: [0.01097526794443546, 0.8295454545454546] and parameters: {'num_topics': 44, 'dropout': 0.42870312432478136, 'num_neurons': 50, 'num_layers': 1, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 430.2933097350932\tTime: 0:00:18.770370\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 414.64390066817356\tTime: 0:00:16.439563\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 409.72317314564054\tTime: 0:00:16.098310\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 407.8159006311475\tTime: 0:00:15.979982\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 406.43991554677507\tTime: 0:00:16.189463\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 405.6482247470473\tTime: 0:00:22.497713\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 404.9409695775032\tTime: 0:00:16.114301\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 404.57074711549313\tTime: 0:00:16.003202\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 404.1765996803416\tTime: 0:00:16.011113\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 403.84325720068836\tTime: 0:00:15.816745\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 403.57040058383643\tTime: 0:00:22.650382\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 403.3866532932041\tTime: 0:00:16.368357\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 403.03068917263784\tTime: 0:00:29.066217\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 402.7981296987841\tTime: 0:00:17.114687\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 402.7621638341726\tTime: 0:00:15.862395\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 402.7024891287355\tTime: 0:00:15.944224\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 402.39332404596166\tTime: 0:00:15.855269\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 402.31473552222036\tTime: 0:00:15.910285\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 402.17055491788443\tTime: 0:00:22.397761\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 402.1014357004156\tTime: 0:00:16.281962\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 402.05161415283186\tTime: 0:00:15.935935\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 402.0300885275615\tTime: 0:00:15.950501\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 402.03774135265036\tTime: 0:00:15.775394\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 402.02335308938444\tTime: 0:00:23.375230\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 401.7772435445347\tTime: 0:00:16.072499\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 401.71194484409085\tTime: 0:00:16.324237\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 401.8319208697129\tTime: 0:00:16.515266\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 401.67494702480866\tTime: 0:00:26.826033\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 401.6765889178045\tTime: 0:00:21.942050\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 401.727747995303\tTime: 0:00:28.513454\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 401.457627781069\tTime: 0:00:19.586085\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 401.3206158345793\tTime: 0:00:19.960345\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 401.56157012413723\tTime: 0:00:27.930326\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 401.3722858938327\tTime: 0:00:19.297050\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 401.498137530091\tTime: 0:00:20.299024\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 401.298272072825\tTime: 0:00:27.888639\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 401.1898529183593\tTime: 0:00:19.842711\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 401.24157730454937\tTime: 0:00:21.719537\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 401.2894598690335\tTime: 0:00:27.571229\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 401.2140989677368\tTime: 0:00:19.399753\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 401.19844109241\tTime: 0:00:27.082749\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 401.123956731809\tTime: 0:00:20.980620\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 401.0411162712518\tTime: 0:00:21.438606\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 401.0151515585623\tTime: 0:00:27.738273\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 400.9209013760767\tTime: 0:00:20.060325\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 401.0641395788667\tTime: 0:00:19.845781\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 400.8906157434834\tTime: 0:00:26.658347\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 401.0422301387478\tTime: 0:00:19.585911\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 400.81902683596394\tTime: 0:00:19.776870\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 400.9666856498721\tTime: 0:00:28.016985\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 400.9171512379636\tTime: 0:00:17.850181\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 400.8058400465118\tTime: 0:00:26.063970\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 400.85361054102725\tTime: 0:00:18.002955\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 400.7988942429756\tTime: 0:00:20.687365\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 400.74921775088393\tTime: 0:00:25.933477\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 400.83687740375575\tTime: 0:00:25.024814\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 400.9211424128301\tTime: 0:00:27.196195\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 400.8710213875489\tTime: 0:00:18.802939\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 400.76781075007756\tTime: 0:00:19.427344\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 400.7047742761698\tTime: 0:00:16.209766\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 400.7391422529715\tTime: 0:00:22.729490\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 400.7317456350382\tTime: 0:00:16.073219\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 400.7694272978494\tTime: 0:00:15.923733\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 400.61757454728286\tTime: 0:00:15.901935\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 400.6407489712043\tTime: 0:00:17.120418\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 400.62927030292025\tTime: 0:00:23.001740\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 400.5058224591303\tTime: 0:00:15.845649\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 400.6636653146275\tTime: 0:00:16.379005\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 400.5977828291911\tTime: 0:00:16.132762\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 400.6169788610547\tTime: 0:00:16.294173\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 400.49713650394006\tTime: 0:00:23.179439\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 400.5753973029155\tTime: 0:00:25.400241\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 400.42295676354564\tTime: 0:00:20.643004\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 400.656128893908\tTime: 0:00:18.145389\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 400.5160133880085\tTime: 0:00:15.927128\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 400.4500442696579\tTime: 0:00:22.644388\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 400.47671361630654\tTime: 0:00:16.683128\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 400.65757548175026\tTime: 0:00:16.303773\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 400.44147133947376\tTime: 0:00:15.965558\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 400.52046189723916\tTime: 0:00:16.132524\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 400.27646871532477\tTime: 0:00:23.314416\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 400.3896319080061\tTime: 0:00:16.117201\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 400.4085589131733\tTime: 0:00:15.871051\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 400.3984367286236\tTime: 0:00:15.882495\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 400.4764197953693\tTime: 0:00:23.224214\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 400.51811265210074\tTime: 0:00:16.124835\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 400.40944691431815\tTime: 0:00:15.916594\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 400.26712514340255\tTime: 0:00:30.047919\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 400.4276936022484\tTime: 0:00:55.886291\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 400.3997690425647\tTime: 0:00:24.432914\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 400.2580270528309\tTime: 0:00:16.644262\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 400.2681499352925\tTime: 0:00:17.797405\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 400.303701607519\tTime: 0:00:27.579728\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 400.3941864447571\tTime: 0:00:18.111289\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 400.2300273757805\tTime: 0:00:18.361725\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 400.210927288446\tTime: 0:00:18.530822\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 400.27534037533854\tTime: 0:00:17.812406\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 400.39018175243785\tTime: 0:00:18.123404\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 400.1734091574278\tTime: 0:00:20.468338\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 400.284518358464\tTime: 0:00:34.739599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 10:57:10,059] Trial 24 finished with values: [0.029703412510495842, 0.9464285714285714] and parameters: {'num_topics': 28, 'dropout': 0.2824070696826984, 'num_neurons': 200, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 422.82315134874796\tTime: 0:00:16.630475\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 411.89731125374965\tTime: 0:00:30.077381\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 408.3941036503587\tTime: 0:00:19.446467\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 406.6305794021054\tTime: 0:00:26.147842\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 405.61030270866246\tTime: 0:00:27.563568\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 404.8860378817799\tTime: 0:00:14.223277\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 404.4237249074936\tTime: 0:00:14.439469\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 404.0600174904085\tTime: 0:00:23.837353\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 403.8162599088806\tTime: 0:00:16.509766\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 403.62386203617507\tTime: 0:00:16.046315\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 403.5430743918322\tTime: 0:00:16.246592\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 403.3050891299697\tTime: 0:00:23.561901\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 403.24371570680995\tTime: 0:00:14.425851\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 403.0493421555283\tTime: 0:00:14.839079\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 403.0634833212264\tTime: 0:00:16.268369\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 402.9485173852074\tTime: 0:00:14.491417\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 402.7877826029207\tTime: 0:00:15.037784\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 402.73301487977693\tTime: 0:00:15.062214\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 402.5577724912783\tTime: 0:00:14.188984\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 402.4940605855173\tTime: 0:00:15.228719\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 402.5095278574132\tTime: 0:00:17.913837\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 402.2887319101219\tTime: 0:00:18.163747\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 402.17875641197435\tTime: 0:00:17.808436\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 402.1722409629951\tTime: 0:00:17.770641\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 402.09677195880823\tTime: 0:00:19.770358\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 402.01967803190587\tTime: 0:00:20.450648\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 402.03692086530504\tTime: 0:00:19.541384\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 402.02843385175703\tTime: 0:00:19.702003\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 401.87945653521825\tTime: 0:00:19.807347\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 401.91406705479386\tTime: 0:00:20.364481\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 401.78112419218525\tTime: 0:00:20.467125\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 402.00187415075135\tTime: 0:00:20.224490\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 401.84040916888824\tTime: 0:00:19.911470\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 401.7111097005943\tTime: 0:00:20.209151\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 401.51855997693804\tTime: 0:00:20.100857\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 401.56170882495724\tTime: 0:00:19.974486\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 401.7729043687232\tTime: 0:00:19.707672\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 401.4521073341585\tTime: 0:00:20.397239\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 401.7611603469636\tTime: 0:00:17.341625\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 401.5844629589469\tTime: 0:00:17.431102\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 401.5228011144846\tTime: 0:00:17.584297\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 401.55281334924865\tTime: 0:00:17.778330\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 401.41925110813725\tTime: 0:00:17.795916\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 401.3439765275309\tTime: 0:00:17.672611\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 401.4818095858135\tTime: 0:00:17.699011\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 401.168941786208\tTime: 0:00:17.779550\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 401.08440767697283\tTime: 0:00:17.571274\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 401.21721514466026\tTime: 0:00:18.183825\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 401.1904228553092\tTime: 0:00:17.709141\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 401.36186573760625\tTime: 0:00:17.522433\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 401.173016673778\tTime: 0:00:24.798958\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 401.1550677003992\tTime: 0:00:17.884712\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 401.26373975612165\tTime: 0:00:17.605334\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 401.1430080755026\tTime: 0:00:17.530122\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 401.08708260823954\tTime: 0:00:17.779489\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 401.06405259699653\tTime: 0:00:17.526892\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 401.1926130867162\tTime: 0:00:17.968393\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 401.0886796695747\tTime: 0:00:24.683306\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 401.03981073509084\tTime: 0:00:20.748947\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 400.97165412188275\tTime: 0:00:19.811388\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 400.96869141212716\tTime: 0:00:25.484436\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 401.0051891223882\tTime: 0:00:17.555938\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 400.9648530065166\tTime: 0:00:17.608159\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 401.0776442488528\tTime: 0:00:17.666772\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 401.0266286106292\tTime: 0:00:17.865645\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 400.9248936529094\tTime: 0:00:17.544294\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 400.83042028552387\tTime: 0:00:17.944614\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 401.0044800438318\tTime: 0:00:17.753123\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 400.7465439399496\tTime: 0:00:24.788360\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 400.84553738927445\tTime: 0:00:18.232314\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 400.8487754436663\tTime: 0:00:17.798548\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 400.87826839519624\tTime: 0:00:17.845175\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 400.6579985265976\tTime: 0:00:18.501302\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 400.97440980891025\tTime: 0:00:17.689513\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 400.8120400760151\tTime: 0:00:24.604172\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 400.8692586088543\tTime: 0:00:17.879418\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 400.77507770331425\tTime: 0:00:17.931781\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 400.808067450914\tTime: 0:00:17.731682\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 400.5742378507485\tTime: 0:00:17.832624\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 400.76688498822216\tTime: 0:00:17.873173\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 400.6494840373575\tTime: 0:00:18.161870\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 400.76996013526855\tTime: 0:00:24.672475\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 400.8117947415933\tTime: 0:00:18.018122\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 400.6687215766169\tTime: 0:00:17.897264\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 400.5874681311357\tTime: 0:00:18.096540\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 400.65545052347807\tTime: 0:00:25.823195\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 400.5483766126909\tTime: 0:00:18.527263\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 400.5972192101811\tTime: 0:00:18.050910\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 400.68383849670647\tTime: 0:00:17.736046\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 400.4983157180353\tTime: 0:00:17.626847\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 400.5380026655829\tTime: 0:00:17.759991\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 400.66388655272044\tTime: 0:00:24.436736\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 400.5631416382212\tTime: 0:00:18.206313\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 400.63335569590197\tTime: 0:00:18.135504\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 400.5475355185765\tTime: 0:00:18.063041\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 400.61110148727244\tTime: 0:00:17.746871\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 400.554711265739\tTime: 0:00:18.104047\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 400.58349789362825\tTime: 0:00:17.923604\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 400.5981675623448\tTime: 0:00:17.679532\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 400.6250928222913\tTime: 0:00:24.983561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 11:29:32,508] Trial 25 finished with values: [0.024056229502831386, 0.9541666666666667] and parameters: {'num_topics': 24, 'dropout': 0.2560502719343839, 'num_neurons': 100, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 437.54526429264934\tTime: 0:00:18.529306\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 422.26581872566857\tTime: 0:00:16.049788\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 420.85805014269727\tTime: 0:00:16.183350\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 420.2948055440951\tTime: 0:00:16.632609\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 419.0310751546867\tTime: 0:00:16.464312\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 417.55413460679495\tTime: 0:00:16.321143\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 416.42565915213834\tTime: 0:00:22.975054\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 415.4582941400667\tTime: 0:00:16.336687\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 414.79447524807466\tTime: 0:00:16.219376\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 414.3051164954652\tTime: 0:00:16.481835\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 413.89501038493006\tTime: 0:00:16.457401\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 413.6513211987747\tTime: 0:00:16.326119\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 413.14877931156786\tTime: 0:00:23.355891\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 412.9897927069228\tTime: 0:00:16.565664\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 412.69090202697146\tTime: 0:00:16.411425\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 412.5278187709227\tTime: 0:00:16.753951\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 412.38082359782874\tTime: 0:00:24.033866\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 412.36453734460804\tTime: 0:00:16.204315\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 412.3196813671904\tTime: 0:00:16.233706\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 412.3306298795066\tTime: 0:00:16.338909\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 412.2019992680005\tTime: 0:00:16.490681\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 411.99048521925306\tTime: 0:00:23.181562\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 412.0294843929253\tTime: 0:00:16.413536\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 411.9862924947576\tTime: 0:00:16.436012\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 411.9852686211728\tTime: 0:00:16.738088\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 411.70367249357037\tTime: 0:00:16.251298\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 411.9054856027373\tTime: 0:00:23.333688\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 411.8092128346451\tTime: 0:00:16.276281\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 411.65214550626973\tTime: 0:00:16.442802\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 411.7009434373942\tTime: 0:00:16.864534\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 411.5308673605822\tTime: 0:00:23.527865\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 411.5780389731673\tTime: 0:00:17.213194\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 411.57760571676585\tTime: 0:00:16.439873\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 411.354361053515\tTime: 0:00:16.456143\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 411.2974716779978\tTime: 0:00:16.557886\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 411.50710103385006\tTime: 0:00:23.017128\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 411.38597547375736\tTime: 0:00:16.482144\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 411.48541231757315\tTime: 0:00:16.449689\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 411.3349377962085\tTime: 0:00:23.751131\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 411.1298379624732\tTime: 0:00:16.637450\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 411.2243734072914\tTime: 0:00:16.561507\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 411.21150644518025\tTime: 0:00:16.669810\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 411.0774352425854\tTime: 0:00:16.691467\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 411.1648097800534\tTime: 0:00:23.740150\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 411.116653634079\tTime: 0:00:16.409149\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 411.09965036814486\tTime: 0:00:16.499669\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 411.06822111496655\tTime: 0:00:16.621547\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 411.0100025110138\tTime: 0:00:23.084206\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 411.0737241140779\tTime: 0:00:18.636535\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 411.03289619066425\tTime: 0:00:18.293343\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 411.11010802795647\tTime: 0:00:18.621334\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 411.09288965820826\tTime: 0:00:18.220294\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 411.04831890639105\tTime: 0:00:18.851531\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 411.00720810823833\tTime: 0:00:23.820345\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 411.035417820058\tTime: 0:00:31.275886\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 410.86902917947884\tTime: 0:00:28.562003\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 411.0316436223482\tTime: 0:00:25.366965\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 411.01359965947773\tTime: 0:00:18.764363\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 410.87293495196013\tTime: 0:00:33.218286\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 410.93906023729596\tTime: 0:00:18.041750\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 410.79992766693465\tTime: 0:00:18.337199\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 410.76608690752414\tTime: 0:00:18.846752\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 410.9337541228231\tTime: 0:00:18.632580\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 410.83377599318015\tTime: 0:00:19.134869\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 410.6325676063647\tTime: 0:00:18.313940\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 410.86564658384583\tTime: 0:00:17.865227\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 410.813174329887\tTime: 0:00:17.740342\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 410.7099529298785\tTime: 0:00:24.913064\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 410.8973937910225\tTime: 0:00:17.755680\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 410.66079542716324\tTime: 0:00:17.937942\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 410.66229013419894\tTime: 0:00:18.207874\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 410.82650382396986\tTime: 0:00:17.962343\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 410.8397588251335\tTime: 0:00:17.915503\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 410.5676509943556\tTime: 0:00:18.254633\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 410.870372586547\tTime: 0:00:18.250650\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 410.6482418642566\tTime: 0:00:17.949201\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 410.4808194323963\tTime: 0:00:18.190406\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 410.6516086650399\tTime: 0:00:17.959063\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 410.768998963564\tTime: 0:00:18.916446\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 410.7348016446038\tTime: 0:00:18.674017\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 410.5955105747621\tTime: 0:00:18.980707\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 410.74003738016484\tTime: 0:00:19.153268\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 410.66017145713437\tTime: 0:00:19.198547\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 410.71942965605723\tTime: 0:00:18.789496\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 410.4876563616659\tTime: 0:00:18.801867\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 410.5314168009596\tTime: 0:00:18.995891\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 410.66334423839146\tTime: 0:00:18.813381\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 410.4827354211333\tTime: 0:00:18.577702\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 410.5155429035136\tTime: 0:00:19.432838\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 410.6795147116904\tTime: 0:00:18.801938\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 410.5562913384276\tTime: 0:00:18.385563\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 410.48245717465255\tTime: 0:00:18.179442\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 410.4310453428261\tTime: 0:00:20.800401\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 410.60718969070894\tTime: 0:00:18.696312\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 410.49244671919666\tTime: 0:00:18.093852\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 410.46228318446504\tTime: 0:00:19.226582\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 410.4487798735707\tTime: 0:00:18.227129\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 410.338820721256\tTime: 0:00:18.157209\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 410.6037175786833\tTime: 0:00:17.991119\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 410.51189701140873\tTime: 0:00:18.734846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 12:02:00,878] Trial 26 finished with values: [-0.03235441120360148, 0.8647058823529412] and parameters: {'num_topics': 34, 'dropout': 0.5732847471753364, 'num_neurons': 200, 'num_layers': 2, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 442.27294646383433\tTime: 0:00:20.493708\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 424.2153931903774\tTime: 0:00:19.307179\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 421.7456390052565\tTime: 0:00:20.044085\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 420.84215843013004\tTime: 0:00:18.884866\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 418.61039064557\tTime: 0:00:19.497582\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 415.9290992704102\tTime: 0:00:19.179740\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 414.0862231663869\tTime: 0:00:18.740645\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 412.94568720526075\tTime: 0:00:19.032511\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 412.02167718236336\tTime: 0:00:19.734024\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 411.4533767625583\tTime: 0:00:19.190702\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 410.91125535114816\tTime: 0:00:18.379140\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 410.60816037605554\tTime: 0:00:19.236790\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 410.1078795583202\tTime: 0:00:19.368555\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 409.78947952076516\tTime: 0:00:18.577913\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 409.52369638860677\tTime: 0:00:18.540056\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 409.4008459574588\tTime: 0:00:18.940087\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 409.1513697587546\tTime: 0:00:18.780519\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 409.00849483750847\tTime: 0:00:18.761464\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 408.94813801495616\tTime: 0:00:19.517322\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 408.574108737035\tTime: 0:00:19.307489\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 408.4992079801164\tTime: 0:00:19.178896\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 408.3682564072726\tTime: 0:00:19.223161\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 408.1499954966312\tTime: 0:00:19.432552\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 408.21789076017455\tTime: 0:00:19.785417\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 407.9577467199607\tTime: 0:00:19.561679\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 407.80735752751684\tTime: 0:00:20.136138\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 407.8202240855737\tTime: 0:00:18.885194\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 407.69152585007146\tTime: 0:00:19.417885\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 407.5003619591843\tTime: 0:00:19.918091\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 407.47163964939256\tTime: 0:00:19.569093\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 407.22341300696326\tTime: 0:00:19.147449\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 407.2314331174067\tTime: 0:00:20.267175\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 407.1294757828956\tTime: 0:00:18.151385\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 407.1042094229576\tTime: 0:00:19.910698\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 407.070454727112\tTime: 0:00:19.474196\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 407.0099082194341\tTime: 0:00:18.425454\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 406.83143831868745\tTime: 0:00:18.438640\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 406.7778701960148\tTime: 0:00:18.629296\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 406.786342626876\tTime: 0:00:19.347695\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 406.48749782545326\tTime: 0:00:18.673009\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 406.62126481189586\tTime: 0:00:18.725615\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 406.5488175461239\tTime: 0:00:18.368144\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 406.5101805887615\tTime: 0:00:18.646270\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 406.39527360793744\tTime: 0:00:18.746271\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 406.43232079530816\tTime: 0:00:18.616061\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 406.4142250767556\tTime: 0:00:18.282274\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 406.26917028258526\tTime: 0:00:18.518591\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 406.15907757195987\tTime: 0:00:19.141152\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 406.1424465502285\tTime: 0:00:18.560920\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 406.154513374638\tTime: 0:00:19.146632\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 406.1526636508054\tTime: 0:00:19.020099\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 406.2387822405942\tTime: 0:00:19.820666\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 406.053493556289\tTime: 0:00:19.876185\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 406.20764456768836\tTime: 0:00:18.725322\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 406.0136802132114\tTime: 0:00:18.748744\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 406.07061528359617\tTime: 0:00:18.533634\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 406.0677234302711\tTime: 0:00:19.280215\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 406.12300804905544\tTime: 0:00:18.771100\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 405.90926068202015\tTime: 0:00:18.987446\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 405.89389894175974\tTime: 0:00:19.552637\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 405.8894601481983\tTime: 0:00:19.277989\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 405.91408714731205\tTime: 0:00:19.328759\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 405.8629186663681\tTime: 0:00:18.862904\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 405.7844911398974\tTime: 0:00:17.815063\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 405.7335710788808\tTime: 0:00:16.999416\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 405.7443699074466\tTime: 0:00:18.017879\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 405.78288943193786\tTime: 0:00:17.243496\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 405.58592493758465\tTime: 0:00:17.644831\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 405.6943620173682\tTime: 0:00:17.462753\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 405.69206092815534\tTime: 0:00:17.962945\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 405.6649405099841\tTime: 0:00:17.491498\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 405.6591140470431\tTime: 0:00:25.500110\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 405.57530084413554\tTime: 0:00:17.201055\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 405.6527215040341\tTime: 0:00:17.430861\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 405.27661880312945\tTime: 0:00:17.421638\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 405.4876906328166\tTime: 0:00:17.702948\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 405.6208622268887\tTime: 0:00:17.449945\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 405.49058024547696\tTime: 0:00:17.439318\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 405.37296463158475\tTime: 0:00:24.902622\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 405.535042012831\tTime: 0:00:17.474327\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 405.51415637283253\tTime: 0:00:17.529968\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 405.43791856351794\tTime: 0:00:17.243437\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 405.4321210088251\tTime: 0:00:17.621827\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 405.39414857385\tTime: 0:00:17.275381\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 405.39960720045326\tTime: 0:00:25.129999\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 405.6347695186141\tTime: 0:00:17.882433\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 405.3245069141771\tTime: 0:00:17.455413\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 405.3219502789885\tTime: 0:00:24.224213\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 405.2382368407597\tTime: 0:00:17.246136\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 405.3221757412854\tTime: 0:00:17.234215\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 405.1461497078467\tTime: 0:00:17.840165\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 405.1975256890375\tTime: 0:00:17.475865\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 405.2619888419304\tTime: 0:00:24.351142\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 405.2944496457252\tTime: 0:00:17.628563\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 405.1792561904057\tTime: 0:00:17.360716\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 405.22077280599984\tTime: 0:00:17.163286\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 405.18005864223653\tTime: 0:00:17.340404\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 405.34272208587225\tTime: 0:00:24.295851\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 405.20436026744574\tTime: 0:00:17.578570\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 405.21912297884694\tTime: 0:00:17.453391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 12:34:46,082] Trial 27 finished with values: [0.0020566948382613106, 0.9219512195121952] and parameters: {'num_topics': 41, 'dropout': 0.3633238032417218, 'num_neurons': 200, 'num_layers': 2, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 439.83323731533983\tTime: 0:00:19.763858\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 422.78223155808035\tTime: 0:00:17.258552\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 417.6725393387262\tTime: 0:00:17.262731\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 415.11814128853064\tTime: 0:00:16.918493\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 413.64349488746427\tTime: 0:00:23.691497\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 412.49509132802507\tTime: 0:00:17.199758\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 411.59048913490653\tTime: 0:00:17.364006\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 410.975720414116\tTime: 0:00:17.199702\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 410.53489581864045\tTime: 0:00:17.636687\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 410.003056597281\tTime: 0:00:16.999110\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 409.67991814153834\tTime: 0:00:23.810937\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 409.55126251539815\tTime: 0:00:16.956000\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 409.3471976116424\tTime: 0:00:17.094101\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 409.04319832630426\tTime: 0:00:17.963057\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 408.77691714212835\tTime: 0:00:23.762837\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 408.6460190881126\tTime: 0:00:16.991384\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 408.4988368011477\tTime: 0:00:17.021006\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 408.61820909618785\tTime: 0:00:16.862071\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 408.42684185579344\tTime: 0:00:16.812371\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 408.23819728017094\tTime: 0:00:23.932786\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 408.15370177648714\tTime: 0:00:16.958705\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 408.3165152711954\tTime: 0:00:17.086064\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 408.16169757020845\tTime: 0:00:24.383480\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 408.0772801592003\tTime: 0:00:17.150110\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 407.97223331534923\tTime: 0:00:17.204563\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 407.9772284549434\tTime: 0:00:16.973026\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 407.93561846607474\tTime: 0:00:24.001740\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 407.9270930306364\tTime: 0:00:17.073212\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 407.602571974559\tTime: 0:00:16.788837\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 407.4342668677241\tTime: 0:00:16.840071\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 407.7959148200651\tTime: 0:00:24.057934\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 407.63388162766586\tTime: 0:00:16.833693\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 407.6479996888047\tTime: 0:00:17.147046\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 407.66993520879174\tTime: 0:00:17.323705\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 407.4274140701403\tTime: 0:00:24.925980\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 407.5695377809867\tTime: 0:00:16.991929\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 407.46924224832856\tTime: 0:00:16.815793\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 407.40003153092795\tTime: 0:00:23.802924\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 407.4878225014458\tTime: 0:00:17.042257\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 407.3027871959161\tTime: 0:00:17.201347\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 407.3577912173583\tTime: 0:00:17.094635\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 407.54742987674433\tTime: 0:00:16.989420\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 407.368110727333\tTime: 0:00:23.747340\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 407.2459836635706\tTime: 0:00:17.118959\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 407.18014764437817\tTime: 0:00:17.009888\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 407.29916528994346\tTime: 0:00:18.005456\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 407.2643863899233\tTime: 0:00:21.174887\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 407.20189957678616\tTime: 0:00:19.573914\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 407.2415126558621\tTime: 0:00:17.621101\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 407.0891502091679\tTime: 0:00:17.288390\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 407.2856539714276\tTime: 0:00:23.995592\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 407.20498864533965\tTime: 0:00:17.007100\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 407.21674934352194\tTime: 0:00:17.049278\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 406.9964474077052\tTime: 0:00:17.031129\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 407.31850472434974\tTime: 0:00:29.283855\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 407.1326444501453\tTime: 0:00:28.912874\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 407.0043868542184\tTime: 0:00:19.012872\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 407.2613924945225\tTime: 0:00:17.510188\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 407.0742335163479\tTime: 0:00:22.150264\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 407.01703621472535\tTime: 0:00:19.633005\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 406.9437549808875\tTime: 0:00:17.279461\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 407.0069692386853\tTime: 0:00:17.666278\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 406.8703541837104\tTime: 0:00:23.926159\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 406.943901689329\tTime: 0:00:17.151645\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 406.90058853814037\tTime: 0:00:17.331205\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 406.8974555011331\tTime: 0:00:23.844604\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 406.8424003851886\tTime: 0:00:17.397942\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 406.83376321037156\tTime: 0:00:17.299259\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 406.9493435660451\tTime: 0:00:17.067851\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 406.9008672988721\tTime: 0:00:23.984025\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 406.7495320316614\tTime: 0:00:16.703501\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 406.9117052839723\tTime: 0:00:17.327635\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 406.8457958003625\tTime: 0:00:24.078652\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 406.7138537802438\tTime: 0:00:17.680811\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 406.6169793753056\tTime: 0:00:17.331252\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 406.93616941247274\tTime: 0:00:24.944915\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 406.8376152803637\tTime: 0:00:17.332167\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 406.73933744792845\tTime: 0:00:17.173057\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 406.69965479804927\tTime: 0:00:17.352037\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 406.7605852675045\tTime: 0:00:24.439805\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 406.77525761767237\tTime: 0:00:17.596778\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 406.76951203905486\tTime: 0:00:17.537215\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 406.7386171293162\tTime: 0:00:17.248241\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 406.7364685522338\tTime: 0:00:24.027414\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 406.6956866542776\tTime: 0:00:17.248785\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 406.82808617405544\tTime: 0:00:17.610211\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 406.5089186537322\tTime: 0:00:17.411529\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 406.6759161674025\tTime: 0:00:17.902625\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 406.6584120210708\tTime: 0:00:23.956443\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 406.5648099784073\tTime: 0:00:17.654725\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 406.6827183847349\tTime: 0:00:17.530276\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 406.5555950794121\tTime: 0:00:24.427936\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 406.50044449645725\tTime: 0:00:17.358771\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 406.7486205953674\tTime: 0:00:17.669623\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 406.5737398354632\tTime: 0:00:24.488165\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 406.52444675416575\tTime: 0:00:17.414064\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 406.463664939959\tTime: 0:00:17.450137\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 406.4576114352361\tTime: 0:00:24.243404\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 406.6511312197914\tTime: 0:00:17.314303\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 406.5522655250884\tTime: 0:00:17.312887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 13:07:44,902] Trial 28 finished with values: [0.003447487817901202, 0.926829268292683] and parameters: {'num_topics': 41, 'dropout': 0.4115247588488429, 'num_neurons': 200, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 437.94603204723586\tTime: 0:00:12.712819\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 418.9882232865747\tTime: 0:00:11.659099\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 413.8048371470182\tTime: 0:00:11.509201\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 411.4651030059289\tTime: 0:00:11.688500\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 410.3121104181947\tTime: 0:00:11.843415\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 409.2899843682413\tTime: 0:00:15.347065\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 408.7172836134972\tTime: 0:00:15.351335\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 408.17911976698264\tTime: 0:00:11.490119\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 407.8497599770556\tTime: 0:00:11.840830\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 407.4051212192272\tTime: 0:00:11.785632\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 407.1340976130529\tTime: 0:00:11.657832\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 406.8202345542532\tTime: 0:00:11.754233\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 406.6369886950016\tTime: 0:00:11.466320\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 406.50477834609853\tTime: 0:00:11.380517\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 406.35241516476015\tTime: 0:00:11.460339\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 406.1909119079356\tTime: 0:00:18.396325\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 405.98377189386565\tTime: 0:00:11.441122\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 406.0819813676606\tTime: 0:00:11.945830\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 405.9102820945469\tTime: 0:00:11.641910\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 405.66205199929004\tTime: 0:00:11.755521\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 405.54458654666695\tTime: 0:00:11.689799\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 405.5800455435309\tTime: 0:00:18.438557\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 405.4904621146938\tTime: 0:00:11.552361\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 405.435950488509\tTime: 0:00:11.431982\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 405.2081575697148\tTime: 0:00:11.475576\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 405.2216923968442\tTime: 0:00:11.574357\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 405.24162535027835\tTime: 0:00:11.430376\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 405.04526612925974\tTime: 0:00:11.738440\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 404.8650083249878\tTime: 0:00:11.433652\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 405.12552864994734\tTime: 0:00:13.568557\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 404.8193998515431\tTime: 0:00:16.914733\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 404.91194933276677\tTime: 0:00:11.444188\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 404.7686921394249\tTime: 0:00:11.541645\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 404.85731171510616\tTime: 0:00:11.553152\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 404.8666432755962\tTime: 0:00:11.659569\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 404.6176297925247\tTime: 0:00:11.817025\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 404.5324021791897\tTime: 0:00:11.591513\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 404.44330372570386\tTime: 0:00:11.383071\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 404.6158894204774\tTime: 0:00:18.517185\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 404.4592167429521\tTime: 0:00:11.442868\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 404.453134329981\tTime: 0:00:11.584685\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 404.4522054091557\tTime: 0:00:11.453071\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 404.49983562336615\tTime: 0:00:11.525424\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 404.2341360937147\tTime: 0:00:12.249504\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 404.2376547454487\tTime: 0:00:11.480012\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 404.2225437025126\tTime: 0:00:11.283266\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 404.2835927802697\tTime: 0:00:11.473608\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 404.1985891966754\tTime: 0:00:18.198079\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 404.1524228344453\tTime: 0:00:11.443518\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 404.07235077025973\tTime: 0:00:11.466922\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 404.22371130922994\tTime: 0:00:11.610023\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 404.19776297910556\tTime: 0:00:11.505958\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 404.1034169990197\tTime: 0:00:11.548438\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 403.94237715573985\tTime: 0:00:19.404068\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 404.0826937521158\tTime: 0:00:11.672825\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 403.9312993460491\tTime: 0:00:11.490682\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 404.0655675802114\tTime: 0:00:11.621082\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 403.7725319408594\tTime: 0:00:12.008049\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 403.84115868960635\tTime: 0:00:11.541434\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 404.1023284032832\tTime: 0:00:11.520107\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 403.7607095692987\tTime: 0:00:18.752685\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 404.14983075267526\tTime: 0:00:11.615803\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 403.9652309074795\tTime: 0:00:11.767629\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 403.69938826914307\tTime: 0:00:11.664507\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 403.88702318654555\tTime: 0:00:11.792888\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 403.8484299037792\tTime: 0:00:11.859857\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 403.7837687642815\tTime: 0:00:11.674805\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 403.7303769194783\tTime: 0:00:11.773925\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 403.83189188800026\tTime: 0:00:11.646191\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 403.82385374204233\tTime: 0:00:11.610355\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 403.80061852631553\tTime: 0:00:18.820406\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 403.70663046487107\tTime: 0:00:11.519264\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 403.71548358834303\tTime: 0:00:11.767109\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 403.57120821490867\tTime: 0:00:11.578249\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 403.6771207267218\tTime: 0:00:11.669507\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 403.5540777453359\tTime: 0:00:11.677655\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 403.71987661357247\tTime: 0:00:18.314468\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 403.4666838499939\tTime: 0:00:11.714243\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 403.8736650780721\tTime: 0:00:11.584482\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 403.3562275639669\tTime: 0:00:11.906256\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 403.54526010517753\tTime: 0:00:11.705829\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 403.4895217517855\tTime: 0:00:11.712554\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 403.528581000985\tTime: 0:00:12.259528\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 403.4285436957539\tTime: 0:00:11.433986\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 403.53587021394014\tTime: 0:00:12.182495\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 403.43020432217617\tTime: 0:00:18.244628\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 403.58907509180113\tTime: 0:00:11.378577\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 403.40560582725635\tTime: 0:00:11.493574\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 403.5393011491598\tTime: 0:00:11.404630\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 403.6345544148001\tTime: 0:00:11.514173\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 403.31637084015085\tTime: 0:00:11.334640\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 403.479545522667\tTime: 0:00:11.581990\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 403.40089984360895\tTime: 0:00:18.230499\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 403.3484169373096\tTime: 0:00:11.498397\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 403.39155774097503\tTime: 0:00:11.912366\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 403.2917248504852\tTime: 0:00:11.606500\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 403.32446739032054\tTime: 0:00:12.082020\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 403.40431160461566\tTime: 0:00:12.012032\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 403.3965804664579\tTime: 0:00:11.637314\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 403.28623250371436\tTime: 0:00:11.648037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 13:29:21,396] Trial 29 finished with values: [0.03254146393692767, 0.8976744186046511] and parameters: {'num_topics': 43, 'dropout': 0.20694688744564027, 'num_neurons': 50, 'num_layers': 1, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 421.623958311294\tTime: 0:00:13.840032\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 412.07452572106797\tTime: 0:00:11.934425\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 409.5923208232246\tTime: 0:00:11.874464\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 407.36263615895115\tTime: 0:00:11.744187\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 406.3356933777411\tTime: 0:00:11.825908\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 405.9972945259018\tTime: 0:00:11.734146\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 405.7165630744917\tTime: 0:00:12.035409\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 405.28921192662494\tTime: 0:00:12.006624\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 405.226932503538\tTime: 0:00:18.699669\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 404.7698757981174\tTime: 0:00:12.068629\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 404.4636183635184\tTime: 0:00:11.615825\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 403.9478122369974\tTime: 0:00:11.782144\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 403.83281269100746\tTime: 0:00:11.923275\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 403.66919692812945\tTime: 0:00:12.412114\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 403.65585861831653\tTime: 0:00:12.058650\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 403.4527514334377\tTime: 0:00:11.636478\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 403.2975744914499\tTime: 0:00:12.130320\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 403.138832578413\tTime: 0:00:12.294989\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 403.21343870628573\tTime: 0:00:18.672143\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 403.13831876827794\tTime: 0:00:11.966806\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 403.2078487987686\tTime: 0:00:11.876739\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 402.98467036369\tTime: 0:00:12.311685\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 403.2876292826817\tTime: 0:00:12.163206\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 403.1527858588637\tTime: 0:00:16.135284\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 402.991089133496\tTime: 0:00:14.961210\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 403.10194216410895\tTime: 0:00:11.920444\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 402.9896648788366\tTime: 0:00:12.176314\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 403.06682867039984\tTime: 0:00:11.922102\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 402.9821697451196\tTime: 0:00:11.977737\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 402.6961134825425\tTime: 0:00:11.897051\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 403.0435094113796\tTime: 0:00:25.154101\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 402.98566698197595\tTime: 0:00:46.541942\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 402.81270757371124\tTime: 0:00:12.879840\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 402.74754834546377\tTime: 0:00:12.198521\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 402.68794585554895\tTime: 0:00:19.680536\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 402.82262754745216\tTime: 0:00:31.806842\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 402.7692040027529\tTime: 0:00:22.350452\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 402.80286073379784\tTime: 0:00:12.727004\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 402.6784892218884\tTime: 0:00:13.929900\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 402.68959601329175\tTime: 0:00:12.125709\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 402.7950200234616\tTime: 0:00:12.646012\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 402.64991677950803\tTime: 0:00:11.872606\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 402.7348399195653\tTime: 0:00:11.856991\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 402.7396041972279\tTime: 0:00:18.863172\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 402.5278701592826\tTime: 0:00:12.030905\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 402.51526307754784\tTime: 0:00:11.793008\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 402.74730400281163\tTime: 0:00:11.906980\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 402.5764706547854\tTime: 0:00:12.092684\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 402.55750206875797\tTime: 0:00:19.351322\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 402.51754653530054\tTime: 0:00:12.372036\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 402.5762826226092\tTime: 0:00:12.134461\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 402.48498493832517\tTime: 0:00:11.882502\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 402.4566986398504\tTime: 0:00:12.444257\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 402.46852336227244\tTime: 0:00:12.101885\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 402.58010596801415\tTime: 0:00:18.618359\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 402.61158932773594\tTime: 0:00:11.807165\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 402.3634466184035\tTime: 0:00:12.237327\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 402.4592082210797\tTime: 0:00:11.995804\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 402.5042925626387\tTime: 0:00:12.196104\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 402.5274841405016\tTime: 0:00:11.938704\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 402.4728364582158\tTime: 0:00:12.299676\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 402.7069694296928\tTime: 0:00:12.166797\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 402.57597910436886\tTime: 0:00:18.621584\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 402.32677420975796\tTime: 0:00:12.083316\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 402.4708207415322\tTime: 0:00:12.063779\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 402.49067464725323\tTime: 0:00:11.948494\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 402.30818771216525\tTime: 0:00:12.682222\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 402.45503000580663\tTime: 0:00:11.989495\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 402.31455369778735\tTime: 0:00:11.851040\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 402.2337192198784\tTime: 0:00:18.943725\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 402.439465797612\tTime: 0:00:12.675719\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 402.41413008726397\tTime: 0:00:12.492496\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 402.40800646075473\tTime: 0:00:12.073720\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 402.3382904183593\tTime: 0:00:12.287005\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 402.38177731289494\tTime: 0:00:11.978034\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 402.26895257078445\tTime: 0:00:11.950283\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 402.30510066388325\tTime: 0:00:18.791719\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 402.4120236787571\tTime: 0:00:12.295761\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 402.09108382936506\tTime: 0:00:11.980024\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 402.1655242582445\tTime: 0:00:12.826394\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 402.36188920948763\tTime: 0:00:11.924748\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 402.25550843547825\tTime: 0:00:12.009396\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 402.2521660616137\tTime: 0:00:19.104396\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 402.28773330829665\tTime: 0:00:12.567584\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 402.19435441380097\tTime: 0:00:12.094074\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 402.26263161885015\tTime: 0:00:11.939522\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 402.3327908720755\tTime: 0:00:12.101640\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 402.17341823028335\tTime: 0:00:19.219856\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 402.1274721143765\tTime: 0:00:12.446236\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 402.321189959001\tTime: 0:00:12.167367\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 402.31264582686845\tTime: 0:00:12.371622\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 402.30556606096724\tTime: 0:00:12.747080\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 402.3815709513466\tTime: 0:00:12.197919\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 402.2949016722852\tTime: 0:00:12.334665\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 402.19210092953057\tTime: 0:00:12.352919\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 402.31499242728785\tTime: 0:00:12.723429\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 402.2221750580663\tTime: 0:00:12.222158\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 402.2850828957793\tTime: 0:00:19.728004\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 402.17997261540376\tTime: 0:00:12.546269\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 402.331870362926\tTime: 0:00:12.213545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 13:53:14,662] Trial 30 finished with values: [-0.006188129312372979, 0.97] and parameters: {'num_topics': 10, 'dropout': 0.28969739027489144, 'num_neurons': 100, 'num_layers': 2, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 426.7627737357655\tTime: 0:00:13.883908\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 412.7979170927603\tTime: 0:00:12.550662\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 408.542021059016\tTime: 0:00:12.591967\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 406.84386739613893\tTime: 0:00:12.456143\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 405.950545590842\tTime: 0:00:12.292850\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 405.0273622997654\tTime: 0:00:12.167626\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 404.4233903505369\tTime: 0:00:12.646197\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 404.13212013459855\tTime: 0:00:18.725563\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 403.7604242335017\tTime: 0:00:12.243565\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 403.36615708808205\tTime: 0:00:12.754879\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 403.0824306760372\tTime: 0:00:12.214644\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 402.9288103715889\tTime: 0:00:12.361239\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 402.6208686550252\tTime: 0:00:19.286998\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 402.51066629875686\tTime: 0:00:12.304675\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 402.1535204663051\tTime: 0:00:12.189126\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 402.2330041907042\tTime: 0:00:12.319224\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 401.9395043664311\tTime: 0:00:12.207616\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 401.8312954303957\tTime: 0:00:12.363755\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 401.88941390234754\tTime: 0:00:12.081523\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 401.7604384488664\tTime: 0:00:18.860531\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 401.69962555921114\tTime: 0:00:12.709628\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 401.47528315390383\tTime: 0:00:12.166047\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 401.72812159859745\tTime: 0:00:12.976688\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 401.3929043070131\tTime: 0:00:12.398890\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 401.35663220597775\tTime: 0:00:12.297176\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 401.15978404252945\tTime: 0:00:13.260241\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 401.28251678368076\tTime: 0:00:19.441282\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 401.1676739372931\tTime: 0:00:12.246490\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 401.2422345906915\tTime: 0:00:12.195275\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 401.1657598953632\tTime: 0:00:12.405413\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 401.02418389848043\tTime: 0:00:12.705676\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 401.00643122202473\tTime: 0:00:19.555185\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 400.8706711092069\tTime: 0:00:12.450610\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 400.9906368130783\tTime: 0:00:12.401307\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 400.8381114590386\tTime: 0:00:11.988828\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 400.9975800453599\tTime: 0:00:12.509344\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 401.0999297533241\tTime: 0:00:20.126616\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 400.8647671045734\tTime: 0:00:12.574659\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 400.8718847046495\tTime: 0:00:12.380001\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 400.87543042802133\tTime: 0:00:12.388232\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 400.9126733245999\tTime: 0:00:18.869604\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 400.7230392567399\tTime: 0:00:12.285397\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 400.6980797576732\tTime: 0:00:12.353875\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 400.6723991318857\tTime: 0:00:12.506120\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 400.6387695826751\tTime: 0:00:14.985342\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 400.5656290331965\tTime: 0:00:16.606892\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 400.66360907761606\tTime: 0:00:12.606582\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 400.34775968789967\tTime: 0:00:12.217909\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 400.496755590936\tTime: 0:00:12.930388\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 400.6182388125447\tTime: 0:00:12.443244\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 400.56971912790095\tTime: 0:00:12.218979\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 400.43884182758313\tTime: 0:00:12.548552\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 400.54451161296123\tTime: 0:00:12.342667\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 400.4655843447914\tTime: 0:00:18.929980\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 400.58372071120607\tTime: 0:00:12.818829\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 400.4588324505849\tTime: 0:00:12.434952\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 400.22775071348644\tTime: 0:00:12.565890\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 400.2855023776024\tTime: 0:00:12.734161\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 400.3630823083578\tTime: 0:00:12.573320\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 400.3903893261491\tTime: 0:00:19.469800\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 400.43576807636066\tTime: 0:00:12.521796\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 400.31811705261936\tTime: 0:00:12.723569\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 400.26047036915577\tTime: 0:00:12.763436\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 400.4445999318838\tTime: 0:00:12.487622\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 400.276390953239\tTime: 0:00:19.172682\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 400.42174566589324\tTime: 0:00:12.391855\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 400.281821369506\tTime: 0:00:12.299660\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 400.3288972946875\tTime: 0:00:12.789095\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 400.381014091063\tTime: 0:00:12.560300\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 400.28700972051786\tTime: 0:00:13.074067\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 400.09529319355113\tTime: 0:00:19.212051\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 400.2073540159177\tTime: 0:00:12.378273\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 400.0919012679371\tTime: 0:00:12.545074\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 400.176612242757\tTime: 0:00:12.475430\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 400.2461772790425\tTime: 0:00:12.392056\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 400.3084685666328\tTime: 0:00:13.016842\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 400.1180803865286\tTime: 0:00:12.403475\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 400.2776040711629\tTime: 0:00:12.291795\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 400.00916329024204\tTime: 0:00:19.130312\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 400.2810483401743\tTime: 0:00:12.583409\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 400.1279987073201\tTime: 0:00:12.447695\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 399.9945709060925\tTime: 0:00:12.505990\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 400.0314400524477\tTime: 0:00:12.791374\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 400.16199326448833\tTime: 0:00:12.429109\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 400.0939230821673\tTime: 0:00:12.770267\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 400.1261508935624\tTime: 0:00:19.187898\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 400.14303364111987\tTime: 0:00:12.596595\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 400.12527185507645\tTime: 0:00:13.101545\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 400.0172810348962\tTime: 0:00:12.447274\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 400.04775793210007\tTime: 0:00:12.518679\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 399.9861447945465\tTime: 0:00:12.735456\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 400.12693020410177\tTime: 0:00:19.347234\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 400.16691567430934\tTime: 0:00:12.593612\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 400.0922434651931\tTime: 0:00:12.482874\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 400.17449760623543\tTime: 0:00:12.491765\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 400.00675450219336\tTime: 0:00:12.571723\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 400.0376879807512\tTime: 0:00:19.199430\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 399.95379165285533\tTime: 0:00:12.357800\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 399.91610047640205\tTime: 0:00:12.303746\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 400.2169766056971\tTime: 0:00:12.589443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 14:16:42,444] Trial 31 finished with values: [0.014637389142775908, 0.9739130434782609] and parameters: {'num_topics': 23, 'dropout': 0.2557590762065598, 'num_neurons': 100, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 434.3478784063981\tTime: 0:00:25.269254\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 423.85509823221105\tTime: 0:00:23.381662\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 420.080710212847\tTime: 0:00:29.865734\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 417.15697281640246\tTime: 0:00:22.860040\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 415.4478870237743\tTime: 0:00:23.222323\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 414.86061867912156\tTime: 0:00:30.791765\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 414.34895752522476\tTime: 0:00:22.798947\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 413.98041445979555\tTime: 0:00:29.079431\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 413.6424914002553\tTime: 0:00:22.422825\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 413.2535511964709\tTime: 0:00:28.950601\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 413.0285517254147\tTime: 0:00:22.640612\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 412.754176966378\tTime: 0:00:22.291795\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 412.57001856886615\tTime: 0:00:29.176953\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 412.5850410871793\tTime: 0:00:22.095928\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 412.45870664277015\tTime: 0:00:21.912954\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 412.2140368903041\tTime: 0:00:28.692865\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 412.16259537885304\tTime: 0:00:21.968984\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 412.10518796899686\tTime: 0:00:21.981499\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 411.9587039613189\tTime: 0:00:29.276647\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 411.958147578554\tTime: 0:00:23.977204\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 411.93889560353665\tTime: 0:00:25.454621\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 411.79226268348475\tTime: 0:00:34.580405\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 411.8399503468696\tTime: 0:00:35.383461\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 411.75567057144735\tTime: 0:00:29.222092\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 411.6795284128042\tTime: 0:00:36.668241\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 411.5034422120077\tTime: 0:00:29.856743\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 411.5172890792499\tTime: 0:00:30.396920\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 411.4632810149139\tTime: 0:00:38.080868\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 411.38077463379454\tTime: 0:00:30.342093\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 411.4013695016291\tTime: 0:00:37.580470\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 411.2641393290877\tTime: 0:00:43.306155\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 411.3224471555753\tTime: 0:00:34.502365\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 411.16725643900924\tTime: 0:00:34.867544\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 411.11605199723306\tTime: 0:00:34.810076\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 411.0172687663385\tTime: 0:00:34.561255\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 410.9979144921199\tTime: 0:00:35.251910\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 411.0734760982049\tTime: 0:00:34.804522\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 410.76548303001346\tTime: 0:00:35.136708\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 411.1341255295315\tTime: 0:00:35.527966\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 410.9248114462264\tTime: 0:00:36.358211\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 410.8128697831448\tTime: 0:00:38.325058\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 410.86088234291543\tTime: 0:00:38.388733\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 410.8720885684077\tTime: 0:00:36.379901\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 410.96398616312393\tTime: 0:00:51.653044\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 410.74062219365925\tTime: 0:00:36.800731\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 410.6569659474794\tTime: 0:00:39.261123\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 410.82922568063316\tTime: 0:00:39.432734\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 410.54954675393066\tTime: 0:00:38.409919\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 410.68495273152456\tTime: 0:00:38.359782\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 410.4853143895048\tTime: 0:00:39.668299\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 410.6049694858196\tTime: 0:00:37.502543\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 410.65469060754486\tTime: 0:00:38.157224\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 410.58326144840095\tTime: 0:00:38.008368\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 410.6165932830602\tTime: 0:00:38.869429\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 410.5244171480055\tTime: 0:00:39.491551\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 410.40623486633\tTime: 0:00:40.323058\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 410.4429778008162\tTime: 0:00:41.754667\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 410.2934482522521\tTime: 0:00:40.373649\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 410.33006137511285\tTime: 0:00:41.691977\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 410.46366218504335\tTime: 0:00:40.859947\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 410.35971528718125\tTime: 0:00:40.925702\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 410.3605677315481\tTime: 0:00:58.469468\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 410.29745272417813\tTime: 0:00:46.920949\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 410.3171818138541\tTime: 0:00:40.400700\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 410.2529804514124\tTime: 0:00:39.997567\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 410.2829515828349\tTime: 0:00:40.732548\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 410.1945200025977\tTime: 0:00:40.289319\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 410.2152259118991\tTime: 0:00:38.942882\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 410.26016398580316\tTime: 0:00:40.099900\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 410.31068542889113\tTime: 0:00:40.258259\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 410.18460686104766\tTime: 0:00:41.690986\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 410.20293979620385\tTime: 0:00:39.952818\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 410.09139348188427\tTime: 0:00:40.142246\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 410.2581434572002\tTime: 0:00:39.034343\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 410.0655680944623\tTime: 0:00:41.345045\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 410.1099037234118\tTime: 0:00:39.282606\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 410.2712727607458\tTime: 0:00:39.685653\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 409.9542191790322\tTime: 0:00:39.312088\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 410.0700327107665\tTime: 0:00:39.532242\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 410.10562082134396\tTime: 0:00:39.394651\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 409.7994513603847\tTime: 0:00:40.400364\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 410.0317132298837\tTime: 0:00:39.795706\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 410.1158410808908\tTime: 0:00:40.339400\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 410.0786651838609\tTime: 0:00:40.637817\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 410.03010391835693\tTime: 0:00:41.343315\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 410.0241405544742\tTime: 0:00:40.550505\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 410.0645957928985\tTime: 0:00:40.905945\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 409.7466780859757\tTime: 0:00:40.332033\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 409.959152828909\tTime: 0:00:38.155910\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 409.85212248252134\tTime: 0:00:40.577576\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 409.97382730954496\tTime: 0:00:40.975189\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 410.05337303791237\tTime: 0:00:39.938058\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 409.9503577423268\tTime: 0:00:40.731372\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 409.94439610485784\tTime: 0:00:40.471723\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 409.89695542884414\tTime: 0:00:38.256763\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 409.87590787326974\tTime: 0:00:36.695444\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 409.76744959900185\tTime: 0:00:39.200472\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 410.01916719707975\tTime: 0:00:41.135319\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 409.8943465604548\tTime: 0:00:39.932446\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 409.8400882028488\tTime: 0:00:39.311897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 15:18:02,178] Trial 32 finished with values: [0.001493669073961431, 0.8795454545454545] and parameters: {'num_topics': 44, 'dropout': 0.48517351505038425, 'num_neurons': 300, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 434.7885107822256\tTime: 0:00:16.222797\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 418.27998846902506\tTime: 0:00:14.811514\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 413.1451291401607\tTime: 0:00:13.965620\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 410.75763853625966\tTime: 0:00:14.364870\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 409.09258239328267\tTime: 0:00:13.983302\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 408.02248433591694\tTime: 0:00:14.035631\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 407.24657593370335\tTime: 0:00:14.091062\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 406.3195519596193\tTime: 0:00:13.902606\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 405.74905671688106\tTime: 0:00:14.221128\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 405.3576333790576\tTime: 0:00:13.894136\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 405.2357559841176\tTime: 0:00:14.451973\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 404.7272070400657\tTime: 0:00:13.829603\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 404.556283477735\tTime: 0:00:14.155260\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 404.2233219845501\tTime: 0:00:14.421344\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 404.2072517167165\tTime: 0:00:14.834195\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 403.8680438012488\tTime: 0:00:14.497908\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 403.96657067805893\tTime: 0:00:14.096949\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 403.70350774961446\tTime: 0:00:13.855250\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 403.7735494597721\tTime: 0:00:14.332185\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 403.6409369299161\tTime: 0:00:14.278547\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 403.4668240201022\tTime: 0:00:14.021256\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 403.5804593318617\tTime: 0:00:14.712145\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 403.3072227937754\tTime: 0:00:14.360472\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 403.4008594014472\tTime: 0:00:14.398526\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 403.22053456089435\tTime: 0:00:13.794846\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 403.18769614999434\tTime: 0:00:14.568733\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 403.1453825924006\tTime: 0:00:14.085092\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 403.1958922444496\tTime: 0:00:15.058844\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 403.09407434540265\tTime: 0:00:14.467543\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 402.9172490191031\tTime: 0:00:13.877421\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 403.0874416839455\tTime: 0:00:14.243273\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 402.88589091621293\tTime: 0:00:14.419978\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 402.79158905150973\tTime: 0:00:14.389392\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 402.80453719180207\tTime: 0:00:13.382939\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 402.7615689927852\tTime: 0:00:13.283231\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 402.634064884655\tTime: 0:00:13.125856\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 402.67705758405504\tTime: 0:00:13.198249\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 402.5647879023499\tTime: 0:00:13.224046\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 402.69024209611024\tTime: 0:00:13.481061\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 402.73408165658145\tTime: 0:00:14.225714\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 402.60518113680604\tTime: 0:00:14.811244\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 402.54544284791575\tTime: 0:00:14.272133\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 402.50973590864317\tTime: 0:00:14.134239\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 402.45014333642473\tTime: 0:00:14.171194\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 402.40592862990627\tTime: 0:00:14.689993\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 402.3747235901297\tTime: 0:00:14.591991\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 402.50182841915154\tTime: 0:00:14.518474\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 402.3614866795788\tTime: 0:00:13.825262\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 402.4678987310638\tTime: 0:00:13.519533\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 402.11602308898483\tTime: 0:00:14.471915\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 402.2262295592605\tTime: 0:00:14.162748\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 402.33050300645783\tTime: 0:00:14.100096\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 402.2297996360867\tTime: 0:00:14.130163\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 402.240366574219\tTime: 0:00:13.773862\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 402.28309112849576\tTime: 0:00:14.550785\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 402.1519668040745\tTime: 0:00:14.213222\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 402.2336178757147\tTime: 0:00:13.899913\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 402.1055130490437\tTime: 0:00:13.734250\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 402.30690412349895\tTime: 0:00:13.392822\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 402.1521833404447\tTime: 0:00:13.286295\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 402.1035015565641\tTime: 0:00:14.412305\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 402.10662805523816\tTime: 0:00:14.446020\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 402.00495921549407\tTime: 0:00:14.136641\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 402.1213497734945\tTime: 0:00:14.176572\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 401.9109746803416\tTime: 0:00:14.280491\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 402.03973341379157\tTime: 0:00:14.228783\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 401.93883470153463\tTime: 0:00:14.804436\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 402.09019810561716\tTime: 0:00:14.079408\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 401.90127645894455\tTime: 0:00:13.810896\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 401.9049965868432\tTime: 0:00:14.242768\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 401.8390479099667\tTime: 0:00:14.608623\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 401.7707481513414\tTime: 0:00:14.152716\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 401.96775606316527\tTime: 0:00:14.149229\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 401.88097710172883\tTime: 0:00:14.226681\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 401.983374855422\tTime: 0:00:13.965524\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 401.9054777053125\tTime: 0:00:14.112986\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 401.88443790023416\tTime: 0:00:13.965383\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 401.75716336840816\tTime: 0:00:14.132467\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 401.83952491442864\tTime: 0:00:14.317787\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 401.7942451942517\tTime: 0:00:14.452063\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 401.7573932385701\tTime: 0:00:14.643313\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 401.7572275395768\tTime: 0:00:14.245306\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 401.92437701439434\tTime: 0:00:14.649740\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 401.5964542619353\tTime: 0:00:14.354877\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 401.8827951623975\tTime: 0:00:14.348138\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 401.6847610628597\tTime: 0:00:13.771815\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 401.71112090391796\tTime: 0:00:13.829647\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 401.8820717582797\tTime: 0:00:13.835724\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 401.6695345705917\tTime: 0:00:14.834054\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 401.8448657041065\tTime: 0:00:14.164341\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 401.64537476844015\tTime: 0:00:13.926222\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 401.77901440431526\tTime: 0:00:14.274886\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 401.85120972386784\tTime: 0:00:14.121372\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 401.73501744632983\tTime: 0:00:14.564271\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 401.7001775708432\tTime: 0:00:14.305243\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 401.6306393858022\tTime: 0:00:14.776920\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 401.86119078327175\tTime: 0:00:14.364825\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 401.8246092868427\tTime: 0:00:14.817556\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 401.5416731682676\tTime: 0:00:14.702250\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 401.65014069905214\tTime: 0:00:13.843747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 15:42:39,759] Trial 33 finished with values: [0.037349219599564304, 0.9416666666666667] and parameters: {'num_topics': 36, 'dropout': 0.25301250335141695, 'num_neurons': 100, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 434.6297402547981\tTime: 0:00:15.393408\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 416.6899637761651\tTime: 0:00:12.920394\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 410.5331265383449\tTime: 0:00:12.816753\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 407.67744518232104\tTime: 0:00:12.280762\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 406.11453859350314\tTime: 0:00:12.217550\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 404.8775560107411\tTime: 0:00:12.340791\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 404.2637107376768\tTime: 0:00:11.525836\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 403.41594557667804\tTime: 0:00:11.422423\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 403.08593342272485\tTime: 0:00:11.468013\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 402.5762277079572\tTime: 0:00:12.066791\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 402.1647379685854\tTime: 0:00:12.605131\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 401.8948511508054\tTime: 0:00:12.476248\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 401.596695886404\tTime: 0:00:12.123515\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 401.43827144984675\tTime: 0:00:12.446587\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 401.2095711353015\tTime: 0:00:11.843620\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 400.97755162491535\tTime: 0:00:11.961461\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 400.692607944207\tTime: 0:00:11.878079\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 400.79503163083956\tTime: 0:00:12.007107\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 400.60595512117516\tTime: 0:00:12.338720\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 400.5097695186141\tTime: 0:00:12.141160\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 400.2150573110613\tTime: 0:00:12.325926\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 400.12360593921613\tTime: 0:00:12.209816\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 400.04245751111955\tTime: 0:00:12.366611\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 399.8217450561386\tTime: 0:00:13.677488\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 399.83285912051923\tTime: 0:00:12.755243\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 399.7307923974907\tTime: 0:00:12.386249\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 399.52565752122825\tTime: 0:00:12.425200\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 399.5670633157489\tTime: 0:00:12.488962\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 399.61674050575255\tTime: 0:00:11.845853\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 399.37422968885176\tTime: 0:00:12.459722\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 399.4502666464493\tTime: 0:00:12.809595\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 399.2514393883294\tTime: 0:00:12.241048\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 399.18918652262937\tTime: 0:00:12.777510\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 399.1248383782818\tTime: 0:00:12.599583\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 399.11771409294363\tTime: 0:00:12.372401\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 399.17804681916704\tTime: 0:00:12.255347\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 398.915295526781\tTime: 0:00:13.349023\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 398.8932504346155\tTime: 0:00:12.341333\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 398.98248990305046\tTime: 0:00:12.406116\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 398.83863683981934\tTime: 0:00:12.068052\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 398.71166645068126\tTime: 0:00:12.262271\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 398.8521747524543\tTime: 0:00:12.156373\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 398.7658561925214\tTime: 0:00:11.961435\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 398.74077059178234\tTime: 0:00:12.544888\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 398.58874883632365\tTime: 0:00:12.410964\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 398.62076393149823\tTime: 0:00:12.348872\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 398.596226779367\tTime: 0:00:12.220738\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 398.5798136530975\tTime: 0:00:12.012171\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 398.56325583865805\tTime: 0:00:12.410133\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 398.5667480064696\tTime: 0:00:11.913266\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 398.5740007076093\tTime: 0:00:13.273975\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 398.4370827955738\tTime: 0:00:11.847634\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 398.50521781024315\tTime: 0:00:12.074229\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 398.37350481544564\tTime: 0:00:12.674542\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 398.18417849003\tTime: 0:00:12.065204\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 398.42030598370616\tTime: 0:00:12.530090\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 398.2364961013903\tTime: 0:00:12.835531\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 398.24314995733187\tTime: 0:00:12.547255\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 398.3269741064376\tTime: 0:00:12.202512\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 398.14560871587724\tTime: 0:00:12.110988\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 398.12670363983864\tTime: 0:00:12.540174\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 398.12447520693456\tTime: 0:00:12.635042\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 398.0915921296688\tTime: 0:00:11.884180\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 398.15752236697654\tTime: 0:00:12.690742\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 398.18781049736\tTime: 0:00:12.038357\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 398.1212020365512\tTime: 0:00:12.551541\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 398.02551657974357\tTime: 0:00:12.671908\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 397.99738323418575\tTime: 0:00:12.607976\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 398.01057270508915\tTime: 0:00:12.157002\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 398.0123606086145\tTime: 0:00:12.294887\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 398.0476103053534\tTime: 0:00:12.064788\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 397.9421023620867\tTime: 0:00:12.233596\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 398.1996582141447\tTime: 0:00:12.042013\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 397.7852612673846\tTime: 0:00:12.277825\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 397.992459097951\tTime: 0:00:12.261576\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 397.9024645255021\tTime: 0:00:12.814409\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 397.8977160426893\tTime: 0:00:12.415016\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 397.94148415901464\tTime: 0:00:12.449061\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 397.85637438260505\tTime: 0:00:13.114801\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 397.77177022504793\tTime: 0:00:11.833516\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 397.9089435728626\tTime: 0:00:12.386807\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 397.8291870016174\tTime: 0:00:12.334395\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 397.8400409284962\tTime: 0:00:12.282279\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 397.7191910068974\tTime: 0:00:13.640391\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 397.71901939401846\tTime: 0:00:12.475240\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 397.87631402130114\tTime: 0:00:12.606514\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 397.8425682146501\tTime: 0:00:12.239990\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 397.68756788112165\tTime: 0:00:12.539334\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 397.7125919554112\tTime: 0:00:12.540770\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 397.73196643234456\tTime: 0:00:12.864907\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 397.7599606362959\tTime: 0:00:12.423298\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 397.6481557272272\tTime: 0:00:12.587105\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 397.88441237134913\tTime: 0:00:12.696589\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 397.7231814471197\tTime: 0:00:12.146275\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 397.66140463084423\tTime: 0:00:12.514052\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 397.64380953556383\tTime: 0:00:12.382116\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 397.6697438339845\tTime: 0:00:12.465988\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 397.60316622823103\tTime: 0:00:12.417473\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 397.67237125213927\tTime: 0:00:12.535563\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 397.441496023518\tTime: 0:00:12.390886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 16:04:15,903] Trial 34 finished with values: [0.06957557441532379, 0.8125] and parameters: {'num_topics': 40, 'dropout': 0.03433790037691362, 'num_neurons': 50, 'num_layers': 1, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 424.575434696304\tTime: 0:00:21.383007\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 414.3252357546618\tTime: 0:00:20.524199\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 409.9393815706575\tTime: 0:00:20.181550\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 406.9987674140055\tTime: 0:00:20.011797\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 405.3805028154503\tTime: 0:00:19.825803\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 404.25865932433896\tTime: 0:00:20.000566\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 403.6520216085299\tTime: 0:00:20.492401\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 403.1792220661838\tTime: 0:00:19.089506\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 402.8752724427918\tTime: 0:00:19.673146\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 402.61551754359675\tTime: 0:00:19.499851\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 402.40261531121877\tTime: 0:00:19.390355\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 402.1529078832609\tTime: 0:00:19.442406\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 401.9791754823967\tTime: 0:00:19.581665\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 401.7665998360862\tTime: 0:00:19.534611\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 401.73979406601455\tTime: 0:00:19.251733\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 401.540073039793\tTime: 0:00:19.244186\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 401.2208922958747\tTime: 0:00:19.315638\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 401.1161667853852\tTime: 0:00:19.512967\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 401.009515148069\tTime: 0:00:19.623236\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 400.9650832440006\tTime: 0:00:20.871559\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 400.8812814280777\tTime: 0:00:21.581628\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 400.86751588594564\tTime: 0:00:23.006066\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 400.6386091363876\tTime: 0:00:24.824544\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 400.5152843454967\tTime: 0:00:25.244917\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 400.43177363232763\tTime: 0:00:26.094869\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 400.5251887815133\tTime: 0:00:25.098070\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 400.4691698491452\tTime: 0:00:25.127353\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 400.4760589748834\tTime: 0:00:25.385153\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 400.2799222408528\tTime: 0:00:25.407935\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 400.34624584338326\tTime: 0:00:25.699448\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 400.1990144087231\tTime: 0:00:25.936860\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 400.31181861752896\tTime: 0:00:25.952297\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 400.07370064219657\tTime: 0:00:26.064782\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 399.91769395634685\tTime: 0:00:26.621643\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 400.0719878193057\tTime: 0:00:26.752283\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 400.02536303911126\tTime: 0:00:26.683716\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 399.87888285158306\tTime: 0:00:26.594033\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 399.925150925299\tTime: 0:00:26.698955\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 399.79203207867863\tTime: 0:00:27.336973\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 399.77716780272044\tTime: 0:00:36.195106\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 399.71641845978615\tTime: 0:00:27.638356\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 399.5807886361125\tTime: 0:00:27.264098\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 399.7335079729463\tTime: 0:00:27.206152\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 399.64622985752607\tTime: 0:00:26.564080\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 399.7170422094218\tTime: 0:00:27.914474\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 399.5200121113439\tTime: 0:00:28.944198\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 399.5754682695427\tTime: 0:00:30.825661\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 399.41402429850297\tTime: 0:00:28.918985\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 399.57748560244346\tTime: 0:00:27.090963\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 399.41880051413335\tTime: 0:00:27.397263\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 399.3380925786833\tTime: 0:00:26.880455\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 399.27168390435753\tTime: 0:00:27.671328\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 399.3549241582741\tTime: 0:00:27.171151\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 399.4844165808602\tTime: 0:00:27.526413\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 399.3776046074532\tTime: 0:00:27.623053\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 399.32255434016025\tTime: 0:00:27.889185\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 399.2162934233478\tTime: 0:00:28.033890\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 399.1842290335491\tTime: 0:00:28.858458\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 399.3715772560629\tTime: 0:00:29.557727\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 399.2630523863006\tTime: 0:00:38.946892\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 399.15663877369667\tTime: 0:00:28.374205\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 399.054462184632\tTime: 0:00:31.170314\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 398.96954363610075\tTime: 0:00:38.771602\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 399.15606182089493\tTime: 0:00:27.572563\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 398.99097831242244\tTime: 0:00:28.823833\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 399.14797213964823\tTime: 0:00:28.337801\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 399.23569574329537\tTime: 0:00:29.316354\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 398.9571894188906\tTime: 0:00:29.148034\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 399.1095532246765\tTime: 0:00:29.197364\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 399.02471295248205\tTime: 0:00:28.603308\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 398.97951992031756\tTime: 0:00:28.825361\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 399.08416399550043\tTime: 0:00:28.510702\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 399.0435145171566\tTime: 0:00:29.167955\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 399.1529623938586\tTime: 0:00:28.663042\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 398.94057588169056\tTime: 0:00:28.493296\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 399.03997415668726\tTime: 0:00:29.133171\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 398.89110347375026\tTime: 0:00:29.655189\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 398.8437885688191\tTime: 0:00:28.241838\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 398.8821072783255\tTime: 0:00:28.795910\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 398.8554891363758\tTime: 0:00:28.532552\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 398.76983227045014\tTime: 0:00:28.607835\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 398.8689065673663\tTime: 0:00:28.685508\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 398.87480469484643\tTime: 0:00:29.331947\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 398.8025544973741\tTime: 0:00:29.285177\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 398.65692168516796\tTime: 0:00:29.569796\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 398.7149049091274\tTime: 0:00:28.293889\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 398.7295534568241\tTime: 0:00:26.925936\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 398.92481886613257\tTime: 0:00:26.552874\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 398.8338299160625\tTime: 0:00:26.578067\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 398.75272901944396\tTime: 0:00:26.572703\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 398.66331687289596\tTime: 0:00:34.290758\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 398.72944425196766\tTime: 0:00:28.805152\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 398.7102225442902\tTime: 0:00:33.803927\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 398.70703782506536\tTime: 0:00:27.993556\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 398.6404768957346\tTime: 0:00:32.957043\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 398.77854724408525\tTime: 0:00:32.823230\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 398.6601455609273\tTime: 0:00:25.745295\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 398.56828300873815\tTime: 0:00:32.090447\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 398.56647490249804\tTime: 0:00:26.423286\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 398.6282427193824\tTime: 0:00:32.471075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 16:49:36,785] Trial 35 finished with values: [0.035804896465600665, 0.9451612903225807] and parameters: {'num_topics': 31, 'dropout': 0.18908476511582786, 'num_neurons': 200, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 429.02862122275354\tTime: 0:00:25.429694\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 418.5901585450284\tTime: 0:00:30.116447\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 413.6274113225711\tTime: 0:00:23.477419\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 410.8408644646148\tTime: 0:00:22.479271\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 409.63570552875575\tTime: 0:00:29.527161\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 408.4820751964145\tTime: 0:00:22.473069\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 407.75936186456215\tTime: 0:00:30.102366\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 407.2120706886378\tTime: 0:00:22.083966\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 406.87504022176853\tTime: 0:00:29.790051\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 406.3572711260156\tTime: 0:00:22.049644\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 406.2262663282014\tTime: 0:00:29.688384\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 405.75944509974704\tTime: 0:00:21.884512\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 405.6537411166826\tTime: 0:00:22.137031\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 405.5611661065739\tTime: 0:00:29.669859\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 405.2626640901226\tTime: 0:00:22.196659\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 404.95785302297264\tTime: 0:00:28.645279\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 405.2049797561452\tTime: 0:00:21.960009\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 404.833121168096\tTime: 0:00:29.273861\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 404.8036194743004\tTime: 0:00:22.055088\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 404.5046495997071\tTime: 0:00:23.334772\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 404.4180624905966\tTime: 0:00:32.553961\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 404.3191148096037\tTime: 0:00:26.916659\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 404.15374879371427\tTime: 0:00:34.964931\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 404.2037046709852\tTime: 0:00:36.095784\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 404.07833040651093\tTime: 0:00:29.500707\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 404.1823954718884\tTime: 0:00:42.196563\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 404.2362243197783\tTime: 0:00:34.437021\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 403.897780764841\tTime: 0:00:33.784850\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 403.84615147833915\tTime: 0:00:35.389464\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 403.82426095530775\tTime: 0:00:35.435507\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 403.8002848141997\tTime: 0:00:35.161811\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 403.90226859590103\tTime: 0:00:35.399841\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 403.6409780332576\tTime: 0:00:36.119680\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 403.60356418497986\tTime: 0:00:35.540600\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 403.6758811248754\tTime: 0:00:36.149311\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 403.79952864495175\tTime: 0:00:36.775379\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 403.61729641099873\tTime: 0:00:36.681557\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 403.56083992456087\tTime: 0:00:37.149555\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 403.3522300711606\tTime: 0:00:35.798702\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 403.54089911043405\tTime: 0:00:37.168970\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 403.3956132339389\tTime: 0:00:38.027681\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 403.44770409737504\tTime: 0:00:37.311526\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 403.36773095302743\tTime: 0:00:37.890865\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 403.2670692002487\tTime: 0:00:37.063415\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 403.2720766084006\tTime: 0:00:37.181384\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 403.21196107972713\tTime: 0:00:40.720324\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 403.13425736142847\tTime: 0:00:38.224074\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 403.14065872016755\tTime: 0:00:38.192513\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 402.99744487083194\tTime: 0:00:37.595886\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 403.27079399313783\tTime: 0:00:37.630605\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 403.0295935610495\tTime: 0:00:38.867437\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 403.16607516791026\tTime: 0:00:38.112117\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 403.0853532007565\tTime: 0:00:37.902588\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 402.9512424743051\tTime: 0:00:37.344801\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 403.1663085643642\tTime: 0:00:38.385922\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 403.03073321455605\tTime: 0:00:38.743162\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 402.96308622401125\tTime: 0:00:38.721541\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 402.93048311964003\tTime: 0:00:39.151785\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 402.9238167012831\tTime: 0:00:37.515098\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 402.9322159615846\tTime: 0:00:37.759714\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 402.80780903982594\tTime: 0:00:38.726821\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 402.8059940646628\tTime: 0:00:39.356165\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 402.6851101290388\tTime: 0:00:38.605233\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 402.6993026096324\tTime: 0:00:38.658929\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 402.82213926620216\tTime: 0:00:39.162369\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 402.64656827136463\tTime: 0:00:38.911235\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 402.8001334407677\tTime: 0:00:38.716979\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 402.732768112799\tTime: 0:00:39.295325\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 402.5596417933781\tTime: 0:00:38.387765\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 402.5898341996257\tTime: 0:00:39.628675\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 402.6757625533058\tTime: 0:00:39.121640\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 402.6105254527759\tTime: 0:00:39.118232\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 402.5276686096536\tTime: 0:00:39.926963\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 402.6188302378366\tTime: 0:00:39.291573\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 402.5522731286556\tTime: 0:00:39.566025\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 402.4328545877647\tTime: 0:00:39.202804\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 402.4434756324405\tTime: 0:00:39.284272\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 402.43354181065695\tTime: 0:00:39.158629\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 402.64841134666744\tTime: 0:00:39.844252\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 402.5725960679493\tTime: 0:00:38.859711\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 402.48837201528767\tTime: 0:00:39.262002\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 402.55333711381223\tTime: 0:00:39.676525\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 402.4984721972443\tTime: 0:00:39.623508\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 402.44780015210074\tTime: 0:00:38.510130\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 402.3767101965379\tTime: 0:00:39.192407\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 402.28531937773874\tTime: 0:00:38.465369\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 402.4666618841331\tTime: 0:00:38.749813\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 402.35208611763477\tTime: 0:00:38.984714\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 402.47550340022707\tTime: 0:00:39.738122\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 402.26346191369515\tTime: 0:00:37.821043\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 402.44779556057466\tTime: 0:00:38.444986\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 402.487305569073\tTime: 0:00:38.692061\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 402.32123587426184\tTime: 0:00:40.004622\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 402.5090463348896\tTime: 0:00:40.543148\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 402.31382026577666\tTime: 0:00:39.551396\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 402.3751060826187\tTime: 0:00:39.866203\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 402.31465298494743\tTime: 0:00:39.825866\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 402.2610252826323\tTime: 0:00:38.369183\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 402.1305470777618\tTime: 0:00:39.542972\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 402.305990758764\tTime: 0:00:40.008339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 17:49:56,873] Trial 36 finished with values: [0.01943737764346765, 0.9315789473684211] and parameters: {'num_topics': 38, 'dropout': 0.29106020570876007, 'num_neurons': 300, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 413.0036099312726\tTime: 0:00:20.714647\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 404.286014534788\tTime: 0:00:19.722218\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 401.7207208299246\tTime: 0:00:18.379890\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 400.2114027501552\tTime: 0:00:19.697212\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 399.29005919175034\tTime: 0:00:19.567661\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 398.7276149145109\tTime: 0:00:19.972792\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 398.29431616087885\tTime: 0:00:18.708905\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 397.90306788876194\tTime: 0:00:19.396169\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 397.6230881252703\tTime: 0:00:19.529996\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 397.1999800617571\tTime: 0:00:17.894347\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 397.1133260265624\tTime: 0:00:17.951895\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 396.8446449068001\tTime: 0:00:19.505019\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 396.86489552037494\tTime: 0:00:18.174423\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 396.72237679135634\tTime: 0:00:18.201727\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 396.58052912305817\tTime: 0:00:25.423312\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 396.45341540484185\tTime: 0:00:17.791261\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 396.39489232761133\tTime: 0:00:17.573708\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 396.46160749548636\tTime: 0:00:17.641404\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 396.27576084893127\tTime: 0:00:17.639456\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 396.29176848834913\tTime: 0:00:18.236855\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 396.15119770508915\tTime: 0:00:19.446610\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 396.1296502608868\tTime: 0:00:21.191632\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 396.0768430091848\tTime: 0:00:22.075896\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 396.1574969115559\tTime: 0:00:22.921731\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 395.94550358094955\tTime: 0:00:24.190019\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 395.9799983573356\tTime: 0:00:24.055552\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 395.8649429783885\tTime: 0:00:24.295480\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 395.94912772758687\tTime: 0:00:24.165399\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 395.90200827473814\tTime: 0:00:24.721190\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 395.9839099702381\tTime: 0:00:24.457545\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 395.75935723630386\tTime: 0:00:33.448849\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 395.81397854486386\tTime: 0:00:37.502525\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 395.7654946373914\tTime: 0:00:32.053308\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 395.67526784391924\tTime: 0:00:33.208580\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 395.66240609778174\tTime: 0:00:24.470401\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 395.57840519328784\tTime: 0:00:26.057345\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 395.74514569169395\tTime: 0:00:25.334885\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 395.62025669642856\tTime: 0:00:25.104133\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 395.72401012574755\tTime: 0:00:26.520317\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 395.65112021849495\tTime: 0:00:25.352489\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 395.4946381260344\tTime: 0:00:25.671530\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 395.6377459662158\tTime: 0:00:25.513189\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 395.56747952840544\tTime: 0:00:26.048257\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 395.4459353313304\tTime: 0:00:25.445162\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 395.5880876932996\tTime: 0:00:26.183869\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 395.4369892019061\tTime: 0:00:26.211969\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 395.57522730625374\tTime: 0:00:26.006532\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 395.44544245855434\tTime: 0:00:26.309489\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 395.3868217471131\tTime: 0:00:25.868707\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 395.47536198122367\tTime: 0:00:26.065245\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 395.4191505720821\tTime: 0:00:26.301368\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 395.4571631553614\tTime: 0:00:31.088442\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 395.44155167281417\tTime: 0:00:26.623518\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 395.47049316369515\tTime: 0:00:26.080571\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 395.3511897415463\tTime: 0:00:25.930041\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 395.2868744725255\tTime: 0:00:25.930395\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 395.49226125827505\tTime: 0:00:26.604144\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 395.2793066090353\tTime: 0:00:26.445245\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 395.39801768758696\tTime: 0:00:26.137953\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 395.32892407246766\tTime: 0:00:26.729927\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 395.31797868238925\tTime: 0:00:26.687325\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 395.2518013475137\tTime: 0:00:28.745366\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 395.2588606903069\tTime: 0:00:28.214451\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 395.43091942481476\tTime: 0:00:28.244636\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 395.23107108475796\tTime: 0:00:28.035685\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 395.2900061871732\tTime: 0:00:27.294467\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 395.22355119353233\tTime: 0:00:27.647148\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 395.2309477380012\tTime: 0:00:27.888798\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 395.2676808282555\tTime: 0:00:27.970434\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 395.3862233059693\tTime: 0:00:28.529517\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 395.23088687273145\tTime: 0:00:28.251201\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 395.2708912600264\tTime: 0:00:28.249406\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 395.2393373377318\tTime: 0:00:27.375191\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 395.1291165418947\tTime: 0:00:27.617865\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 395.23887124273585\tTime: 0:00:27.532581\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 395.1959782345501\tTime: 0:00:25.630511\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 395.22872404355206\tTime: 0:00:27.656787\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 395.1868483338858\tTime: 0:00:28.418983\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 395.2061814870844\tTime: 0:00:28.029045\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 395.16569260195683\tTime: 0:00:28.347498\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 395.2097833740032\tTime: 0:00:29.091423\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 395.1652823031859\tTime: 0:00:28.120944\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 395.1440335285724\tTime: 0:00:28.347364\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 395.23217969954817\tTime: 0:00:29.154500\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 395.10441751091975\tTime: 0:00:28.373509\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 395.0521239224239\tTime: 0:00:28.151639\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 395.16698065358645\tTime: 0:00:28.112773\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 395.2173605307422\tTime: 0:00:28.610836\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 395.2361022586488\tTime: 0:00:28.212777\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 395.15579051680163\tTime: 0:00:29.024696\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 395.10187112401735\tTime: 0:00:29.073625\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 395.18414866347655\tTime: 0:00:28.473512\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 395.1062833234597\tTime: 0:00:28.085009\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 395.2852120829572\tTime: 0:00:28.615849\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 395.1374830003338\tTime: 0:00:28.085242\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 395.2062548045729\tTime: 0:00:28.574047\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 395.1217628639133\tTime: 0:00:28.798507\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 395.1087797545583\tTime: 0:00:28.215890\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 395.04962121011766\tTime: 0:00:27.505112\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 394.99729687676313\tTime: 0:00:27.093728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 18:33:28,803] Trial 37 finished with values: [0.06570286309629972, 0.9727272727272728] and parameters: {'num_topics': 11, 'dropout': 0.03457635564856709, 'num_neurons': 200, 'num_layers': 1, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 422.6027830378301\tTime: 0:00:25.258055\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 412.9924826065645\tTime: 0:00:21.492841\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 410.6789391914447\tTime: 0:00:20.858736\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 409.7099144345238\tTime: 0:00:21.018843\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 409.00064850714426\tTime: 0:00:20.707018\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 408.33945211119106\tTime: 0:00:21.022770\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 408.0504711566943\tTime: 0:00:20.726864\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 407.5652859176352\tTime: 0:00:21.306549\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 407.4429959832595\tTime: 0:00:21.509183\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 407.3409252931524\tTime: 0:00:21.593413\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 407.1273977316539\tTime: 0:00:21.011646\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 406.9804362052631\tTime: 0:00:21.392867\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 407.1162978826379\tTime: 0:00:21.398238\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 406.8361349723539\tTime: 0:00:21.286894\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 406.6151823621925\tTime: 0:00:21.489900\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 406.5507390814244\tTime: 0:00:21.037453\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 406.4482454566666\tTime: 0:00:21.048293\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 406.54677332524636\tTime: 0:00:21.164837\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 406.3077660631418\tTime: 0:00:20.923783\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 406.2275746927424\tTime: 0:00:21.179852\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 406.10958216965696\tTime: 0:00:22.208031\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 406.1606664603786\tTime: 0:00:20.868376\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 406.15049196916374\tTime: 0:00:20.637980\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 406.1528928597876\tTime: 0:00:21.390981\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 405.908346233685\tTime: 0:00:20.134179\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 406.16080846709735\tTime: 0:00:20.680370\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 406.0479278185652\tTime: 0:00:20.882963\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 405.9442758803388\tTime: 0:00:20.964275\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 405.78167748944463\tTime: 0:00:21.524149\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 405.92820840415305\tTime: 0:00:20.866802\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 406.0376848585134\tTime: 0:00:20.861360\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 406.0031945267246\tTime: 0:00:21.622930\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 405.9598454881681\tTime: 0:00:21.224848\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 406.0539013205376\tTime: 0:00:21.401394\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 405.8696395218583\tTime: 0:00:21.616191\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 405.8298466018887\tTime: 0:00:21.013547\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 405.7241063274021\tTime: 0:00:21.266787\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 405.8061818397136\tTime: 0:00:20.936425\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 405.693163996382\tTime: 0:00:21.362310\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 405.891891820413\tTime: 0:00:21.538592\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 405.8250601747043\tTime: 0:00:21.688189\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 405.571924676639\tTime: 0:00:21.057263\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 405.64226395441915\tTime: 0:00:21.393348\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 405.6249430283443\tTime: 0:00:21.178215\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 405.67515224765856\tTime: 0:00:21.024131\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 405.77102918947\tTime: 0:00:21.780570\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 405.8448727934228\tTime: 0:00:22.278152\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 405.57048113757\tTime: 0:00:21.559324\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 405.712053644893\tTime: 0:00:21.510154\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 405.6142694198779\tTime: 0:00:44.185145\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 405.5910008145735\tTime: 0:00:21.864132\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 405.52074014371993\tTime: 0:00:20.513299\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 405.5698033915877\tTime: 0:00:20.251856\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 405.5477099231033\tTime: 0:00:21.237406\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 405.5444907857989\tTime: 0:00:21.383711\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 405.56370099629504\tTime: 0:00:22.030285\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 405.5123011024364\tTime: 0:00:21.532232\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 405.65518689641635\tTime: 0:00:21.310090\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 405.40586941758585\tTime: 0:00:21.248606\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 405.4639107518995\tTime: 0:00:21.464492\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 405.55598715900754\tTime: 0:00:22.949241\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 405.54726153303193\tTime: 0:00:20.801776\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 405.45320309267566\tTime: 0:00:20.827658\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 405.5631714647747\tTime: 0:00:20.906165\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 405.46441714212835\tTime: 0:00:20.630836\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 405.5295830189056\tTime: 0:00:20.658275\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 405.3716873057601\tTime: 0:00:20.746245\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 405.274098459363\tTime: 0:00:21.730001\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 405.4773909949668\tTime: 0:00:20.999903\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 405.45318729782593\tTime: 0:00:21.931636\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 405.5420466613655\tTime: 0:00:20.993615\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 405.5822605324466\tTime: 0:00:21.020655\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 405.46987103027675\tTime: 0:00:21.110750\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 405.4330501500437\tTime: 0:00:21.646606\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 405.3029923820338\tTime: 0:00:20.874775\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 405.44042700604876\tTime: 0:00:21.464330\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 405.3632355551324\tTime: 0:00:21.392891\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 405.4569898160686\tTime: 0:00:21.309685\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 405.3254438058917\tTime: 0:00:21.134428\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 405.30103797140646\tTime: 0:00:20.786119\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 405.3022134755487\tTime: 0:00:20.939780\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 405.37876736551897\tTime: 0:00:21.211674\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 405.2902022637032\tTime: 0:00:21.242441\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 405.30845053111835\tTime: 0:00:22.318633\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 405.25667662991094\tTime: 0:00:20.938176\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 405.4503631419615\tTime: 0:00:21.665809\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 405.2091262715221\tTime: 0:00:29.086912\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 405.5081780589408\tTime: 0:00:21.300870\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 405.4087065766522\tTime: 0:00:21.336649\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 405.3174175244372\tTime: 0:00:20.525674\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 405.25174437585804\tTime: 0:00:22.269394\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 405.31713468643034\tTime: 0:00:36.668587\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 405.240333478499\tTime: 0:00:36.691100\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 405.3986211610434\tTime: 0:00:33.713693\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 405.4329215505811\tTime: 0:00:31.971267\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 405.30724924096563\tTime: 0:00:21.513731\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 405.3825045738946\tTime: 0:00:21.611474\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 405.285956608095\tTime: 0:00:20.584141\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 405.3452804842069\tTime: 0:00:19.885112\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 405.29743248473113\tTime: 0:00:20.255708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 19:11:22,180] Trial 38 finished with values: [-0.0069633691403946105, 0.9769230769230769] and parameters: {'num_topics': 13, 'dropout': 0.45518437913526, 'num_neurons': 300, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 415.47284982873975\tTime: 0:00:26.311212\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 408.0903299007819\tTime: 0:00:24.561949\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 405.9750928443307\tTime: 0:00:23.968903\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 404.3205218735895\tTime: 0:00:24.844696\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 402.7349133105182\tTime: 0:00:24.159908\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 401.8698459201389\tTime: 0:00:24.271549\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 401.2482085334504\tTime: 0:00:23.659534\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 400.71958113968583\tTime: 0:00:24.190609\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 400.3879271662012\tTime: 0:00:23.753100\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 400.0797857366189\tTime: 0:00:23.660633\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 399.83542104514595\tTime: 0:00:22.614846\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 399.6213953948977\tTime: 0:00:23.955551\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 399.46919897778673\tTime: 0:00:22.348845\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 399.139898914431\tTime: 0:00:21.573902\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 399.21445130308246\tTime: 0:00:21.504356\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 398.86952039930554\tTime: 0:00:24.945580\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 398.98902180805925\tTime: 0:00:24.778582\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 398.9096642587969\tTime: 0:00:21.645683\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 398.81335916636107\tTime: 0:00:28.197410\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 398.7558547834739\tTime: 0:00:30.563903\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 398.6596608427015\tTime: 0:00:25.652289\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 398.5590570533481\tTime: 0:00:27.534778\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 398.47893216824644\tTime: 0:00:35.287518\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 398.46718807302244\tTime: 0:00:35.802426\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 398.4438793561461\tTime: 0:00:29.574390\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 398.32754521881816\tTime: 0:00:41.760656\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 398.37976372667947\tTime: 0:00:34.756713\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 398.41036466855206\tTime: 0:00:33.640422\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 398.2051403493615\tTime: 0:00:42.544250\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 398.30254145744\tTime: 0:00:32.685352\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 398.17124070822047\tTime: 0:00:32.948612\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 398.1753877378484\tTime: 0:00:34.363537\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 398.1773237823714\tTime: 0:00:34.138532\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 397.9817955541098\tTime: 0:00:34.687769\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 398.1882302363086\tTime: 0:00:34.641791\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 398.11386415341957\tTime: 0:00:35.880539\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 398.06924726594826\tTime: 0:00:35.426382\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 398.02184853811684\tTime: 0:00:34.792876\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 397.9078612583338\tTime: 0:00:35.996985\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 397.8739179793289\tTime: 0:00:36.171657\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 397.91610932886437\tTime: 0:00:35.667180\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 397.95856966836396\tTime: 0:00:35.551429\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 398.0344193651616\tTime: 0:00:36.060267\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 397.97001538785685\tTime: 0:00:37.668336\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 397.8963599630092\tTime: 0:00:37.309239\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 397.89753451211396\tTime: 0:00:36.574372\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 397.772678502372\tTime: 0:00:36.341811\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 397.94992518383737\tTime: 0:00:36.509525\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 397.85815247863064\tTime: 0:00:36.305406\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 397.69936747871293\tTime: 0:00:36.457958\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 397.7055917882062\tTime: 0:00:36.854984\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 397.8976830938981\tTime: 0:00:37.187179\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 397.6794225138466\tTime: 0:00:36.824812\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 397.652484213966\tTime: 0:00:38.616350\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 397.69340613510167\tTime: 0:00:38.386564\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 397.66943642213005\tTime: 0:00:37.986446\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 397.67835816136784\tTime: 0:00:37.250626\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 397.8043865162821\tTime: 0:00:39.099961\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 397.5687715103814\tTime: 0:00:37.330427\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 397.72047817695403\tTime: 0:00:36.870231\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 397.64931451821445\tTime: 0:00:38.480513\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 397.4634698184665\tTime: 0:00:39.325922\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 397.4823873099916\tTime: 0:00:37.527163\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 397.77060886280606\tTime: 0:00:49.785906\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 397.44767169956697\tTime: 0:00:37.991862\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 397.6143989376458\tTime: 0:00:36.078777\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 397.636609434947\tTime: 0:00:37.289175\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 397.62273039029003\tTime: 0:00:38.529362\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 397.65569372743175\tTime: 0:00:38.382177\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 397.7049358611558\tTime: 0:00:40.270914\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 397.6886370455197\tTime: 0:00:37.420429\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 397.4640682228781\tTime: 0:00:38.303567\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 397.5420472123486\tTime: 0:00:38.344094\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 397.5066948123542\tTime: 0:00:36.660308\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 397.6824641611186\tTime: 0:00:36.984179\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 397.3703228878686\tTime: 0:00:39.996198\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 397.48141791027234\tTime: 0:00:39.008194\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 397.5445187022775\tTime: 0:00:38.948323\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 397.4146151728118\tTime: 0:00:38.569048\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 397.4903103372076\tTime: 0:00:38.173319\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 397.58640994966805\tTime: 0:00:38.044033\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 397.4545930761844\tTime: 0:00:38.174441\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 397.42930273011405\tTime: 0:00:38.529356\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 397.5128564750656\tTime: 0:00:39.909685\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 397.562848515196\tTime: 0:00:39.308188\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 397.3772909878555\tTime: 0:00:38.808709\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 397.4102243147827\tTime: 0:00:39.198035\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 397.42024129828667\tTime: 0:00:36.546177\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 397.49608331805274\tTime: 0:00:37.983490\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 397.3431666190617\tTime: 0:00:38.792884\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 397.42185747873646\tTime: 0:00:38.597665\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 397.3826079015976\tTime: 0:00:39.203374\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 397.3627837122334\tTime: 0:00:38.633849\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 397.38101831526694\tTime: 0:00:38.070212\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 397.3866131081678\tTime: 0:00:38.584433\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 397.360910075733\tTime: 0:00:38.498958\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 397.50083117641805\tTime: 0:00:38.806178\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 397.45511202882864\tTime: 0:00:38.805471\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 397.46577270755756\tTime: 0:00:38.991511\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 397.2819447897272\tTime: 0:00:38.378249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 20:10:18,597] Trial 39 finished with values: [0.03958860361429498, 0.9928571428571429] and parameters: {'num_topics': 14, 'dropout': 0.16603425305446737, 'num_neurons': 300, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 446.30865101551336\tTime: 0:00:26.518254\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 427.501864233055\tTime: 0:00:23.430865\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 422.26134709351254\tTime: 0:00:22.692068\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 419.59884292073366\tTime: 0:00:22.354223\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 418.14719165367814\tTime: 0:00:22.255888\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 417.1782988388508\tTime: 0:00:22.415590\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 416.31980761579166\tTime: 0:00:22.462718\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 415.4147183903182\tTime: 0:00:22.236204\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 414.90160293481534\tTime: 0:00:22.610625\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 414.5240679348976\tTime: 0:00:22.160776\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 414.2206833630717\tTime: 0:00:22.064444\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 413.92763082690374\tTime: 0:00:22.195984\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 413.55436102412926\tTime: 0:00:22.760247\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 413.3239287492125\tTime: 0:00:22.177779\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 413.0483252977954\tTime: 0:00:22.184664\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 413.00552265084303\tTime: 0:00:21.710430\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 413.0280128639133\tTime: 0:00:22.169192\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 413.00474558096835\tTime: 0:00:22.315719\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 412.6986525964676\tTime: 0:00:21.929425\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 412.6172434064216\tTime: 0:00:22.199201\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 412.5429798798592\tTime: 0:00:22.086733\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 412.4612236439056\tTime: 0:00:22.069069\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 412.19857839740604\tTime: 0:00:22.083621\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 412.077603806691\tTime: 0:00:21.938465\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 412.29181161196215\tTime: 0:00:22.016897\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 412.1653084932507\tTime: 0:00:22.089808\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 412.1858246806943\tTime: 0:00:21.939633\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 412.0106549485044\tTime: 0:00:21.738610\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 412.1554474747047\tTime: 0:00:21.968392\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 411.9621528218564\tTime: 0:00:21.996158\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 411.8337140994085\tTime: 0:00:22.001516\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 411.86713897675236\tTime: 0:00:22.035755\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 411.7087013165999\tTime: 0:00:22.149403\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 411.71402730319767\tTime: 0:00:22.271506\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 411.67495888931205\tTime: 0:00:21.661385\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 411.7583949993653\tTime: 0:00:22.656745\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 411.6897243923611\tTime: 0:00:22.450816\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 411.52394451467643\tTime: 0:00:22.100221\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 411.6289506592403\tTime: 0:00:22.456410\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 411.6582610516931\tTime: 0:00:23.176153\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 411.5109587238702\tTime: 0:00:22.044621\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 411.3830368603306\tTime: 0:00:22.200302\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 411.51125092859024\tTime: 0:00:22.440676\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 411.5546358912468\tTime: 0:00:22.134535\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 411.54518774272645\tTime: 0:00:21.914785\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 411.513804772131\tTime: 0:00:22.295564\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 411.44888940901694\tTime: 0:00:22.482948\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 411.31178618298867\tTime: 0:00:25.756294\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 411.62761430475626\tTime: 0:00:21.790709\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 411.393299986365\tTime: 0:00:23.011734\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 411.4484770899745\tTime: 0:00:22.286648\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 411.2179092732077\tTime: 0:00:21.902752\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 411.27212142169515\tTime: 0:00:22.237058\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 411.1592901045193\tTime: 0:00:21.850057\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 411.3626340652152\tTime: 0:00:22.314116\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 411.1238664807723\tTime: 0:00:22.179017\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 411.3155104248947\tTime: 0:00:22.959974\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 411.35835990541307\tTime: 0:00:22.599090\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 411.2483189137375\tTime: 0:00:22.579432\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 411.15529128935344\tTime: 0:00:21.569183\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 411.18867319001305\tTime: 0:00:21.734993\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 410.98718508742854\tTime: 0:00:22.325536\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 411.32020149526534\tTime: 0:00:22.688076\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 411.12558837651864\tTime: 0:00:22.230945\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 411.1802184273443\tTime: 0:00:22.889253\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 411.1428126968846\tTime: 0:00:22.161868\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 410.98561405086326\tTime: 0:00:22.147353\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 411.0776565908749\tTime: 0:00:22.762143\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 411.01276877691737\tTime: 0:00:22.363895\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 410.97283219727956\tTime: 0:00:22.649176\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 410.9083101993883\tTime: 0:00:22.455744\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 411.11883339680656\tTime: 0:00:22.740228\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 410.9739967184914\tTime: 0:00:23.162811\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 410.84941616358236\tTime: 0:00:22.533023\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 411.2587388863029\tTime: 0:00:22.549307\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 410.7338687934322\tTime: 0:00:22.564442\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 410.9865676557328\tTime: 0:00:22.654006\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 410.8537936878197\tTime: 0:00:22.600361\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 411.01153013010844\tTime: 0:00:23.236520\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 410.87898158776\tTime: 0:00:22.230280\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 410.8959058794455\tTime: 0:00:22.792993\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 410.8833997745524\tTime: 0:00:22.675139\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 411.009254459584\tTime: 0:00:22.928439\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 410.8995896791897\tTime: 0:00:23.006547\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 410.87944977649187\tTime: 0:00:22.559283\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 410.8286450178783\tTime: 0:00:21.978681\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 410.9294345252788\tTime: 0:00:22.247987\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 410.8700648441026\tTime: 0:00:22.769777\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 410.69277614099053\tTime: 0:00:23.297562\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 410.7481282368422\tTime: 0:00:22.493884\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 410.97704115741135\tTime: 0:00:22.569618\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 410.7407222154635\tTime: 0:00:23.116894\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 410.88986280666984\tTime: 0:00:22.580748\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 410.71556403188004\tTime: 0:00:22.850215\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 410.80295278471283\tTime: 0:00:21.670048\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 410.8574476977427\tTime: 0:00:21.095598\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 410.5459221665068\tTime: 0:00:20.859836\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 410.7755155879739\tTime: 0:00:21.210324\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 410.589218347415\tTime: 0:00:20.859556\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 410.87211857862223\tTime: 0:00:21.126463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 20:48:43,969] Trial 40 finished with values: [-0.006879098628452791, 0.882] and parameters: {'num_topics': 50, 'dropout': 0.4874449971620841, 'num_neurons': 300, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 431.15369964601905\tTime: 0:00:15.405063\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 418.54631163811547\tTime: 0:00:14.143606\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 415.18126562588157\tTime: 0:00:14.107422\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 413.30262865970343\tTime: 0:00:14.119276\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 412.57709186989865\tTime: 0:00:14.108395\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 412.0391987030298\tTime: 0:00:13.987583\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 411.53290706339806\tTime: 0:00:14.191577\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 411.30261896240035\tTime: 0:00:13.769925\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 410.93285631417854\tTime: 0:00:14.801905\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 410.8928815697759\tTime: 0:00:14.159595\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 410.6661592479359\tTime: 0:00:13.944466\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 410.5113261194214\tTime: 0:00:13.855606\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 410.45074313666026\tTime: 0:00:22.296618\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 410.3102739179573\tTime: 0:00:17.277061\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 409.93075152188885\tTime: 0:00:14.696920\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 410.17019839506696\tTime: 0:00:15.541899\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 410.00514026855063\tTime: 0:00:19.850551\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 409.8888509077851\tTime: 0:00:13.887177\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 409.70799117280944\tTime: 0:00:14.668871\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 409.7951342238984\tTime: 0:00:14.702743\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 409.6042979843127\tTime: 0:00:16.092866\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 409.6710098095567\tTime: 0:00:17.361221\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 409.6196716258087\tTime: 0:00:17.610931\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 409.4603609145203\tTime: 0:00:17.812799\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 409.6025720112912\tTime: 0:00:24.585838\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 409.30053903781595\tTime: 0:00:17.768491\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 409.36303720120554\tTime: 0:00:17.861956\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 409.2217422424514\tTime: 0:00:17.880360\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 409.1867459612202\tTime: 0:00:17.780051\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 409.0589182790637\tTime: 0:00:18.043574\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 408.9789866421119\tTime: 0:00:18.116922\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 409.1765754370839\tTime: 0:00:24.572266\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 409.15871491486354\tTime: 0:00:18.076053\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 408.9669017087236\tTime: 0:00:18.031836\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 408.929347249551\tTime: 0:00:18.957920\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 408.67369276681103\tTime: 0:00:18.055842\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 408.80747569503217\tTime: 0:00:24.948368\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 408.8805505673216\tTime: 0:00:17.733797\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 408.91835072823807\tTime: 0:00:18.002749\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 408.74854713095004\tTime: 0:00:18.050389\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 408.8486051898791\tTime: 0:00:18.158391\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 408.655573025394\tTime: 0:00:18.185745\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 408.72551162824186\tTime: 0:00:24.882568\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 408.66818642506865\tTime: 0:00:18.223848\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 408.6344154201224\tTime: 0:00:18.178073\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 408.6389011941788\tTime: 0:00:18.037125\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 408.54862172671943\tTime: 0:00:25.326776\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 408.7248708715936\tTime: 0:00:18.181717\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 408.53409729715594\tTime: 0:00:18.928043\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 408.5813094253672\tTime: 0:00:18.456266\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 408.47438817363934\tTime: 0:00:18.795593\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 408.6246425588773\tTime: 0:00:25.513550\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 408.45215931023375\tTime: 0:00:18.149954\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 408.4911013285893\tTime: 0:00:18.498602\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 408.4774819439155\tTime: 0:00:18.408888\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 408.36012848779666\tTime: 0:00:18.575883\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 408.3481328136754\tTime: 0:00:18.472616\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 408.3721002859823\tTime: 0:00:18.410214\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 408.216954235782\tTime: 0:00:25.368962\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 408.2677490766992\tTime: 0:00:18.517735\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 408.32810700121775\tTime: 0:00:19.124880\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 408.26028810393626\tTime: 0:00:18.794124\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 408.1642226524298\tTime: 0:00:18.506495\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 408.24289062793855\tTime: 0:00:25.192604\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 408.10102598936\tTime: 0:00:18.367679\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 408.1723662565589\tTime: 0:00:18.576986\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 408.27339922503853\tTime: 0:00:18.628032\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 408.1483814466496\tTime: 0:00:18.696126\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 408.09589604256\tTime: 0:00:18.587660\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 408.0602154770133\tTime: 0:00:18.775453\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 407.98597347552516\tTime: 0:00:25.543229\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 408.02253094908974\tTime: 0:00:18.698500\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 408.03887608404096\tTime: 0:00:18.785409\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 408.1199916000785\tTime: 0:00:18.468872\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 408.0914742927434\tTime: 0:00:18.886958\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 407.9324449501735\tTime: 0:00:25.165092\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 408.0688226048837\tTime: 0:00:18.519677\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 408.0189410736031\tTime: 0:00:18.419264\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 408.0397034770415\tTime: 0:00:18.341839\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 407.9604923056308\tTime: 0:00:18.566930\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 407.9380028272046\tTime: 0:00:20.677799\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 407.9803808498246\tTime: 0:00:24.161505\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 407.7834773309378\tTime: 0:00:18.624880\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 407.8719785003444\tTime: 0:00:18.696094\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 407.8431979516357\tTime: 0:00:18.609498\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 407.82233027368727\tTime: 0:00:19.085994\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 407.93741852796114\tTime: 0:00:18.701049\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 407.85750455920174\tTime: 0:00:25.856402\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 407.95096977438783\tTime: 0:00:18.588402\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 407.9795462573112\tTime: 0:00:18.644500\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 407.7761659684608\tTime: 0:00:18.347714\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 407.7573811904057\tTime: 0:00:18.502612\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 407.7200591726496\tTime: 0:00:18.427356\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 407.7097722808762\tTime: 0:00:18.447941\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 407.7501576913719\tTime: 0:00:18.720283\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 407.7751245368803\tTime: 0:00:18.563261\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 407.74771242823994\tTime: 0:00:26.066872\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 407.81994161488893\tTime: 0:00:18.750772\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 407.6883611866325\tTime: 0:00:18.472451\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 407.8224626565674\tTime: 0:00:18.913116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 21:20:41,777] Trial 41 finished with values: [0.001625821363162621, 0.8794871794871795] and parameters: {'num_topics': 39, 'dropout': 0.4148372733132932, 'num_neurons': 100, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 419.84130866721443\tTime: 0:00:14.417639\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 408.6134832404155\tTime: 0:00:13.359699\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 404.8535432476209\tTime: 0:00:13.574250\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 403.07551179720997\tTime: 0:00:13.344664\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 401.8472721045616\tTime: 0:00:14.130000\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 400.96962319806477\tTime: 0:00:13.318654\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 400.18952159383696\tTime: 0:00:13.238433\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 399.6847508513057\tTime: 0:00:13.142481\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 399.3624333236948\tTime: 0:00:13.375597\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 399.08277669344295\tTime: 0:00:17.259589\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 398.7699475728532\tTime: 0:00:16.558326\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 398.31125911252633\tTime: 0:00:13.520252\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 398.33660995654435\tTime: 0:00:13.748377\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 398.14634240501346\tTime: 0:00:13.135753\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 397.912470562808\tTime: 0:00:13.134690\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 397.7736443023349\tTime: 0:00:13.246914\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 397.60848788042813\tTime: 0:00:13.070205\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 397.5892552265525\tTime: 0:00:13.569016\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 397.41512670554994\tTime: 0:00:13.250407\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 397.32578919885\tTime: 0:00:21.056320\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 397.29737246430216\tTime: 0:00:16.528361\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 397.1186200928708\tTime: 0:00:18.629421\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 397.044683189108\tTime: 0:00:18.287974\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 396.94571728893965\tTime: 0:00:18.864787\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 396.8598568898457\tTime: 0:00:18.626132\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 396.8892748342055\tTime: 0:00:18.988972\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 396.701236303294\tTime: 0:00:19.115162\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 396.68592110274204\tTime: 0:00:18.749274\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 396.67110931711005\tTime: 0:00:19.291450\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 396.6155118868366\tTime: 0:00:18.904451\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 396.57573384341146\tTime: 0:00:19.935472\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 396.4813105052941\tTime: 0:00:18.792142\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 396.44515539634347\tTime: 0:00:18.754087\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 396.5515673927302\tTime: 0:00:18.793573\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 396.346530775126\tTime: 0:00:21.951330\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 396.3712782926164\tTime: 0:00:18.913233\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 396.377738533086\tTime: 0:00:19.011127\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 396.2647369621229\tTime: 0:00:21.093137\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 396.276344486995\tTime: 0:00:19.289474\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 396.17876629293846\tTime: 0:00:19.254776\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 396.2945529734282\tTime: 0:00:19.236266\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 396.2407758812204\tTime: 0:00:20.352698\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 396.10501407872096\tTime: 0:00:19.787658\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 396.10861052927294\tTime: 0:00:19.332372\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 395.92613351188123\tTime: 0:00:18.807521\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 396.0883167186207\tTime: 0:00:19.773847\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 396.0709132185408\tTime: 0:00:18.611681\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 395.95072642350533\tTime: 0:00:19.654054\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 395.9968134808969\tTime: 0:00:19.132252\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 395.94247328393\tTime: 0:00:19.458192\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 395.898005896548\tTime: 0:00:19.657151\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 395.8691787530326\tTime: 0:00:19.227181\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 395.84102825353335\tTime: 0:00:25.304540\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 395.743360506352\tTime: 0:00:48.833908\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 395.87896347878115\tTime: 0:00:18.110236\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 395.7454738572463\tTime: 0:00:18.321623\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 395.8286662123627\tTime: 0:00:18.264935\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 395.7122719811414\tTime: 0:00:19.984712\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 395.68152785711936\tTime: 0:00:19.491267\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 395.8214118215414\tTime: 0:00:19.815973\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 395.7238291461554\tTime: 0:00:19.881781\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 395.60982842238394\tTime: 0:00:19.493099\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 395.57262413135675\tTime: 0:00:20.141290\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 395.56837487599205\tTime: 0:00:19.857107\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 395.68198862594505\tTime: 0:00:19.764802\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 395.6795018554173\tTime: 0:00:18.336976\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 395.70106726166966\tTime: 0:00:19.645312\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 395.6551688976341\tTime: 0:00:19.494027\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 395.5244659283787\tTime: 0:00:19.340969\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 395.6000755801926\tTime: 0:00:20.777452\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 395.6187945341298\tTime: 0:00:20.326811\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 395.4928319298691\tTime: 0:00:18.475125\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 395.5982751877163\tTime: 0:00:20.390053\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 395.5348934677791\tTime: 0:00:19.913784\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 395.509123325599\tTime: 0:00:20.486987\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 395.5404036664034\tTime: 0:00:19.760542\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 395.55737512224476\tTime: 0:00:21.130116\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 395.34979784796275\tTime: 0:00:20.687253\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 395.48098829035956\tTime: 0:00:19.583430\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 395.5093224141701\tTime: 0:00:20.237867\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 395.53326931644006\tTime: 0:00:19.849145\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 395.3918230209861\tTime: 0:00:18.935874\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 395.43426264352007\tTime: 0:00:19.835531\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 395.3856050294328\tTime: 0:00:21.221426\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 395.4857763704934\tTime: 0:00:19.942270\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 395.43035249990595\tTime: 0:00:20.546144\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 395.3543497032625\tTime: 0:00:20.383377\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 395.3734819312796\tTime: 0:00:19.851517\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 395.33919535305233\tTime: 0:00:19.634268\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 395.3746800257302\tTime: 0:00:20.349816\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 395.337280356085\tTime: 0:00:20.378538\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 395.40061458127633\tTime: 0:00:19.611303\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 395.5270129397286\tTime: 0:00:20.866528\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 395.29795231894843\tTime: 0:00:20.645756\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 395.20430274480697\tTime: 0:00:20.145898\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 395.29899900323477\tTime: 0:00:20.228375\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 395.11063109460804\tTime: 0:00:21.408313\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 395.34951390798966\tTime: 0:00:19.828056\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 395.32297121399654\tTime: 0:00:19.475410\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 395.03981400425744\tTime: 0:00:20.197945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 21:53:04,482] Trial 42 finished with values: [0.04600972296174738, 0.9541666666666667] and parameters: {'num_topics': 24, 'dropout': 0.055163514424525585, 'num_neurons': 100, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 427.0364686918162\tTime: 0:00:23.344927\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 415.37001363940374\tTime: 0:00:20.894873\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 411.84132321316906\tTime: 0:00:20.594249\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 409.8761966251622\tTime: 0:00:20.411951\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 408.2891679581711\tTime: 0:00:19.924293\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 407.3806401204464\tTime: 0:00:20.202483\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 406.7173503191882\tTime: 0:00:20.183627\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 406.02690450624857\tTime: 0:00:19.810897\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 405.6098099828152\tTime: 0:00:20.089123\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 405.29137442521437\tTime: 0:00:20.104087\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 405.14669694429165\tTime: 0:00:19.822660\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 404.83832369774035\tTime: 0:00:19.707075\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 404.8024207554023\tTime: 0:00:19.247924\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 404.589484141677\tTime: 0:00:20.089697\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 404.3279630109597\tTime: 0:00:19.226049\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 404.0652943660432\tTime: 0:00:19.278519\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 404.08879684532036\tTime: 0:00:19.502490\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 403.95611713225475\tTime: 0:00:19.454826\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 403.79871653255003\tTime: 0:00:18.908716\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 403.784068792962\tTime: 0:00:19.643870\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 403.48923397329656\tTime: 0:00:20.668847\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 403.52052779482153\tTime: 0:00:21.964344\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 403.4591290264378\tTime: 0:00:23.469185\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 403.57207079736514\tTime: 0:00:24.843987\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 403.3625363208079\tTime: 0:00:24.240573\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 403.48472006236835\tTime: 0:00:25.335545\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 403.30463104259525\tTime: 0:00:25.676316\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 403.2329259143675\tTime: 0:00:25.262466\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 403.13780260728157\tTime: 0:00:25.703866\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 402.9647651063412\tTime: 0:00:25.948530\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 402.9027283729571\tTime: 0:00:26.475398\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 402.94063516747536\tTime: 0:00:26.285510\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 403.12504760494244\tTime: 0:00:26.702582\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 403.02199994828106\tTime: 0:00:26.814731\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 402.8086672511496\tTime: 0:00:26.371477\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 402.86196763275314\tTime: 0:00:26.730213\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 402.7440057075975\tTime: 0:00:27.219504\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 402.9138479839013\tTime: 0:00:27.944237\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 402.7051088698587\tTime: 0:00:27.683859\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 402.4780001619156\tTime: 0:00:28.007279\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 402.7192725891329\tTime: 0:00:28.261175\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 402.51846704445006\tTime: 0:00:27.911453\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 402.4392455880209\tTime: 0:00:28.616020\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 402.6050697647493\tTime: 0:00:27.923030\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 402.48430976359737\tTime: 0:00:28.880920\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 402.47152302462996\tTime: 0:00:27.695909\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 402.3791198477935\tTime: 0:00:28.116466\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 402.4605833647761\tTime: 0:00:28.204062\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 402.59121654283507\tTime: 0:00:28.051956\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 402.3444540462438\tTime: 0:00:26.648585\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 402.2578357882203\tTime: 0:00:28.314068\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 402.2816650739581\tTime: 0:00:29.444502\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 402.2844394209476\tTime: 0:00:28.951571\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 402.20152494498984\tTime: 0:00:28.682240\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 402.26676178839426\tTime: 0:00:28.403956\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 402.3648779991114\tTime: 0:00:28.273052\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 402.2702250479576\tTime: 0:00:28.689005\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 401.92443681443007\tTime: 0:00:28.589123\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 402.15718604687385\tTime: 0:00:28.412551\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 402.03540661673435\tTime: 0:00:28.691839\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 402.2849111727037\tTime: 0:00:28.683717\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 401.9974105262168\tTime: 0:00:28.742209\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 402.0256364369405\tTime: 0:00:28.399616\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 402.1225641035813\tTime: 0:00:28.619641\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 402.0232100174316\tTime: 0:00:28.969933\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 401.947577701845\tTime: 0:00:27.005943\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 402.0955439276076\tTime: 0:00:25.902572\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 402.0365528085741\tTime: 0:00:25.296241\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 402.0516888294121\tTime: 0:00:25.673452\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 401.88105295373975\tTime: 0:00:25.676711\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 401.793147415463\tTime: 0:00:25.357074\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 401.98615416125966\tTime: 0:00:25.472029\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 401.7621478556618\tTime: 0:00:26.585573\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 401.76461611315636\tTime: 0:00:28.063760\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 401.87514384332917\tTime: 0:00:29.431428\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 401.96693796341356\tTime: 0:00:29.714082\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 401.88554780065164\tTime: 0:00:29.042026\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 401.78253841895173\tTime: 0:00:28.793355\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 401.8416089530204\tTime: 0:00:30.889498\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 401.89071995275947\tTime: 0:00:29.337864\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 401.7516200005995\tTime: 0:00:30.986200\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 401.8180355438483\tTime: 0:00:29.710528\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 401.65670562631647\tTime: 0:00:30.456408\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 401.6770234598332\tTime: 0:00:30.117845\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 401.67346789222944\tTime: 0:00:29.971193\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 401.81973319633687\tTime: 0:00:29.645694\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 401.6736144170099\tTime: 0:00:30.132481\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 401.62755975742635\tTime: 0:00:29.835605\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 401.685755991464\tTime: 0:00:29.667312\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 401.52724945842033\tTime: 0:00:28.813599\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 401.6808338020363\tTime: 0:00:29.351173\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 401.5834724749398\tTime: 0:00:29.135232\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 401.52504416680773\tTime: 0:00:30.227071\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 401.586307319877\tTime: 0:00:30.682909\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 401.4967798341938\tTime: 0:00:30.538107\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 401.4710361024717\tTime: 0:00:29.779123\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 401.46145678323415\tTime: 0:00:32.947640\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 401.54149222540764\tTime: 0:00:30.655674\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 401.53665698111786\tTime: 0:00:28.893633\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 401.5439596380614\tTime: 0:00:31.602932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 22:38:14,846] Trial 43 finished with values: [0.03137382277301838, 0.9333333333333333] and parameters: {'num_topics': 36, 'dropout': 0.2789086532949821, 'num_neurons': 200, 'num_layers': 1, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 426.19904081918116\tTime: 0:00:23.429932\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 412.61738864557475\tTime: 0:00:22.431431\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 408.60326830703895\tTime: 0:00:22.290791\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 406.9976744471362\tTime: 0:00:21.974514\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 405.9218002499553\tTime: 0:00:22.483143\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 405.24101350187834\tTime: 0:00:22.432049\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 404.7271502520711\tTime: 0:00:22.324316\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 404.3138760252694\tTime: 0:00:22.256088\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 403.7752245954168\tTime: 0:00:22.687158\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 403.6631147722838\tTime: 0:00:21.883325\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 403.3510298829741\tTime: 0:00:22.103533\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 403.15001900524476\tTime: 0:00:21.176743\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 402.89869076857883\tTime: 0:00:21.900402\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 402.6890376102554\tTime: 0:00:22.000869\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 402.6648985250696\tTime: 0:00:22.657537\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 402.65599206643066\tTime: 0:00:22.222636\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 402.2877926675459\tTime: 0:00:21.993364\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 402.3532003524529\tTime: 0:00:22.036827\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 402.1112221893102\tTime: 0:00:21.326143\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 402.21133259163656\tTime: 0:00:21.482644\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 402.04288279663166\tTime: 0:00:23.071806\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 401.96262203909015\tTime: 0:00:22.052994\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 401.97149209611024\tTime: 0:00:22.740279\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 401.7445556622259\tTime: 0:00:22.977900\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 401.8446397642909\tTime: 0:00:23.834975\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 401.73680773744877\tTime: 0:00:21.706520\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 401.6557853375602\tTime: 0:00:21.653524\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 401.6876544222053\tTime: 0:00:22.406323\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 401.58477588063266\tTime: 0:00:22.240778\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 401.41026674048373\tTime: 0:00:21.907863\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 401.4179704396816\tTime: 0:00:22.093091\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 401.4819425564089\tTime: 0:00:22.516525\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 401.2106462503174\tTime: 0:00:22.709533\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 401.4893692947534\tTime: 0:00:23.083279\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 401.25487160917635\tTime: 0:00:52.874448\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 401.1731244093461\tTime: 0:00:32.994586\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 401.1599632589756\tTime: 0:00:21.597139\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 401.2169871111088\tTime: 0:00:21.286972\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 401.1167350326535\tTime: 0:00:21.203502\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 401.27481752908017\tTime: 0:00:22.097587\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 401.0966106676799\tTime: 0:00:21.314125\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 401.12764560059804\tTime: 0:00:21.415440\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 401.138261392568\tTime: 0:00:21.311324\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 401.1812767190086\tTime: 0:00:21.278986\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 401.19391785150077\tTime: 0:00:21.193063\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 400.8782009181289\tTime: 0:00:21.142950\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 400.87520004360846\tTime: 0:00:21.467795\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 400.9959062320747\tTime: 0:00:21.467992\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 400.6944420568509\tTime: 0:00:21.327884\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 400.94344679765663\tTime: 0:00:22.005260\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 400.95599191215535\tTime: 0:00:21.892927\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 400.96078410629656\tTime: 0:00:23.037461\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 400.58895108386463\tTime: 0:00:22.125358\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 400.75358649612343\tTime: 0:00:23.082578\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 400.78252975015045\tTime: 0:00:22.505381\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 400.8366552473576\tTime: 0:00:22.729405\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 400.66535459217255\tTime: 0:00:22.920394\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 400.6296540075721\tTime: 0:00:22.938225\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 400.73722012995563\tTime: 0:00:21.635670\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 400.77068871862775\tTime: 0:00:22.668780\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 400.7815276220332\tTime: 0:00:22.151200\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 400.6982316086239\tTime: 0:00:22.228895\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 400.52514036846225\tTime: 0:00:22.846893\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 400.5437317514387\tTime: 0:00:20.963065\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 400.5621810174998\tTime: 0:00:21.560660\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 400.51773651428385\tTime: 0:00:21.571229\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 400.69181893636454\tTime: 0:00:22.119364\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 400.40960133652345\tTime: 0:00:22.017062\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 400.6675546310793\tTime: 0:00:21.739699\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 400.5015824602822\tTime: 0:00:21.729001\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 400.4111805276391\tTime: 0:00:21.307864\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 400.36081424140053\tTime: 0:00:22.225974\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 400.31844679765663\tTime: 0:00:21.794426\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 400.46882533776\tTime: 0:00:21.605146\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 400.292820535538\tTime: 0:00:22.017615\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 400.4805790200905\tTime: 0:00:22.124712\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 400.37574316604605\tTime: 0:00:21.334161\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 400.40012835703004\tTime: 0:00:21.533359\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 400.23173156660226\tTime: 0:00:21.367994\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 400.1398993919497\tTime: 0:00:21.413613\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 400.1857565791794\tTime: 0:00:21.638363\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 400.2029274541817\tTime: 0:00:21.431062\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 400.4680676257593\tTime: 0:00:21.372855\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 400.1895471594542\tTime: 0:00:21.572466\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 400.27518984674737\tTime: 0:00:22.164368\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 400.3843808183818\tTime: 0:00:21.356830\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 400.18037534733975\tTime: 0:00:21.468694\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 400.3404500518365\tTime: 0:00:21.647717\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 400.1084835460275\tTime: 0:00:21.972926\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 400.16453436195275\tTime: 0:00:21.496683\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 400.13976212368584\tTime: 0:00:21.476237\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 400.13842812006317\tTime: 0:00:21.319067\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 400.1454859201036\tTime: 0:00:21.668504\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 400.159309058339\tTime: 0:00:21.456030\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 400.2024746195131\tTime: 0:00:22.345427\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 400.2504704661288\tTime: 0:00:21.722009\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 400.03670594515205\tTime: 0:00:22.132288\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 400.0286710316285\tTime: 0:00:21.641302\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 400.15965720621284\tTime: 0:00:33.899833\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 400.1065815155298\tTime: 0:00:21.813013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 23:17:02,873] Trial 44 finished with values: [0.025556015483082305, 0.9629629629629629] and parameters: {'num_topics': 27, 'dropout': 0.28794102864508697, 'num_neurons': 300, 'num_layers': 1, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 432.06360545582027\tTime: 0:00:15.145349\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 419.0899762430767\tTime: 0:00:13.350322\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 417.8855930180005\tTime: 0:00:13.672976\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 416.8136889481306\tTime: 0:00:13.367157\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 414.74957067394496\tTime: 0:00:13.407197\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 413.3945298909083\tTime: 0:00:13.775104\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 412.4867452559618\tTime: 0:00:13.334641\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 411.78316690704224\tTime: 0:00:13.501343\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 411.1246759851872\tTime: 0:00:13.374298\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 410.82700025977016\tTime: 0:00:13.583158\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 410.4543637937378\tTime: 0:00:13.365765\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 410.2094452172901\tTime: 0:00:13.478333\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 409.9527949978372\tTime: 0:00:13.476604\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 409.7006079621323\tTime: 0:00:16.551935\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 409.71716305838834\tTime: 0:00:13.309969\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 409.48278717681535\tTime: 0:00:13.568420\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 409.25177346776735\tTime: 0:00:13.394477\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 409.31036971555756\tTime: 0:00:13.385377\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 409.1925475931881\tTime: 0:00:13.819340\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 408.8726242342069\tTime: 0:00:14.999974\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 408.8707624254777\tTime: 0:00:14.445337\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 408.8230863694708\tTime: 0:00:14.275410\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 408.7268551087744\tTime: 0:00:14.600275\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 408.86453550799763\tTime: 0:00:13.843777\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 408.6333103316243\tTime: 0:00:13.513207\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 408.7712179930226\tTime: 0:00:13.460023\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 408.52327003786064\tTime: 0:00:13.367973\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 408.40777464378573\tTime: 0:00:13.846920\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 408.35644351262175\tTime: 0:00:14.454731\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 408.4530937776226\tTime: 0:00:13.773650\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 408.47808751110784\tTime: 0:00:14.286161\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 408.42518560050405\tTime: 0:00:15.270060\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 408.4518521187726\tTime: 0:00:13.816137\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 408.20150547691924\tTime: 0:00:13.552729\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 408.3921850903671\tTime: 0:00:13.763809\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 408.1974107906887\tTime: 0:00:13.876877\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 408.37167632282967\tTime: 0:00:14.395978\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 408.1259649551221\tTime: 0:00:16.245480\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 408.3292266724028\tTime: 0:00:15.809334\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 408.3599126860707\tTime: 0:00:13.500027\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 407.88697157779234\tTime: 0:00:13.850525\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 408.1726121052316\tTime: 0:00:14.092588\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 408.11050455214917\tTime: 0:00:14.392644\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 408.23552703226085\tTime: 0:00:13.919961\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 407.85530007569776\tTime: 0:00:13.894156\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 408.00699583280436\tTime: 0:00:14.651061\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 407.95019590021536\tTime: 0:00:13.973207\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 408.0382013868319\tTime: 0:00:12.823606\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 408.0817457305419\tTime: 0:00:12.932345\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 408.13872939763877\tTime: 0:00:12.986067\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 408.07806251851304\tTime: 0:00:13.254492\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 407.91009898448664\tTime: 0:00:13.528803\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 407.85640828643363\tTime: 0:00:14.278444\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 407.7389591796434\tTime: 0:00:14.163592\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 407.885991084946\tTime: 0:00:13.877073\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 407.9740276470111\tTime: 0:00:14.267702\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 407.9929179199696\tTime: 0:00:13.699615\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 408.05128459145556\tTime: 0:00:15.250859\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 407.7270778528878\tTime: 0:00:13.613213\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 407.71734190751243\tTime: 0:00:14.174754\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 407.65961782928514\tTime: 0:00:13.916479\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 407.6930336705056\tTime: 0:00:13.956467\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 407.8631589684773\tTime: 0:00:13.908863\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 407.86575824976023\tTime: 0:00:14.240376\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 407.848209657456\tTime: 0:00:14.158339\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 407.6932852494029\tTime: 0:00:14.044333\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 407.70190365406137\tTime: 0:00:14.242228\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 407.747028952033\tTime: 0:00:14.307933\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 407.63857857518997\tTime: 0:00:14.311399\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 407.5855680841773\tTime: 0:00:14.531627\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 407.58949729180773\tTime: 0:00:14.413765\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 407.68371323987486\tTime: 0:00:14.402313\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 407.57624721276\tTime: 0:00:13.854651\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 407.59235057631366\tTime: 0:00:14.465914\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 407.44750570671596\tTime: 0:00:13.791237\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 407.74472095716493\tTime: 0:00:14.223085\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 407.56994503098434\tTime: 0:00:14.248606\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 407.4638055508538\tTime: 0:00:14.109328\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 407.62014488358534\tTime: 0:00:13.978612\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 407.5671872869532\tTime: 0:00:13.667246\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 407.79606777298204\tTime: 0:00:13.959220\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 407.49390675102734\tTime: 0:00:13.912498\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 407.58343104100845\tTime: 0:00:13.865338\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 407.6017963005083\tTime: 0:00:13.991826\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 407.4096022915609\tTime: 0:00:14.697855\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 407.45038040609955\tTime: 0:00:14.311718\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 407.3728618548649\tTime: 0:00:14.315865\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 407.44257036715754\tTime: 0:00:13.999298\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 407.43804797108913\tTime: 0:00:14.134990\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 407.5180095815232\tTime: 0:00:13.984736\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 407.4339308414791\tTime: 0:00:14.456604\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 407.28222384431655\tTime: 0:00:14.123234\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 407.5044388302937\tTime: 0:00:14.474562\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 407.36255156467456\tTime: 0:00:14.055986\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 407.49980737629767\tTime: 0:00:14.286796\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 407.38201651303785\tTime: 0:00:14.588057\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 407.50471586461157\tTime: 0:00:14.947309\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 407.40690067434457\tTime: 0:00:13.834042\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 407.4813129296199\tTime: 0:00:14.396779\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 407.3378050389538\tTime: 0:00:14.670012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-14 23:41:25,436] Trial 45 finished with values: [-0.012669106096583975, 0.8692307692307693] and parameters: {'num_topics': 26, 'dropout': 0.5018796111645731, 'num_neurons': 100, 'num_layers': 2, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 419.4412927842074\tTime: 0:00:25.983834\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 413.0467860713228\tTime: 0:00:23.451568\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 411.32487346488756\tTime: 0:00:21.912613\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 410.0354464344486\tTime: 0:00:21.898844\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 409.02599160066626\tTime: 0:00:21.361639\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 408.5860183843235\tTime: 0:00:22.640198\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 408.1988438610782\tTime: 0:00:21.037640\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 407.83094989050863\tTime: 0:00:21.124682\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 407.6624172643732\tTime: 0:00:20.792585\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 407.6960659510645\tTime: 0:00:21.084926\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 407.34225709284254\tTime: 0:00:20.609276\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 407.24773861830477\tTime: 0:00:20.984444\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 407.07994258314994\tTime: 0:00:20.811018\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 407.1904437192037\tTime: 0:00:20.809478\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 407.24819688934025\tTime: 0:00:21.377725\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 407.07279923379554\tTime: 0:00:32.909561\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 407.2295070640445\tTime: 0:00:27.588625\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 406.9700954273396\tTime: 0:00:21.078598\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 406.83215378864816\tTime: 0:00:21.247877\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 406.8636972789955\tTime: 0:00:22.295551\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 406.94040963171403\tTime: 0:00:30.911126\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 406.81170063367466\tTime: 0:00:26.176720\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 406.5308289754241\tTime: 0:00:27.439724\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 406.6742177802697\tTime: 0:00:28.217191\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 406.7845217378272\tTime: 0:00:35.690962\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 406.8519802186771\tTime: 0:00:29.231150\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 406.81382287376346\tTime: 0:00:29.447711\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 406.52681260226245\tTime: 0:00:37.464970\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 406.6187178372781\tTime: 0:00:30.409863\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 406.6602110544554\tTime: 0:00:30.577325\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 406.65231638450456\tTime: 0:00:37.649058\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 406.51511287954656\tTime: 0:00:30.771836\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 406.57501653684034\tTime: 0:00:30.913862\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 406.4081211387102\tTime: 0:00:31.233065\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 406.45367572600475\tTime: 0:00:37.888413\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 406.56225562061564\tTime: 0:00:31.517491\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 406.40508051994\tTime: 0:00:31.415116\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 406.5086596181966\tTime: 0:00:38.432065\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 406.4007270920315\tTime: 0:00:31.960042\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 406.4925786612902\tTime: 0:00:31.365970\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 406.5365178027792\tTime: 0:00:39.635460\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 406.36530398253547\tTime: 0:00:33.298271\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 406.4399613518393\tTime: 0:00:31.797515\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 406.25667325054775\tTime: 0:00:38.600277\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 406.32481006509533\tTime: 0:00:32.041682\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 406.228696751521\tTime: 0:00:32.337499\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 406.43449941933727\tTime: 0:00:39.828056\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 406.4361925537642\tTime: 0:00:32.310175\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 406.2548966972749\tTime: 0:00:35.866391\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 406.4073674672525\tTime: 0:00:39.190779\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 406.5031185277848\tTime: 0:00:32.412600\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 406.5218169851496\tTime: 0:00:32.549485\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 406.3125974505496\tTime: 0:00:32.790638\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 406.38049392625584\tTime: 0:00:39.061026\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 406.47761465738546\tTime: 0:00:32.705180\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 406.2755601808753\tTime: 0:00:32.770904\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 406.2551307916408\tTime: 0:00:36.721399\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 406.31154771748993\tTime: 0:00:34.547412\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 406.4361793669013\tTime: 0:00:32.542043\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 406.3867801662529\tTime: 0:00:32.904664\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 406.24599078961904\tTime: 0:00:40.178934\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 406.20970594250736\tTime: 0:00:32.828494\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 406.4843458713585\tTime: 0:00:33.082506\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 406.2043312857331\tTime: 0:00:40.209941\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 406.2476625091684\tTime: 0:00:32.762313\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 406.1101607019437\tTime: 0:00:39.472045\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 406.15289050892625\tTime: 0:00:33.051852\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 406.15758782377236\tTime: 0:00:33.770453\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 406.36708968212827\tTime: 0:00:39.970469\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 406.31458881377887\tTime: 0:00:33.050737\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 406.21482409826837\tTime: 0:00:32.892310\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 406.04425580986\tTime: 0:00:33.125603\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 406.09599286866205\tTime: 0:00:39.651709\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 406.1353913581749\tTime: 0:00:33.019380\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 406.0697378613274\tTime: 0:00:32.911075\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 405.9656881500132\tTime: 0:00:40.034863\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 406.07531226785244\tTime: 0:00:33.217221\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 405.9695948775319\tTime: 0:00:33.321006\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 406.23287393829224\tTime: 0:00:33.171074\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 406.1471029453942\tTime: 0:00:40.107671\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 406.22098559568474\tTime: 0:00:34.229998\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 406.0652589194618\tTime: 0:00:40.268388\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 406.24285312435353\tTime: 0:00:33.541033\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 406.16104950385073\tTime: 0:00:33.245081\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 406.1378376865409\tTime: 0:00:33.337698\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 406.0080650706669\tTime: 0:00:39.986364\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 406.14717145096336\tTime: 0:00:33.380367\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 405.94673058364833\tTime: 0:00:34.104412\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 406.2148818780326\tTime: 0:00:40.537890\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 406.1667261361125\tTime: 0:00:33.900802\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 406.25304168400424\tTime: 0:00:33.383714\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 406.2417326083277\tTime: 0:00:39.934437\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 406.0994654582064\tTime: 0:00:33.998086\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 405.92774135411963\tTime: 0:00:33.813067\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 406.0271102800816\tTime: 0:00:33.609132\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 406.12107266571223\tTime: 0:00:40.998483\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 406.15611666208247\tTime: 0:00:33.978638\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 406.05307572741526\tTime: 0:00:48.532592\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 406.20634090487005\tTime: 0:00:34.590336\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 406.0837581413267\tTime: 0:00:41.678696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-15 00:36:11,387] Trial 46 finished with values: [-0.022292517757675455, 0.95] and parameters: {'num_topics': 10, 'dropout': 0.45950325894064187, 'num_neurons': 300, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 433.93415421004005\tTime: 0:00:23.257915\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 420.7973713256037\tTime: 0:00:20.530296\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 415.65737426271113\tTime: 0:00:20.503391\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 411.41860921279056\tTime: 0:00:20.357070\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 409.2295074680988\tTime: 0:00:20.545912\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 407.9135706557258\tTime: 0:00:27.111398\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 406.71632872300034\tTime: 0:00:20.383828\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 405.773088984804\tTime: 0:00:20.733210\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 404.82798056895547\tTime: 0:00:20.598747\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 404.1935006470746\tTime: 0:00:20.612653\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 403.6296199935469\tTime: 0:00:21.078936\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 403.0690037681369\tTime: 0:00:20.543411\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 402.6069611061743\tTime: 0:00:20.673774\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 402.25210178024855\tTime: 0:00:27.216181\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 401.9003634798978\tTime: 0:00:20.400217\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 401.5252532098073\tTime: 0:00:20.861069\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 401.13929518384913\tTime: 0:00:20.626347\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 400.9082843766456\tTime: 0:00:20.831815\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 400.6396587959828\tTime: 0:00:27.175020\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 400.45467947033916\tTime: 0:00:20.579983\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 400.2406468409713\tTime: 0:00:28.633375\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 400.151527890913\tTime: 0:00:20.896031\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 399.9006260049932\tTime: 0:00:20.870840\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 399.7013435613434\tTime: 0:00:21.064103\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 399.6047553003108\tTime: 0:00:20.627518\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 399.49196677615805\tTime: 0:00:27.117748\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 399.22485702355095\tTime: 0:00:20.721452\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 399.2357125666469\tTime: 0:00:20.570409\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 398.93152238402024\tTime: 0:00:20.634546\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 398.78179808128573\tTime: 0:00:27.561510\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 398.7787883910352\tTime: 0:00:20.572305\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 398.62515416507983\tTime: 0:00:20.899925\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 398.6537641477775\tTime: 0:00:20.515973\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 398.4896506634718\tTime: 0:00:20.963970\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 398.53234818284295\tTime: 0:00:20.710211\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 398.52460841261615\tTime: 0:00:27.461627\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 398.34380811035413\tTime: 0:00:21.024400\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 398.13968465545776\tTime: 0:00:20.579629\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 398.2002056709828\tTime: 0:00:20.588805\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 398.089833722107\tTime: 0:00:27.420674\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 398.01124457391813\tTime: 0:00:22.391207\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 397.8256247487517\tTime: 0:00:28.984542\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 397.8217782987874\tTime: 0:00:20.876634\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 397.7191727509897\tTime: 0:00:20.754509\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 397.6507124358803\tTime: 0:00:20.753797\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 397.6272900695502\tTime: 0:00:21.010435\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 397.7454846932479\tTime: 0:00:28.535455\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 397.4326690533787\tTime: 0:00:20.893113\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 397.61217719000365\tTime: 0:00:20.798689\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 397.47560393628225\tTime: 0:00:21.147509\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 397.37266162759533\tTime: 0:00:27.392656\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 397.25859335329216\tTime: 0:00:20.845782\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 397.21032855344447\tTime: 0:00:20.867010\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 397.0770591782329\tTime: 0:00:21.094835\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 397.20320537007257\tTime: 0:00:20.833363\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 397.1962547546171\tTime: 0:00:21.104001\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 397.0512532956138\tTime: 0:00:27.792951\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 397.0654924261124\tTime: 0:00:21.300731\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 397.0084959394747\tTime: 0:00:20.922052\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 397.0584452049481\tTime: 0:00:21.028436\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 397.0073988218658\tTime: 0:00:21.036053\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 396.7988225417043\tTime: 0:00:27.946470\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 396.90165186211726\tTime: 0:00:20.871564\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 396.8867403852709\tTime: 0:00:21.517846\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 396.84932179853826\tTime: 0:00:27.607106\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 396.5495892898283\tTime: 0:00:20.857991\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 396.67735500474873\tTime: 0:00:20.794175\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 396.87232343415\tTime: 0:00:20.858049\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 396.59652544895573\tTime: 0:00:20.918845\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 396.7046782949085\tTime: 0:00:27.536336\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 396.70179224527243\tTime: 0:00:21.303024\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 396.6188138552716\tTime: 0:00:21.047817\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 396.5017257893605\tTime: 0:00:22.720434\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 396.52890086505823\tTime: 0:00:22.446934\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 396.50056361901\tTime: 0:00:21.110953\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 396.3326130881855\tTime: 0:00:21.019712\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 396.35859543233516\tTime: 0:00:21.636201\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 396.49214121741704\tTime: 0:00:27.945480\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 396.26174302998993\tTime: 0:00:21.074189\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 396.3515274574729\tTime: 0:00:21.251295\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 396.35154758672326\tTime: 0:00:21.368491\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 396.3333300641668\tTime: 0:00:21.065200\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 396.1565014319684\tTime: 0:00:28.237765\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 396.11894410768355\tTime: 0:00:28.940740\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 396.12669236305055\tTime: 0:00:21.018930\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 396.20177795644327\tTime: 0:00:20.953422\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 396.1998529315829\tTime: 0:00:21.069624\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 396.19187487951837\tTime: 0:00:21.241644\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 396.16316278128056\tTime: 0:00:28.422217\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 396.0735321882875\tTime: 0:00:21.199549\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 396.0892111479021\tTime: 0:00:20.978995\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 396.1499417574099\tTime: 0:00:21.066113\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 396.1501964218127\tTime: 0:00:21.416853\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 396.02288537817134\tTime: 0:00:27.871203\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 396.0840115201022\tTime: 0:00:21.315192\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 395.98154709745023\tTime: 0:00:21.115450\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 396.1190453783829\tTime: 0:00:21.204274\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 395.89670157254994\tTime: 0:00:21.041699\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 395.9447671001655\tTime: 0:00:21.118093\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 395.9402565318683\tTime: 0:00:28.189507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-15 01:14:44,584] Trial 47 finished with values: [0.049236189061073056, 0.926829268292683] and parameters: {'num_topics': 41, 'dropout': 0.10669488347501321, 'num_neurons': 300, 'num_layers': 2, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 428.86398595742355\tTime: 0:00:15.323080\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 416.15079493642094\tTime: 0:00:14.055252\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 412.21777751477515\tTime: 0:00:13.896217\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 410.3147031978767\tTime: 0:00:14.059058\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 409.14679439484127\tTime: 0:00:14.031136\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 408.5514023549166\tTime: 0:00:13.927852\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 408.0300839360354\tTime: 0:00:14.302967\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 407.63350251454006\tTime: 0:00:13.895020\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 407.32589307753614\tTime: 0:00:14.059989\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 406.98908506092255\tTime: 0:00:13.991269\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 406.7766473073234\tTime: 0:00:13.789242\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 406.6244238553068\tTime: 0:00:14.070144\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 406.4569940770048\tTime: 0:00:14.063870\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 406.3897540793322\tTime: 0:00:13.913545\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 406.1403538428355\tTime: 0:00:13.700971\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 406.0259305884676\tTime: 0:00:13.871084\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 405.8398595081175\tTime: 0:00:13.719382\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 405.7827728806397\tTime: 0:00:14.014040\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 405.68318863047045\tTime: 0:00:14.008324\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 405.51879410803616\tTime: 0:00:15.333143\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 405.50177346776735\tTime: 0:00:15.756320\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 405.45440614597436\tTime: 0:00:23.910320\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 405.31762465736193\tTime: 0:00:18.060966\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 405.2582377855121\tTime: 0:00:18.059433\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 405.15572979846064\tTime: 0:00:18.430171\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 405.03023119546003\tTime: 0:00:18.203742\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 405.07347451871993\tTime: 0:00:18.428384\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 405.0167505483384\tTime: 0:00:18.249225\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 404.8121187564061\tTime: 0:00:18.440734\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 405.0243830237837\tTime: 0:00:18.638578\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 404.9501573313962\tTime: 0:00:18.145267\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 404.7181307684261\tTime: 0:00:18.342685\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 404.7299533971122\tTime: 0:00:18.460196\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 404.7122108587461\tTime: 0:00:26.380323\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 404.6006519085468\tTime: 0:00:18.159617\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 404.65878724058246\tTime: 0:00:18.566972\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 404.5618615207487\tTime: 0:00:18.357639\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 404.5163844016823\tTime: 0:00:18.314927\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 404.426178178247\tTime: 0:00:18.853966\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 404.4010087913399\tTime: 0:00:18.646923\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 404.34204973952455\tTime: 0:00:18.877291\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 404.256995943001\tTime: 0:00:25.014838\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 404.27949688187624\tTime: 0:00:18.534573\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 404.22892537278784\tTime: 0:00:18.706393\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 404.3521328777364\tTime: 0:00:18.553468\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 404.17294181353674\tTime: 0:00:18.528295\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 404.2424189863791\tTime: 0:00:18.846068\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 404.129603464053\tTime: 0:00:18.501092\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 404.1589453360086\tTime: 0:00:18.824381\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 404.0524103601872\tTime: 0:00:18.604916\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 404.1985217930725\tTime: 0:00:18.668123\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 404.03208672298155\tTime: 0:00:18.335973\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 404.09971780848\tTime: 0:00:25.214813\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 404.07152326706256\tTime: 0:00:18.594732\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 404.0902317890525\tTime: 0:00:18.510524\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 404.0721754474277\tTime: 0:00:18.564184\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 403.8851283555607\tTime: 0:00:18.627952\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 403.96130989113163\tTime: 0:00:18.205778\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 403.94757391842745\tTime: 0:00:18.793327\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 403.82485572323077\tTime: 0:00:18.623761\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 403.84454367283297\tTime: 0:00:18.467161\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 403.92660732064104\tTime: 0:00:25.367219\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 403.81274063269905\tTime: 0:00:18.368300\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 403.6514066011599\tTime: 0:00:19.248188\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 403.83527958941033\tTime: 0:00:18.737313\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 403.72492633722874\tTime: 0:00:18.596158\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 403.727479042071\tTime: 0:00:18.837283\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 403.7635170120082\tTime: 0:00:18.905075\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 403.7505985513405\tTime: 0:00:18.779355\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 403.7133047806529\tTime: 0:00:18.623588\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 403.85690204078276\tTime: 0:00:18.400654\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 403.73413770993193\tTime: 0:00:18.784973\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 403.7982714116452\tTime: 0:00:18.747746\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 403.7696543028991\tTime: 0:00:18.902692\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 403.64568735953605\tTime: 0:00:19.058406\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 403.66339845513147\tTime: 0:00:18.490392\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 403.5996368139599\tTime: 0:00:18.398519\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 403.6958519492167\tTime: 0:00:18.790864\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 403.6251088742665\tTime: 0:00:18.631028\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 403.4394841637163\tTime: 0:00:18.426082\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 403.44358737181926\tTime: 0:00:18.499161\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 403.6880582561075\tTime: 0:00:18.502795\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 403.5373252501904\tTime: 0:00:25.414423\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 403.6378196877586\tTime: 0:00:18.612311\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 403.58238942576685\tTime: 0:00:18.561230\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 403.589835301592\tTime: 0:00:18.912551\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 403.5336901940871\tTime: 0:00:18.358242\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 403.56878429319005\tTime: 0:00:18.425595\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 403.3902581186997\tTime: 0:00:18.358178\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 403.54839538284955\tTime: 0:00:25.510958\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 403.4266785811259\tTime: 0:00:18.297075\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 403.4257575393593\tTime: 0:00:18.571043\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 403.4909176675459\tTime: 0:00:18.541123\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 403.4923357511942\tTime: 0:00:18.855054\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 403.433961108819\tTime: 0:00:18.559544\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 403.3808729659172\tTime: 0:00:26.972634\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 403.3518786173879\tTime: 0:00:18.882933\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 403.37050779780714\tTime: 0:00:18.911421\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 403.4920254007631\tTime: 0:00:18.623127\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 403.31416305074805\tTime: 0:00:18.724800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-15 01:45:57,669] Trial 48 finished with values: [0.028531295707150617, 0.94] and parameters: {'num_topics': 35, 'dropout': 0.2890307828033601, 'num_neurons': 100, 'num_layers': 2, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [26586/2658600]\tTrain Loss: 439.27148580755613\tTime: 0:00:23.348715\n",
      "Epoch: [2/100]\tSamples: [53172/2658600]\tTrain Loss: 422.55804996638267\tTime: 0:00:20.757790\n",
      "Epoch: [3/100]\tSamples: [79758/2658600]\tTrain Loss: 417.0997653399581\tTime: 0:00:20.569704\n",
      "Epoch: [4/100]\tSamples: [106344/2658600]\tTrain Loss: 414.5152610572764\tTime: 0:00:19.985652\n",
      "Epoch: [5/100]\tSamples: [132930/2658600]\tTrain Loss: 413.02028337870496\tTime: 0:00:20.143417\n",
      "Epoch: [6/100]\tSamples: [159516/2658600]\tTrain Loss: 411.68399269851847\tTime: 0:00:26.785909\n",
      "Epoch: [7/100]\tSamples: [186102/2658600]\tTrain Loss: 410.820102501963\tTime: 0:00:20.331504\n",
      "Epoch: [8/100]\tSamples: [212688/2658600]\tTrain Loss: 410.1682572080348\tTime: 0:00:20.044223\n",
      "Epoch: [9/100]\tSamples: [239274/2658600]\tTrain Loss: 409.4866017432225\tTime: 0:00:20.087074\n",
      "Epoch: [10/100]\tSamples: [265860/2658600]\tTrain Loss: 409.06440507927107\tTime: 0:00:19.948703\n",
      "Epoch: [11/100]\tSamples: [292446/2658600]\tTrain Loss: 408.8392454190528\tTime: 0:00:20.242253\n",
      "Epoch: [12/100]\tSamples: [319032/2658600]\tTrain Loss: 408.4170323358041\tTime: 0:00:26.526881\n",
      "Epoch: [13/100]\tSamples: [345618/2658600]\tTrain Loss: 408.2794204054178\tTime: 0:00:19.825441\n",
      "Epoch: [14/100]\tSamples: [372204/2658600]\tTrain Loss: 408.1140453901372\tTime: 0:00:20.265403\n",
      "Epoch: [15/100]\tSamples: [398790/2658600]\tTrain Loss: 407.69444576680394\tTime: 0:00:20.130100\n",
      "Epoch: [16/100]\tSamples: [425376/2658600]\tTrain Loss: 407.59500550689273\tTime: 0:00:20.410480\n",
      "Epoch: [17/100]\tSamples: [451962/2658600]\tTrain Loss: 407.4460565108868\tTime: 0:00:28.924281\n",
      "Epoch: [18/100]\tSamples: [478548/2658600]\tTrain Loss: 407.44065507633246\tTime: 0:00:21.031025\n",
      "Epoch: [19/100]\tSamples: [505134/2658600]\tTrain Loss: 407.1361753337048\tTime: 0:00:20.089234\n",
      "Epoch: [20/100]\tSamples: [531720/2658600]\tTrain Loss: 407.1494775724888\tTime: 0:00:20.059957\n",
      "Epoch: [21/100]\tSamples: [558306/2658600]\tTrain Loss: 407.0415406384587\tTime: 0:00:27.588364\n",
      "Epoch: [22/100]\tSamples: [584892/2658600]\tTrain Loss: 407.0082061223482\tTime: 0:00:20.305268\n",
      "Epoch: [23/100]\tSamples: [611478/2658600]\tTrain Loss: 407.0792433488255\tTime: 0:00:20.048327\n",
      "Epoch: [24/100]\tSamples: [638064/2658600]\tTrain Loss: 406.8120410677847\tTime: 0:00:20.028952\n",
      "Epoch: [25/100]\tSamples: [664650/2658600]\tTrain Loss: 406.8700017014359\tTime: 0:00:20.056550\n",
      "Epoch: [26/100]\tSamples: [691236/2658600]\tTrain Loss: 406.6328953678628\tTime: 0:00:26.715678\n",
      "Epoch: [27/100]\tSamples: [717822/2658600]\tTrain Loss: 406.4546858617435\tTime: 0:00:20.317555\n",
      "Epoch: [28/100]\tSamples: [744408/2658600]\tTrain Loss: 406.68630069338656\tTime: 0:00:20.281259\n",
      "Epoch: [29/100]\tSamples: [770994/2658600]\tTrain Loss: 406.57752486917457\tTime: 0:00:27.366013\n",
      "Epoch: [30/100]\tSamples: [797580/2658600]\tTrain Loss: 406.5642209039767\tTime: 0:00:20.721516\n",
      "Epoch: [31/100]\tSamples: [824166/2658600]\tTrain Loss: 406.34956158639653\tTime: 0:00:20.496039\n",
      "Epoch: [32/100]\tSamples: [850752/2658600]\tTrain Loss: 406.38722469944236\tTime: 0:00:20.174526\n",
      "Epoch: [33/100]\tSamples: [877338/2658600]\tTrain Loss: 406.4566166535606\tTime: 0:00:26.673411\n",
      "Epoch: [34/100]\tSamples: [903924/2658600]\tTrain Loss: 406.28483190459735\tTime: 0:00:20.188361\n",
      "Epoch: [35/100]\tSamples: [930510/2658600]\tTrain Loss: 406.3126778940866\tTime: 0:00:20.213492\n",
      "Epoch: [36/100]\tSamples: [957096/2658600]\tTrain Loss: 406.3758287888245\tTime: 0:00:20.082110\n",
      "Epoch: [37/100]\tSamples: [983682/2658600]\tTrain Loss: 406.2383371564216\tTime: 0:00:20.291375\n",
      "Epoch: [38/100]\tSamples: [1010268/2658600]\tTrain Loss: 406.06801725120835\tTime: 0:00:26.534637\n",
      "Epoch: [39/100]\tSamples: [1036854/2658600]\tTrain Loss: 405.94330677447715\tTime: 0:00:20.377856\n",
      "Epoch: [40/100]\tSamples: [1063440/2658600]\tTrain Loss: 406.19176989886597\tTime: 0:00:20.066003\n",
      "Epoch: [41/100]\tSamples: [1090026/2658600]\tTrain Loss: 406.210841739132\tTime: 0:00:20.330133\n",
      "Epoch: [42/100]\tSamples: [1116612/2658600]\tTrain Loss: 406.1760092835515\tTime: 0:00:27.467968\n",
      "Epoch: [43/100]\tSamples: [1143198/2658600]\tTrain Loss: 406.01084636004384\tTime: 0:00:20.788668\n",
      "Epoch: [44/100]\tSamples: [1169784/2658600]\tTrain Loss: 406.0365634609146\tTime: 0:00:27.749445\n",
      "Epoch: [45/100]\tSamples: [1196370/2658600]\tTrain Loss: 406.0704618164283\tTime: 0:00:20.096641\n",
      "Epoch: [46/100]\tSamples: [1222956/2658600]\tTrain Loss: 406.0236113535437\tTime: 0:00:19.846352\n",
      "Epoch: [47/100]\tSamples: [1249542/2658600]\tTrain Loss: 405.9825308961954\tTime: 0:00:20.448074\n",
      "Epoch: [48/100]\tSamples: [1276128/2658600]\tTrain Loss: 406.22613030883264\tTime: 0:00:20.083137\n",
      "Epoch: [49/100]\tSamples: [1302714/2658600]\tTrain Loss: 406.0260366710863\tTime: 0:00:27.638669\n",
      "Epoch: [50/100]\tSamples: [1329300/2658600]\tTrain Loss: 405.85523700649543\tTime: 0:00:20.136212\n",
      "Epoch: [51/100]\tSamples: [1355886/2658600]\tTrain Loss: 405.84249555834134\tTime: 0:00:20.174371\n",
      "Epoch: [52/100]\tSamples: [1382472/2658600]\tTrain Loss: 405.7895900479811\tTime: 0:00:20.202448\n",
      "Epoch: [53/100]\tSamples: [1409058/2658600]\tTrain Loss: 405.8742887542433\tTime: 0:00:26.790073\n",
      "Epoch: [54/100]\tSamples: [1435644/2658600]\tTrain Loss: 405.8531298633444\tTime: 0:00:20.344137\n",
      "Epoch: [55/100]\tSamples: [1462230/2658600]\tTrain Loss: 405.6851808752727\tTime: 0:00:20.361338\n",
      "Epoch: [56/100]\tSamples: [1488816/2658600]\tTrain Loss: 405.58010618840746\tTime: 0:00:20.654422\n",
      "Epoch: [57/100]\tSamples: [1515402/2658600]\tTrain Loss: 405.5434111527214\tTime: 0:00:20.078781\n",
      "Epoch: [58/100]\tSamples: [1541988/2658600]\tTrain Loss: 405.7458242457849\tTime: 0:00:27.030338\n",
      "Epoch: [59/100]\tSamples: [1568574/2658600]\tTrain Loss: 405.6787601953918\tTime: 0:00:20.273479\n",
      "Epoch: [60/100]\tSamples: [1595160/2658600]\tTrain Loss: 405.67240500903904\tTime: 0:00:20.232361\n",
      "Epoch: [61/100]\tSamples: [1621746/2658600]\tTrain Loss: 405.70639280748094\tTime: 0:00:20.287231\n",
      "Epoch: [62/100]\tSamples: [1648332/2658600]\tTrain Loss: 405.7316154193584\tTime: 0:00:26.952052\n",
      "Epoch: [63/100]\tSamples: [1674918/2658600]\tTrain Loss: 405.8978692159995\tTime: 0:00:20.449546\n",
      "Epoch: [64/100]\tSamples: [1701504/2658600]\tTrain Loss: 405.6277457325989\tTime: 0:00:20.534067\n",
      "Epoch: [65/100]\tSamples: [1728090/2658600]\tTrain Loss: 405.4747735312406\tTime: 0:00:20.233132\n",
      "Epoch: [66/100]\tSamples: [1754676/2658600]\tTrain Loss: 405.6105038542372\tTime: 0:00:27.364470\n",
      "Epoch: [67/100]\tSamples: [1781262/2658600]\tTrain Loss: 405.37475984481966\tTime: 0:00:20.191161\n",
      "Epoch: [68/100]\tSamples: [1807848/2658600]\tTrain Loss: 405.5818505642655\tTime: 0:00:20.376441\n",
      "Epoch: [69/100]\tSamples: [1834434/2658600]\tTrain Loss: 405.44843539891764\tTime: 0:00:27.772080\n",
      "Epoch: [70/100]\tSamples: [1861020/2658600]\tTrain Loss: 405.3744127254476\tTime: 0:00:20.295951\n",
      "Epoch: [71/100]\tSamples: [1887606/2658600]\tTrain Loss: 405.5783239416422\tTime: 0:00:20.376311\n",
      "Epoch: [72/100]\tSamples: [1914192/2658600]\tTrain Loss: 405.37347965388267\tTime: 0:00:20.404282\n",
      "Epoch: [73/100]\tSamples: [1940778/2658600]\tTrain Loss: 405.45535369002954\tTime: 0:00:25.313540\n",
      "Epoch: [74/100]\tSamples: [1967364/2658600]\tTrain Loss: 405.20975979192525\tTime: 0:00:21.899654\n",
      "Epoch: [75/100]\tSamples: [1993950/2658600]\tTrain Loss: 405.4857581513179\tTime: 0:00:20.344401\n",
      "Epoch: [76/100]\tSamples: [2020536/2658600]\tTrain Loss: 405.3781265721385\tTime: 0:00:21.131417\n",
      "Epoch: [77/100]\tSamples: [2047122/2658600]\tTrain Loss: 405.38958441326025\tTime: 0:00:20.361825\n",
      "Epoch: [78/100]\tSamples: [2073708/2658600]\tTrain Loss: 405.40667164902345\tTime: 0:00:26.958081\n",
      "Epoch: [79/100]\tSamples: [2100294/2658600]\tTrain Loss: 405.2090457177885\tTime: 0:00:20.307921\n",
      "Epoch: [80/100]\tSamples: [2126880/2658600]\tTrain Loss: 405.40636526567084\tTime: 0:00:20.648593\n",
      "Epoch: [81/100]\tSamples: [2153466/2658600]\tTrain Loss: 405.3213258681731\tTime: 0:00:20.839384\n",
      "Epoch: [82/100]\tSamples: [2180052/2658600]\tTrain Loss: 405.4415471180203\tTime: 0:00:20.582059\n",
      "Epoch: [83/100]\tSamples: [2206638/2658600]\tTrain Loss: 405.50561051428616\tTime: 0:00:20.426584\n",
      "Epoch: [84/100]\tSamples: [2233224/2658600]\tTrain Loss: 405.1859489824884\tTime: 0:00:26.973252\n",
      "Epoch: [85/100]\tSamples: [2259810/2658600]\tTrain Loss: 405.3761021499215\tTime: 0:00:20.423039\n",
      "Epoch: [86/100]\tSamples: [2286396/2658600]\tTrain Loss: 405.20961969528133\tTime: 0:00:20.326843\n",
      "Epoch: [87/100]\tSamples: [2312982/2658600]\tTrain Loss: 405.12514024357273\tTime: 0:00:20.600854\n",
      "Epoch: [88/100]\tSamples: [2339568/2658600]\tTrain Loss: 405.0205779342864\tTime: 0:00:28.260290\n",
      "Epoch: [89/100]\tSamples: [2366154/2658600]\tTrain Loss: 405.0691983753785\tTime: 0:00:20.608984\n",
      "Epoch: [90/100]\tSamples: [2392740/2658600]\tTrain Loss: 405.09196650433967\tTime: 0:00:20.738444\n",
      "Epoch: [91/100]\tSamples: [2419326/2658600]\tTrain Loss: 405.34722872382315\tTime: 0:00:27.697727\n",
      "Epoch: [92/100]\tSamples: [2445912/2658600]\tTrain Loss: 405.21075876107255\tTime: 0:00:20.433018\n",
      "Epoch: [93/100]\tSamples: [2472498/2658600]\tTrain Loss: 405.10573770323197\tTime: 0:00:20.447398\n",
      "Epoch: [94/100]\tSamples: [2499084/2658600]\tTrain Loss: 405.1745853962142\tTime: 0:00:26.954819\n",
      "Epoch: [95/100]\tSamples: [2525670/2658600]\tTrain Loss: 405.08944340565756\tTime: 0:00:20.821830\n",
      "Epoch: [96/100]\tSamples: [2552256/2658600]\tTrain Loss: 405.23374210404444\tTime: 0:00:20.358522\n",
      "Epoch: [97/100]\tSamples: [2578842/2658600]\tTrain Loss: 405.10557644883585\tTime: 0:00:20.508561\n",
      "Epoch: [98/100]\tSamples: [2605428/2658600]\tTrain Loss: 404.97286944373917\tTime: 0:00:27.233396\n",
      "Epoch: [99/100]\tSamples: [2632014/2658600]\tTrain Loss: 405.1137450308668\tTime: 0:00:20.764076\n",
      "Epoch: [100/100]\tSamples: [2658600/2658600]\tTrain Loss: 405.12469817144125\tTime: 0:00:20.440206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-15 02:23:40,577] Trial 49 finished with values: [0.026318954114752924, 0.9166666666666666] and parameters: {'num_topics': 42, 'dropout': 0.377587470715686, 'num_neurons': 300, 'num_layers': 1, 'activation': 'sigmoid'}.\n"
     ]
    }
   ],
   "source": [
    "def objectiveProdLDA(trial) -> Tuple[float, float]:\n",
    "    # Define hyperparameters to optimize\n",
    "    num_topics = trial.suggest_int(\"num_topics\", 10, 50)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 0.60)\n",
    "    num_neurons = trial.suggest_categorical(\"num_neurons\", [50, 100, 200, 300])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 2)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"softplus\",\"relu\", \"sigmoid\"])\n",
    "\n",
    "\n",
    "    # Train ProdLDA model\n",
    "    model = ProdLDA(\n",
    "        num_topics=num_topics,\n",
    "        dropout = dropout,\n",
    "        num_neurons=num_neurons,\n",
    "        num_layers=num_layers,\n",
    "        activation=activation,\n",
    "        use_partitions=False\n",
    "    )\n",
    "\n",
    "    output = model.train_model(dataset)\n",
    "\n",
    "    # Compute coherence score (can also use perplexity, but coherence is often better)\n",
    "    coherence_metrics = Coherence(texts=dataset.get_corpus(), #list of our documents\n",
    "                    measure='c_npmi')\n",
    "    coherence = coherence_metrics.score(output)\n",
    "\n",
    "    diverisity_metric = TopicDiversity(topk=10) # Initialize metric\n",
    "    diversity = diverisity_metric.score(output)\n",
    "\n",
    "    return coherence, diversity  # Optuna will maximize these\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(\n",
    "    directions=[\"maximize\",\"maximize\"],\n",
    "    storage=f\"sqlite:///{optuna_folder}ProdLDA_Study.db\",\n",
    "    study_name=\"ProdLDA_Study\"\n",
    "  )\n",
    "study.optimize(objectiveProdLDA, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "kHi5jmrFpJUG",
   "metadata": {
    "id": "kHi5jmrFpJUG"
   },
   "outputs": [],
   "source": [
    "def train_final_ProdLDA_model(params):\n",
    "    \"\"\"Train final LDA model with selected parameters\"\"\"\n",
    "    print(f\"\\nTraining final model with parameters: {params}\")\n",
    "\n",
    "    model = ProdLDA(\n",
    "        num_topics=params[\"num_topics\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "        num_neurons=params[\"num_neurons\"],\n",
    "        num_layers=params[\"num_layers\"],\n",
    "        activation=params[\"activation\"],\n",
    "        use_partitions=False\n",
    "    )\n",
    "\n",
    "    output = model.train_model(dataset)\n",
    "\n",
    "    # Calculate final metrics\n",
    "    coherence_metrics = Coherence(texts=dataset.get_corpus(), #list of our documents\n",
    "                    measure='c_npmi')\n",
    "    coherence = coherence_metrics.score(output)\n",
    "\n",
    "    diverisity_metric = TopicDiversity(topk=10) # Initialize metric\n",
    "    diversity = diverisity_metric.score(output)\n",
    "\n",
    "    print(f\"Final model metrics:\")\n",
    "    print(f\"  Coherence: {coherence:.4f}\")\n",
    "    print(f\"  Diversity: {diversity:.4f}\")\n",
    "\n",
    "    return model, output, coherence, diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c59607-72da-499c-8e48-a4982ee91462",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProdLDA_study = optuna.load_study(\n",
    "    storage=f\"sqlite:///{optuna_folder}ProdLDA_Study.db\",\n",
    "    study_name=\"ProdLDA_Study\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "DEpVYXivu_-e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62207,
     "status": "ok",
     "timestamp": 1754646250339,
     "user": {
      "displayName": "Giovanna Tonazzo",
      "userId": "15393083850265173052"
     },
     "user_tz": -120
    },
    "id": "DEpVYXivu_-e",
    "outputId": "e948b41d-3fc4-46ef-c99c-95d0209d0bb5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model with parameters: {'num_topics': 18, 'dropout': 0.1373903093404273, 'num_neurons': 300, 'num_layers': 1, 'activation': 'sigmoid'}\n",
      "Epoch: [1/100]\tSamples: [5843/584300]\tTrain Loss: 2246.3884215086428\tTime: 0:00:06.148821\n",
      "Epoch: [2/100]\tSamples: [11686/584300]\tTrain Loss: 2196.185287817688\tTime: 0:00:04.758910\n",
      "Epoch: [3/100]\tSamples: [17529/584300]\tTrain Loss: 2170.5700049739003\tTime: 0:00:04.917258\n",
      "Epoch: [4/100]\tSamples: [23372/584300]\tTrain Loss: 2154.904042497433\tTime: 0:00:04.796963\n",
      "Epoch: [5/100]\tSamples: [29215/584300]\tTrain Loss: 2144.8061088054083\tTime: 0:00:04.559612\n",
      "Epoch: [6/100]\tSamples: [35058/584300]\tTrain Loss: 2136.6208911838953\tTime: 0:00:05.026892\n",
      "Epoch: [7/100]\tSamples: [40901/584300]\tTrain Loss: 2131.2892764311996\tTime: 0:00:04.862225\n",
      "Epoch: [8/100]\tSamples: [46744/584300]\tTrain Loss: 2126.4075543117833\tTime: 0:00:04.804730\n",
      "Epoch: [9/100]\tSamples: [52587/584300]\tTrain Loss: 2124.5415561355467\tTime: 0:00:04.724329\n",
      "Epoch: [10/100]\tSamples: [58430/584300]\tTrain Loss: 2121.4220682066148\tTime: 0:00:05.270310\n",
      "Epoch: [11/100]\tSamples: [64273/584300]\tTrain Loss: 2119.57029110688\tTime: 0:00:04.907859\n",
      "Epoch: [12/100]\tSamples: [70116/584300]\tTrain Loss: 2118.235836417508\tTime: 0:00:04.607203\n",
      "Epoch: [13/100]\tSamples: [75959/584300]\tTrain Loss: 2117.7563397173967\tTime: 0:00:05.039212\n",
      "Epoch: [14/100]\tSamples: [81802/584300]\tTrain Loss: 2116.904630139697\tTime: 0:00:04.684928\n",
      "Epoch: [15/100]\tSamples: [87645/584300]\tTrain Loss: 2116.242321207\tTime: 0:00:04.856274\n",
      "Epoch: [16/100]\tSamples: [93488/584300]\tTrain Loss: 2114.796926477195\tTime: 0:00:04.790549\n",
      "Epoch: [17/100]\tSamples: [99331/584300]\tTrain Loss: 2114.1535705117235\tTime: 0:00:04.705056\n",
      "Epoch: [18/100]\tSamples: [105174/584300]\tTrain Loss: 2112.7812807526097\tTime: 0:00:04.877802\n",
      "Epoch: [19/100]\tSamples: [111017/584300]\tTrain Loss: 2112.7169128658225\tTime: 0:00:04.918854\n",
      "Epoch: [20/100]\tSamples: [116860/584300]\tTrain Loss: 2112.8637966904844\tTime: 0:00:04.479855\n",
      "Epoch: [21/100]\tSamples: [122703/584300]\tTrain Loss: 2111.9286312147015\tTime: 0:00:04.771968\n",
      "Epoch: [22/100]\tSamples: [128546/584300]\tTrain Loss: 2112.0189075068456\tTime: 0:00:04.509847\n",
      "Epoch: [23/100]\tSamples: [134389/584300]\tTrain Loss: 2111.7580077122198\tTime: 0:00:05.108790\n",
      "Epoch: [24/100]\tSamples: [140232/584300]\tTrain Loss: 2111.7512260931885\tTime: 0:00:04.747283\n",
      "Epoch: [25/100]\tSamples: [146075/584300]\tTrain Loss: 2111.8722790625534\tTime: 0:00:04.916085\n",
      "Epoch: [26/100]\tSamples: [151918/584300]\tTrain Loss: 2111.105540283245\tTime: 0:00:04.718510\n",
      "Epoch: [27/100]\tSamples: [157761/584300]\tTrain Loss: 2111.0442088824234\tTime: 0:00:04.988631\n",
      "Epoch: [28/100]\tSamples: [163604/584300]\tTrain Loss: 2111.4821922332276\tTime: 0:00:05.218886\n",
      "Epoch: [29/100]\tSamples: [169447/584300]\tTrain Loss: 2110.199180643505\tTime: 0:00:05.314232\n",
      "Epoch: [30/100]\tSamples: [175290/584300]\tTrain Loss: 2109.743701063238\tTime: 0:00:04.971529\n",
      "Epoch: [31/100]\tSamples: [181133/584300]\tTrain Loss: 2110.7092808702723\tTime: 0:00:05.462541\n",
      "Epoch: [32/100]\tSamples: [186976/584300]\tTrain Loss: 2108.6296590204092\tTime: 0:00:04.269080\n",
      "Epoch: [33/100]\tSamples: [192819/584300]\tTrain Loss: 2109.75791812853\tTime: 0:00:04.355347\n",
      "Epoch: [34/100]\tSamples: [198662/584300]\tTrain Loss: 2109.261822372925\tTime: 0:00:04.235654\n",
      "Epoch: [35/100]\tSamples: [204505/584300]\tTrain Loss: 2109.0284381417937\tTime: 0:00:04.279559\n",
      "Epoch: [36/100]\tSamples: [210348/584300]\tTrain Loss: 2110.2800947715214\tTime: 0:00:04.426590\n",
      "Epoch: [37/100]\tSamples: [216191/584300]\tTrain Loss: 2108.9730299610646\tTime: 0:00:04.792733\n",
      "Epoch: [38/100]\tSamples: [222034/584300]\tTrain Loss: 2108.033485581037\tTime: 0:00:04.836203\n",
      "Epoch: [39/100]\tSamples: [227877/584300]\tTrain Loss: 2108.757158004236\tTime: 0:00:04.891357\n",
      "Epoch: [40/100]\tSamples: [233720/584300]\tTrain Loss: 2109.231208818244\tTime: 0:00:05.120751\n",
      "Epoch: [41/100]\tSamples: [239563/584300]\tTrain Loss: 2107.025037304253\tTime: 0:00:04.841821\n",
      "Epoch: [42/100]\tSamples: [245406/584300]\tTrain Loss: 2107.8333368988533\tTime: 0:00:04.722958\n",
      "Epoch: [43/100]\tSamples: [251249/584300]\tTrain Loss: 2107.4536210529695\tTime: 0:00:04.695957\n",
      "Epoch: [44/100]\tSamples: [257092/584300]\tTrain Loss: 2107.7892015552798\tTime: 0:00:04.803146\n",
      "Epoch: [45/100]\tSamples: [262935/584300]\tTrain Loss: 2107.4699319698784\tTime: 0:00:04.461198\n",
      "Epoch: [46/100]\tSamples: [268778/584300]\tTrain Loss: 2107.2862727034485\tTime: 0:00:04.734031\n",
      "Epoch: [47/100]\tSamples: [274621/584300]\tTrain Loss: 2108.498206989132\tTime: 0:00:04.724085\n",
      "Epoch: [48/100]\tSamples: [280464/584300]\tTrain Loss: 2107.7042407849135\tTime: 0:00:04.378781\n",
      "Epoch: [49/100]\tSamples: [286307/584300]\tTrain Loss: 2107.2416981323804\tTime: 0:00:04.536828\n",
      "Epoch: [50/100]\tSamples: [292150/584300]\tTrain Loss: 2107.485055568629\tTime: 0:00:04.993614\n",
      "Epoch: [51/100]\tSamples: [297993/584300]\tTrain Loss: 2108.1732976424782\tTime: 0:00:04.781330\n",
      "Epoch: [52/100]\tSamples: [303836/584300]\tTrain Loss: 2106.797898527084\tTime: 0:00:04.802270\n",
      "Epoch: [53/100]\tSamples: [309679/584300]\tTrain Loss: 2107.9759380883106\tTime: 0:00:04.979924\n",
      "Epoch: [54/100]\tSamples: [315522/584300]\tTrain Loss: 2108.036532763563\tTime: 0:00:04.776840\n",
      "Epoch: [55/100]\tSamples: [321365/584300]\tTrain Loss: 2107.4315386734124\tTime: 0:00:04.770992\n",
      "Epoch: [56/100]\tSamples: [327208/584300]\tTrain Loss: 2106.7738573399793\tTime: 0:00:04.636276\n",
      "Epoch: [57/100]\tSamples: [333051/584300]\tTrain Loss: 2106.974244689158\tTime: 0:00:04.854647\n",
      "Epoch: [58/100]\tSamples: [338894/584300]\tTrain Loss: 2106.2956402158566\tTime: 0:00:04.781446\n",
      "Epoch: [59/100]\tSamples: [344737/584300]\tTrain Loss: 2106.2322396992126\tTime: 0:00:04.935670\n",
      "Epoch: [60/100]\tSamples: [350580/584300]\tTrain Loss: 2107.597176241871\tTime: 0:00:04.779684\n",
      "Epoch: [61/100]\tSamples: [356423/584300]\tTrain Loss: 2107.379981254279\tTime: 0:00:05.636701\n",
      "Epoch: [62/100]\tSamples: [362266/584300]\tTrain Loss: 2106.73064925445\tTime: 0:00:04.724620\n",
      "Epoch: [63/100]\tSamples: [368109/584300]\tTrain Loss: 2106.076615447972\tTime: 0:00:04.930307\n",
      "Epoch: [64/100]\tSamples: [373952/584300]\tTrain Loss: 2106.121710139269\tTime: 0:00:04.867521\n",
      "Epoch: [65/100]\tSamples: [379795/584300]\tTrain Loss: 2106.8389733174313\tTime: 0:00:04.504042\n",
      "Epoch: [66/100]\tSamples: [385638/584300]\tTrain Loss: 2107.0419826608763\tTime: 0:00:04.709507\n",
      "Epoch: [67/100]\tSamples: [391481/584300]\tTrain Loss: 2107.337426995978\tTime: 0:00:04.799929\n",
      "Epoch: [68/100]\tSamples: [397324/584300]\tTrain Loss: 2106.0543238169603\tTime: 0:00:04.705624\n",
      "Epoch: [69/100]\tSamples: [403167/584300]\tTrain Loss: 2107.8424517050316\tTime: 0:00:04.678992\n",
      "Epoch: [70/100]\tSamples: [409010/584300]\tTrain Loss: 2107.1184189414685\tTime: 0:00:05.001819\n",
      "Epoch: [71/100]\tSamples: [414853/584300]\tTrain Loss: 2105.8767181349476\tTime: 0:00:04.854657\n",
      "Epoch: [72/100]\tSamples: [420696/584300]\tTrain Loss: 2107.0356797129043\tTime: 0:00:04.598880\n",
      "Epoch: [73/100]\tSamples: [426539/584300]\tTrain Loss: 2107.420792641836\tTime: 0:00:04.867308\n",
      "Epoch: [74/100]\tSamples: [432382/584300]\tTrain Loss: 2106.3909840032948\tTime: 0:00:04.468540\n",
      "Epoch: [75/100]\tSamples: [438225/584300]\tTrain Loss: 2106.6017650661047\tTime: 0:00:04.472320\n",
      "Epoch: [76/100]\tSamples: [444068/584300]\tTrain Loss: 2106.206229676536\tTime: 0:00:04.411682\n",
      "Epoch: [77/100]\tSamples: [449911/584300]\tTrain Loss: 2105.4667577657024\tTime: 0:00:05.275706\n",
      "Epoch: [78/100]\tSamples: [455754/584300]\tTrain Loss: 2106.5294168770324\tTime: 0:00:04.590085\n",
      "Epoch: [79/100]\tSamples: [461597/584300]\tTrain Loss: 2105.891391141109\tTime: 0:00:04.814948\n",
      "Epoch: [80/100]\tSamples: [467440/584300]\tTrain Loss: 2105.5053776955333\tTime: 0:00:04.622810\n",
      "Epoch: [81/100]\tSamples: [473283/584300]\tTrain Loss: 2105.596687542786\tTime: 0:00:05.056641\n",
      "Epoch: [82/100]\tSamples: [479126/584300]\tTrain Loss: 2105.4027261520196\tTime: 0:00:05.061480\n",
      "Epoch: [83/100]\tSamples: [484969/584300]\tTrain Loss: 2105.4860269499827\tTime: 0:00:04.428834\n",
      "Epoch: [84/100]\tSamples: [490812/584300]\tTrain Loss: 2105.5260875727367\tTime: 0:00:04.727275\n",
      "Epoch: [85/100]\tSamples: [496655/584300]\tTrain Loss: 2105.4105607136744\tTime: 0:00:04.801820\n",
      "Epoch: [86/100]\tSamples: [502498/584300]\tTrain Loss: 2105.991699469451\tTime: 0:00:04.776874\n",
      "Epoch: [87/100]\tSamples: [508341/584300]\tTrain Loss: 2104.9028210839892\tTime: 0:00:04.917476\n",
      "Epoch: [88/100]\tSamples: [514184/584300]\tTrain Loss: 2104.496811088054\tTime: 0:00:04.994009\n",
      "Epoch: [89/100]\tSamples: [520027/584300]\tTrain Loss: 2105.9715270943866\tTime: 0:00:04.562046\n",
      "Epoch: [90/100]\tSamples: [525870/584300]\tTrain Loss: 2105.2466559879344\tTime: 0:00:04.642962\n",
      "Epoch: [91/100]\tSamples: [531713/584300]\tTrain Loss: 2105.521925942367\tTime: 0:00:05.013883\n",
      "Epoch: [92/100]\tSamples: [537556/584300]\tTrain Loss: 2105.985997534443\tTime: 0:00:04.815956\n",
      "Epoch: [93/100]\tSamples: [543399/584300]\tTrain Loss: 2104.510327528667\tTime: 0:00:04.894067\n",
      "Epoch: [94/100]\tSamples: [549242/584300]\tTrain Loss: 2104.70342450368\tTime: 0:00:04.749255\n",
      "Epoch: [95/100]\tSamples: [555085/584300]\tTrain Loss: 2104.6885536111586\tTime: 0:00:05.149855\n",
      "Epoch: [96/100]\tSamples: [560928/584300]\tTrain Loss: 2104.485883214958\tTime: 0:00:04.841752\n",
      "Epoch: [97/100]\tSamples: [566771/584300]\tTrain Loss: 2104.5439976253638\tTime: 0:00:04.937496\n",
      "Epoch: [98/100]\tSamples: [572614/584300]\tTrain Loss: 2105.130409785213\tTime: 0:00:04.808965\n",
      "Epoch: [99/100]\tSamples: [578457/584300]\tTrain Loss: 2105.2043323742087\tTime: 0:00:04.876692\n",
      "Epoch: [100/100]\tSamples: [584300/584300]\tTrain Loss: 2104.912375786197\tTime: 0:00:05.180992\n",
      "Final model metrics:\n",
      "  Coherence: 0.0680\n",
      "  Diversity: 0.9833\n",
      "\n",
      "Final model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "if ProdLDA_study.best_trials:\n",
    "    # Get balanced solution\n",
    "    pareto_trials = ProdLDA_study.best_trials\n",
    "\n",
    "    # Pick the first Pareto optimal solution\n",
    "    selected_params = pareto_trials[0].params\n",
    "    final_model, final_output, final_coherence, final_diversity = train_final_ProdLDA_model(selected_params)\n",
    "\n",
    "    print(f\"\\nFinal model trained successfully!\")\n",
    "\n",
    "    pickle.dump(final_output, open(optuna_folder + \"Optuna_ProdLDA_output.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K4zMVmwVy563",
   "metadata": {
    "id": "K4zMVmwVy563"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "gMuLfIpwa4cK",
   "metadata": {
    "id": "gMuLfIpwa4cK"
   },
   "source": [
    "## ETM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cSQ_yBKW2Eru",
   "metadata": {
    "id": "cSQ_yBKW2Eru"
   },
   "source": [
    "### OCTIS ETM optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b419ef-9b7c-4511-9fe5-5b91178659c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "96b419ef-9b7c-4511-9fe5-5b91178659c9",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "33a671fe-8e3f-4dd3-f400-d8f7a36ac740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.29 .. NELBO: 1726.29\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.74 .. NELBO: 1714.74\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.1 .. NELBO: 1723.1\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.23 .. NELBO: 1726.23\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.52 .. NELBO: 1714.52\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.99 .. NELBO: 1722.99\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.06 .. NELBO: 1726.06\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.59 .. NELBO: 1714.59\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.88 .. NELBO: 1722.88\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.0 .. NELBO: 1726.0\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.25 .. NELBO: 1714.25\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.72 .. NELBO: 1722.72\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.8 .. NELBO: 1725.8\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.17 .. NELBO: 1714.17\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.62 .. NELBO: 1722.62\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.67 .. NELBO: 1725.67\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.22 .. NELBO: 1714.22\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.53 .. NELBO: 1722.53\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.61 .. NELBO: 1725.61\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.13 .. NELBO: 1714.13\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.5 .. NELBO: 1722.5\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.59 .. NELBO: 1725.59\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.05 .. NELBO: 1714.05\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.49 .. NELBO: 1722.49\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.56 .. NELBO: 1725.56\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.07 .. NELBO: 1714.07\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.55 .. NELBO: 1722.55\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.59 .. NELBO: 1725.59\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.99 .. NELBO: 1713.99\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.41 .. NELBO: 1722.41\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.44 .. NELBO: 1725.44\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.94 .. NELBO: 1713.94\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.23 .. NELBO: 1722.23\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.29 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.68 .. NELBO: 1713.68\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.16 .. NELBO: 1725.16\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.6 .. NELBO: 1713.6\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.03 .. NELBO: 1722.03\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.56 .. NELBO: 1713.56\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.0 .. NELBO: 1725.0\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.92 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.96 .. NELBO: 1724.96\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.89 .. NELBO: 1724.89\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.42 .. NELBO: 1713.42\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.82 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.33 .. NELBO: 1713.33\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.82 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.7 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.6 .. NELBO: 1724.6\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.85 .. NELBO: 1712.85\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.91 .. NELBO: 1712.91\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.37 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.39 .. NELBO: 1724.39\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.82 .. NELBO: 1712.82\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.84 .. NELBO: 1712.84\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.37 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.82 .. NELBO: 1712.82\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.32 .. NELBO: 1721.32\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.84 .. NELBO: 1712.84\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.69 .. NELBO: 1712.69\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.25 .. NELBO: 1721.25\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.25 .. NELBO: 1724.25\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.79 .. NELBO: 1712.79\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.27 .. NELBO: 1721.27\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.29 .. NELBO: 1724.29\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.78 .. NELBO: 1712.78\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.38 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.81 .. NELBO: 1712.81\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.0 .. NELBO: 1713.0\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 88 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 89 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 90 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 91 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 92 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.38 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.8 .. NELBO: 1712.8\n",
      "Epoch: 93 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.32 .. NELBO: 1721.32\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.71 .. NELBO: 1712.71\n",
      "Epoch: 94 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.3 .. NELBO: 1721.3\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.74 .. NELBO: 1712.74\n",
      "Epoch: 95 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.28 .. NELBO: 1721.28\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.28 .. NELBO: 1724.28\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 96 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.34 .. NELBO: 1721.34\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.73 .. NELBO: 1712.73\n",
      "Epoch: 97 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.75 .. NELBO: 1712.75\n",
      "Epoch: 98 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 99 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.73 .. NELBO: 1712.73\n",
      "Epoch: 100 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Current call:  31\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.590628172569627, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.91 .. Rec_loss: 1991.5 .. NELBO: 1992.41\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.71 .. Rec_loss: 1877.45 .. NELBO: 1878.16\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 1866.13 .. NELBO: 1866.9\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1723.22 .. NELBO: 1723.32\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1730.91 .. NELBO: 1730.99\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1734.23 .. NELBO: 1734.31\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1719.98 .. NELBO: 1720.06\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1728.13 .. NELBO: 1728.2\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1731.33 .. NELBO: 1731.4\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1718.61 .. NELBO: 1718.64\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1726.78 .. NELBO: 1726.8\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1729.94 .. NELBO: 1729.96\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.79 .. NELBO: 1717.79\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.96 .. NELBO: 1725.96\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.1 .. NELBO: 1729.1\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.14 .. NELBO: 1717.14\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.33 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.48 .. NELBO: 1728.48\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.6 .. NELBO: 1716.6\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.97 .. NELBO: 1727.97\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1716.09 .. NELBO: 1716.1\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.37 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.52 .. NELBO: 1727.53\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1715.66 .. NELBO: 1715.67\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1723.99 .. NELBO: 1724.01\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.12 .. NELBO: 1727.14\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1715.17 .. NELBO: 1715.28\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 1723.44 .. NELBO: 1723.57\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 1726.48 .. NELBO: 1726.63\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 1714.0 .. NELBO: 1714.51\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 1722.31 .. NELBO: 1722.8\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 1725.41 .. NELBO: 1725.91\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.79 .. Rec_loss: 1713.21 .. NELBO: 1714.0\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.75 .. Rec_loss: 1721.66 .. NELBO: 1722.41\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.78 .. Rec_loss: 1724.68 .. NELBO: 1725.46\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.03 .. Rec_loss: 1712.12 .. NELBO: 1713.15\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.0 .. Rec_loss: 1720.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 1.03 .. Rec_loss: 1723.71 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.47 .. Rec_loss: 1710.76 .. NELBO: 1712.23\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.4 .. Rec_loss: 1719.68 .. NELBO: 1721.08\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 1.49 .. Rec_loss: 1722.2 .. NELBO: 1723.69\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.05 .. Rec_loss: 1709.03 .. NELBO: 1711.08\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.67 .. Rec_loss: 1718.78 .. NELBO: 1720.45\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 1.9 .. Rec_loss: 1720.79 .. NELBO: 1722.69\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1706.43 .. NELBO: 1708.67\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.79 .. Rec_loss: 1716.74 .. NELBO: 1718.53\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 1.99 .. Rec_loss: 1718.6 .. NELBO: 1720.59\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1705.18 .. NELBO: 1707.78\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.0 .. Rec_loss: 1715.59 .. NELBO: 1717.59\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1717.41 .. NELBO: 1719.58\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1703.78 .. NELBO: 1706.53\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.15 .. Rec_loss: 1714.36 .. NELBO: 1716.51\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.35 .. Rec_loss: 1716.03 .. NELBO: 1718.38\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1702.97 .. NELBO: 1705.68\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.2 .. Rec_loss: 1713.54 .. NELBO: 1715.74\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 1715.08 .. NELBO: 1717.51\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1701.71 .. NELBO: 1704.38\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1712.56 .. NELBO: 1714.77\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.45 .. Rec_loss: 1714.19 .. NELBO: 1716.64\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1701.18 .. NELBO: 1703.97\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1711.98 .. NELBO: 1714.38\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1713.65 .. NELBO: 1716.24\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1700.63 .. NELBO: 1703.48\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.46 .. Rec_loss: 1711.46 .. NELBO: 1713.92\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1712.99 .. NELBO: 1715.7\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1701.35 .. NELBO: 1704.12\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1711.92 .. NELBO: 1714.32\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1713.17 .. NELBO: 1715.84\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1701.19 .. NELBO: 1703.95\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1711.63 .. NELBO: 1714.19\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1713.14 .. NELBO: 1715.88\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1701.73 .. NELBO: 1704.47\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1711.99 .. NELBO: 1714.52\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1713.28 .. NELBO: 1716.05\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1700.99 .. NELBO: 1703.62\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1711.58 .. NELBO: 1714.07\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1713.02 .. NELBO: 1715.8\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1700.56 .. NELBO: 1703.3\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1710.8 .. NELBO: 1713.44\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1712.23 .. NELBO: 1715.14\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1699.93 .. NELBO: 1702.62\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1710.4 .. NELBO: 1712.9\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1711.68 .. NELBO: 1714.53\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1699.6 .. NELBO: 1702.29\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.57 .. Rec_loss: 1710.14 .. NELBO: 1712.71\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 2.94 .. Rec_loss: 1711.26 .. NELBO: 1714.2\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1700.11 .. NELBO: 1702.75\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1710.39 .. NELBO: 1712.98\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1711.63 .. NELBO: 1714.52\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1699.55 .. NELBO: 1702.31\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1710.38 .. NELBO: 1713.02\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1711.6 .. NELBO: 1714.5\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1700.27 .. NELBO: 1702.89\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1710.8 .. NELBO: 1713.46\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1712.19 .. NELBO: 1715.1\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1701.16 .. NELBO: 1703.94\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1711.64 .. NELBO: 1714.28\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1712.84 .. NELBO: 1715.74\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1700.38 .. NELBO: 1703.13\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1710.67 .. NELBO: 1713.37\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1712.0 .. NELBO: 1715.0\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1699.74 .. NELBO: 1702.37\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1710.12 .. NELBO: 1712.77\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1711.27 .. NELBO: 1714.26\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1699.44 .. NELBO: 1702.15\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1709.69 .. NELBO: 1712.34\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.03 .. Rec_loss: 1710.79 .. NELBO: 1713.82\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1699.14 .. NELBO: 1701.97\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1709.61 .. NELBO: 1712.35\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1710.76 .. NELBO: 1713.85\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1698.22 .. NELBO: 1700.99\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1709.1 .. NELBO: 1711.82\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 1710.23 .. NELBO: 1713.29\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1698.36 .. NELBO: 1701.15\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1709.02 .. NELBO: 1711.77\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 3.17 .. Rec_loss: 1710.11 .. NELBO: 1713.28\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1697.85 .. NELBO: 1700.69\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1708.76 .. NELBO: 1711.44\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 3.11 .. Rec_loss: 1709.77 .. NELBO: 1712.88\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1697.96 .. NELBO: 1700.88\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1708.91 .. NELBO: 1711.75\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1709.97 .. NELBO: 1713.25\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 1697.64 .. NELBO: 1700.7\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1708.44 .. NELBO: 1711.35\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1709.57 .. NELBO: 1712.85\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1696.81 .. NELBO: 1700.02\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1708.18 .. NELBO: 1711.15\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 1709.31 .. NELBO: 1712.64\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.34 .. Rec_loss: 1696.78 .. NELBO: 1700.12\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1707.97 .. NELBO: 1711.02\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 3.32 .. Rec_loss: 1709.29 .. NELBO: 1712.61\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1695.81 .. NELBO: 1699.37\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1707.39 .. NELBO: 1710.67\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 3.59 .. Rec_loss: 1708.51 .. NELBO: 1712.1\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.73 .. Rec_loss: 1695.02 .. NELBO: 1698.75\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.45 .. Rec_loss: 1706.46 .. NELBO: 1709.91\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 3.7 .. Rec_loss: 1707.72 .. NELBO: 1711.42\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.02 .. Rec_loss: 1694.0 .. NELBO: 1698.02\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.71 .. Rec_loss: 1705.25 .. NELBO: 1708.96\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 4.03 .. Rec_loss: 1706.51 .. NELBO: 1710.54\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.06 .. Rec_loss: 1693.49 .. NELBO: 1697.55\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.77 .. Rec_loss: 1704.71 .. NELBO: 1708.48\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 4.04 .. Rec_loss: 1705.89 .. NELBO: 1709.93\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.25 .. Rec_loss: 1692.61 .. NELBO: 1696.86\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.99 .. Rec_loss: 1703.93 .. NELBO: 1707.92\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 4.24 .. Rec_loss: 1705.24 .. NELBO: 1709.48\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1691.91 .. NELBO: 1696.36\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.01 .. Rec_loss: 1703.34 .. NELBO: 1707.35\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 4.33 .. Rec_loss: 1704.51 .. NELBO: 1708.84\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 1691.28 .. NELBO: 1695.74\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1702.82 .. NELBO: 1706.96\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 4.39 .. Rec_loss: 1704.23 .. NELBO: 1708.62\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.49 .. Rec_loss: 1690.8 .. NELBO: 1695.29\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.22 .. Rec_loss: 1702.34 .. NELBO: 1706.56\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 1703.75 .. NELBO: 1708.21\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.74 .. Rec_loss: 1690.9 .. NELBO: 1695.64\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.37 .. Rec_loss: 1702.23 .. NELBO: 1706.6\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 4.56 .. Rec_loss: 1703.87 .. NELBO: 1708.43\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.58 .. Rec_loss: 1691.05 .. NELBO: 1695.63\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.36 .. Rec_loss: 1702.33 .. NELBO: 1706.69\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 4.6 .. Rec_loss: 1703.87 .. NELBO: 1708.47\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.57 .. Rec_loss: 1691.25 .. NELBO: 1695.82\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.4 .. Rec_loss: 1702.44 .. NELBO: 1706.84\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 4.69 .. Rec_loss: 1703.83 .. NELBO: 1708.52\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.43 .. Rec_loss: 1691.85 .. NELBO: 1696.28\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.33 .. Rec_loss: 1702.84 .. NELBO: 1707.17\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 4.67 .. Rec_loss: 1704.18 .. NELBO: 1708.85\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.57 .. Rec_loss: 1691.35 .. NELBO: 1695.92\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.43 .. Rec_loss: 1702.83 .. NELBO: 1707.26\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 1704.16 .. NELBO: 1708.86\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.25 .. Rec_loss: 1691.78 .. NELBO: 1696.03\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.33 .. Rec_loss: 1702.89 .. NELBO: 1707.22\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 4.66 .. Rec_loss: 1704.09 .. NELBO: 1708.75\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1692.21 .. NELBO: 1696.63\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.38 .. Rec_loss: 1703.58 .. NELBO: 1707.96\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 4.64 .. Rec_loss: 1704.87 .. NELBO: 1709.51\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.54 .. Rec_loss: 1692.74 .. NELBO: 1697.28\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.38 .. Rec_loss: 1704.32 .. NELBO: 1708.7\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 4.59 .. Rec_loss: 1705.96 .. NELBO: 1710.55\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1692.48 .. NELBO: 1696.9\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.26 .. Rec_loss: 1704.29 .. NELBO: 1708.55\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 4.67 .. Rec_loss: 1706.18 .. NELBO: 1710.85\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.38 .. Rec_loss: 1691.88 .. NELBO: 1696.26\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.02 .. Rec_loss: 1704.74 .. NELBO: 1708.76\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 4.4 .. Rec_loss: 1705.83 .. NELBO: 1710.23\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1691.12 .. NELBO: 1696.15\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.54 .. Rec_loss: 1702.82 .. NELBO: 1707.36\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 4.96 .. Rec_loss: 1704.27 .. NELBO: 1709.23\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.56 .. Rec_loss: 1689.77 .. NELBO: 1694.33\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.21 .. Rec_loss: 1702.15 .. NELBO: 1706.36\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 4.64 .. Rec_loss: 1703.38 .. NELBO: 1708.02\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.77 .. Rec_loss: 1690.01 .. NELBO: 1694.78\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.38 .. Rec_loss: 1702.01 .. NELBO: 1706.39\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 4.78 .. Rec_loss: 1703.19 .. NELBO: 1707.97\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.73 .. Rec_loss: 1689.13 .. NELBO: 1693.86\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.43 .. Rec_loss: 1701.26 .. NELBO: 1705.69\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1702.48 .. NELBO: 1707.32\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.83 .. Rec_loss: 1689.0 .. NELBO: 1693.83\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.52 .. Rec_loss: 1700.92 .. NELBO: 1705.44\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 4.94 .. Rec_loss: 1702.16 .. NELBO: 1707.1\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.8 .. Rec_loss: 1688.85 .. NELBO: 1693.65\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.57 .. Rec_loss: 1700.35 .. NELBO: 1704.92\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 4.94 .. Rec_loss: 1701.66 .. NELBO: 1706.6\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1689.13 .. NELBO: 1693.97\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.66 .. Rec_loss: 1700.48 .. NELBO: 1705.14\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 5.04 .. Rec_loss: 1701.7 .. NELBO: 1706.74\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.81 .. Rec_loss: 1688.6 .. NELBO: 1693.41\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.65 .. Rec_loss: 1700.02 .. NELBO: 1704.67\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 5.04 .. Rec_loss: 1701.34 .. NELBO: 1706.38\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.98 .. Rec_loss: 1688.1 .. NELBO: 1693.08\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.71 .. Rec_loss: 1699.62 .. NELBO: 1704.33\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1700.96 .. NELBO: 1706.04\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.96 .. Rec_loss: 1688.09 .. NELBO: 1693.05\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.75 .. Rec_loss: 1699.81 .. NELBO: 1704.56\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1701.22 .. NELBO: 1706.29\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1687.55 .. NELBO: 1692.7\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.88 .. Rec_loss: 1699.09 .. NELBO: 1703.97\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 5.27 .. Rec_loss: 1700.44 .. NELBO: 1705.71\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1687.74 .. NELBO: 1692.81\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.75 .. Rec_loss: 1699.68 .. NELBO: 1704.43\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 5.13 .. Rec_loss: 1700.95 .. NELBO: 1706.08\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.96 .. Rec_loss: 1687.77 .. NELBO: 1692.73\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.77 .. Rec_loss: 1699.69 .. NELBO: 1704.46\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1700.97 .. NELBO: 1706.12\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1688.48 .. NELBO: 1693.48\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.79 .. Rec_loss: 1700.18 .. NELBO: 1704.97\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 5.2 .. Rec_loss: 1701.45 .. NELBO: 1706.65\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.13 .. Rec_loss: 1687.82 .. NELBO: 1692.95\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.85 .. Rec_loss: 1699.95 .. NELBO: 1704.8\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 5.29 .. Rec_loss: 1701.16 .. NELBO: 1706.45\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.04 .. Rec_loss: 1688.39 .. NELBO: 1693.43\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.85 .. Rec_loss: 1700.13 .. NELBO: 1704.98\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1701.46 .. NELBO: 1706.64\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1688.51 .. NELBO: 1693.82\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1700.21 .. NELBO: 1705.11\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 5.29 .. Rec_loss: 1701.32 .. NELBO: 1706.61\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.12 .. Rec_loss: 1688.1 .. NELBO: 1693.22\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.86 .. Rec_loss: 1699.65 .. NELBO: 1704.51\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 5.16 .. Rec_loss: 1701.05 .. NELBO: 1706.21\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1687.49 .. NELBO: 1692.77\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 1699.37 .. NELBO: 1704.28\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1700.75 .. NELBO: 1706.0\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.3 .. Rec_loss: 1687.78 .. NELBO: 1693.08\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.86 .. Rec_loss: 1699.3 .. NELBO: 1704.16\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1700.6 .. NELBO: 1705.83\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1688.29 .. NELBO: 1693.6\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.72 .. Rec_loss: 1699.67 .. NELBO: 1704.39\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1701.02 .. NELBO: 1706.1\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1688.73 .. NELBO: 1693.91\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.73 .. Rec_loss: 1699.97 .. NELBO: 1704.7\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 5.04 .. Rec_loss: 1701.43 .. NELBO: 1706.47\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.12 .. Rec_loss: 1688.6 .. NELBO: 1693.72\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.77 .. Rec_loss: 1699.69 .. NELBO: 1704.46\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 5.04 .. Rec_loss: 1701.22 .. NELBO: 1706.26\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.27 .. Rec_loss: 1688.06 .. NELBO: 1693.33\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.81 .. Rec_loss: 1699.54 .. NELBO: 1704.35\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1701.18 .. NELBO: 1706.21\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.45 .. Rec_loss: 1687.92 .. NELBO: 1693.37\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 1699.63 .. NELBO: 1704.54\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1701.43 .. NELBO: 1706.5\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 1687.85 .. NELBO: 1693.33\n",
      "Epoch: 88 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 1699.58 .. NELBO: 1704.49\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1701.14 .. NELBO: 1706.24\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1688.35 .. NELBO: 1693.66\n",
      "Epoch: 89 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1699.66 .. NELBO: 1704.61\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1701.06 .. NELBO: 1706.23\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.46 .. Rec_loss: 1687.49 .. NELBO: 1692.95\n",
      "Epoch: 90 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.93 .. Rec_loss: 1699.16 .. NELBO: 1704.09\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1700.9 .. NELBO: 1706.08\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1686.96 .. NELBO: 1692.45\n",
      "Epoch: 91 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1698.66 .. NELBO: 1703.63\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1700.22 .. NELBO: 1705.37\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.45 .. Rec_loss: 1686.22 .. NELBO: 1691.67\n",
      "Epoch: 92 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1698.03 .. NELBO: 1703.03\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 5.24 .. Rec_loss: 1699.61 .. NELBO: 1704.85\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1686.69 .. NELBO: 1692.18\n",
      "Epoch: 93 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1698.05 .. NELBO: 1703.11\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 5.27 .. Rec_loss: 1699.67 .. NELBO: 1704.94\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1685.82 .. NELBO: 1691.29\n",
      "Epoch: 94 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.2 .. Rec_loss: 1697.27 .. NELBO: 1702.47\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1698.92 .. NELBO: 1704.34\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 1686.77 .. NELBO: 1691.99\n",
      "Epoch: 95 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.12 .. Rec_loss: 1697.56 .. NELBO: 1702.68\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1699.18 .. NELBO: 1704.54\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.51 .. Rec_loss: 1686.08 .. NELBO: 1691.59\n",
      "Epoch: 96 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.26 .. Rec_loss: 1697.33 .. NELBO: 1702.59\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 5.45 .. Rec_loss: 1699.0 .. NELBO: 1704.45\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.3 .. Rec_loss: 1686.12 .. NELBO: 1691.42\n",
      "Epoch: 97 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1697.0 .. NELBO: 1702.31\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 5.59 .. Rec_loss: 1698.51 .. NELBO: 1704.1\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.37 .. Rec_loss: 1686.01 .. NELBO: 1691.38\n",
      "Epoch: 98 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.29 .. Rec_loss: 1697.32 .. NELBO: 1702.61\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1698.84 .. NELBO: 1704.4\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.57 .. Rec_loss: 1685.49 .. NELBO: 1691.06\n",
      "Epoch: 99 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.46 .. Rec_loss: 1696.92 .. NELBO: 1702.38\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 1698.51 .. NELBO: 1704.19\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1686.28 .. NELBO: 1691.53\n",
      "Epoch: 100 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1697.36 .. NELBO: 1702.64\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1698.7 .. NELBO: 1704.26\n",
      "****************************************************************************************************\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.590628172569627, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 1992.21 .. NELBO: 1992.44\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.43 .. Rec_loss: 1878.56 .. NELBO: 1878.99\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 1867.14 .. NELBO: 1867.62\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1723.12 .. NELBO: 1723.16\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1730.92 .. NELBO: 1730.95\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1734.22 .. NELBO: 1734.25\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1719.81 .. NELBO: 1719.84\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1728.0 .. NELBO: 1728.02\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.19 .. NELBO: 1731.21\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.48 .. NELBO: 1718.48\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.67 .. NELBO: 1726.67\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.82 .. NELBO: 1729.82\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.7 .. NELBO: 1717.7\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.88 .. NELBO: 1725.88\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.02 .. NELBO: 1729.02\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.07 .. NELBO: 1717.07\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.27 .. NELBO: 1725.27\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.42 .. NELBO: 1728.42\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.54 .. NELBO: 1716.54\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.93 .. NELBO: 1727.93\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.06 .. NELBO: 1716.06\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.51 .. NELBO: 1727.51\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.67 .. NELBO: 1715.67\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.0 .. NELBO: 1724.0\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.16 .. NELBO: 1727.16\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.35 .. NELBO: 1715.35\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.71 .. NELBO: 1723.72\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.84 .. NELBO: 1726.85\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1715.1 .. NELBO: 1715.12\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1723.45 .. NELBO: 1723.48\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1726.56 .. NELBO: 1726.59\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1714.66 .. NELBO: 1714.77\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 1722.98 .. NELBO: 1723.12\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 1726.05 .. NELBO: 1726.21\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 1713.94 .. NELBO: 1714.34\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 1722.29 .. NELBO: 1722.65\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 1725.4 .. NELBO: 1725.78\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 1713.42 .. NELBO: 1714.04\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 1721.78 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.58 .. Rec_loss: 1724.84 .. NELBO: 1725.42\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.91 .. Rec_loss: 1712.39 .. NELBO: 1713.3\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.9 .. Rec_loss: 1721.03 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.92 .. Rec_loss: 1723.95 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.33 .. Rec_loss: 1711.14 .. NELBO: 1712.47\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.22 .. Rec_loss: 1720.26 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 1.42 .. Rec_loss: 1722.7 .. NELBO: 1724.12\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.56 .. Rec_loss: 1708.85 .. NELBO: 1710.41\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.25 .. Rec_loss: 1719.09 .. NELBO: 1720.34\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 1.44 .. Rec_loss: 1721.19 .. NELBO: 1722.63\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.1 .. Rec_loss: 1708.01 .. NELBO: 1710.11\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.62 .. Rec_loss: 1717.9 .. NELBO: 1719.52\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 1.84 .. Rec_loss: 1719.58 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.38 .. Rec_loss: 1706.33 .. NELBO: 1708.71\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.82 .. Rec_loss: 1716.74 .. NELBO: 1718.56\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.03 .. Rec_loss: 1718.3 .. NELBO: 1720.33\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1705.21 .. NELBO: 1707.74\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.94 .. Rec_loss: 1715.92 .. NELBO: 1717.86\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.18 .. Rec_loss: 1717.35 .. NELBO: 1719.53\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1704.13 .. NELBO: 1706.63\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.95 .. Rec_loss: 1714.79 .. NELBO: 1716.74\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.23 .. Rec_loss: 1716.15 .. NELBO: 1718.38\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.55 .. Rec_loss: 1702.9 .. NELBO: 1705.45\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.0 .. Rec_loss: 1713.92 .. NELBO: 1715.92\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.3 .. Rec_loss: 1715.22 .. NELBO: 1717.52\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1702.56 .. NELBO: 1705.0\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.98 .. Rec_loss: 1713.38 .. NELBO: 1715.36\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1714.84 .. NELBO: 1717.08\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1701.99 .. NELBO: 1704.86\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.32 .. Rec_loss: 1713.02 .. NELBO: 1715.34\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1714.53 .. NELBO: 1717.03\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1702.54 .. NELBO: 1705.18\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.23 .. Rec_loss: 1713.73 .. NELBO: 1715.96\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1715.03 .. NELBO: 1717.47\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1702.89 .. NELBO: 1705.45\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.27 .. Rec_loss: 1713.79 .. NELBO: 1716.06\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1715.01 .. NELBO: 1717.49\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1702.8 .. NELBO: 1705.4\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 1713.55 .. NELBO: 1715.88\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1714.92 .. NELBO: 1717.41\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1702.97 .. NELBO: 1705.41\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.19 .. Rec_loss: 1713.63 .. NELBO: 1715.82\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.42 .. Rec_loss: 1714.88 .. NELBO: 1717.3\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1702.44 .. NELBO: 1704.88\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.3 .. Rec_loss: 1712.97 .. NELBO: 1715.27\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1714.07 .. NELBO: 1716.66\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.28 .. Rec_loss: 1701.79 .. NELBO: 1704.07\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.23 .. Rec_loss: 1712.32 .. NELBO: 1714.55\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1713.43 .. NELBO: 1716.04\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1700.76 .. NELBO: 1703.26\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.28 .. Rec_loss: 1711.7 .. NELBO: 1713.98\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1712.96 .. NELBO: 1715.61\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1700.21 .. NELBO: 1702.86\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1711.39 .. NELBO: 1713.76\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1712.57 .. NELBO: 1715.27\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1700.64 .. NELBO: 1703.2\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1711.55 .. NELBO: 1713.91\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1712.76 .. NELBO: 1715.41\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1700.67 .. NELBO: 1703.36\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.41 .. Rec_loss: 1711.56 .. NELBO: 1713.97\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1712.8 .. NELBO: 1715.51\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1700.21 .. NELBO: 1702.91\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.45 .. Rec_loss: 1711.03 .. NELBO: 1713.48\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1712.25 .. NELBO: 1714.98\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1700.29 .. NELBO: 1703.16\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 1711.48 .. NELBO: 1713.95\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1712.77 .. NELBO: 1715.47\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1700.66 .. NELBO: 1703.42\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1711.37 .. NELBO: 1713.86\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1712.58 .. NELBO: 1715.34\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1699.33 .. NELBO: 1702.16\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1710.51 .. NELBO: 1713.04\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1711.83 .. NELBO: 1714.68\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1699.77 .. NELBO: 1702.52\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.57 .. Rec_loss: 1710.57 .. NELBO: 1713.14\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1711.93 .. NELBO: 1714.76\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1698.91 .. NELBO: 1701.9\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1710.34 .. NELBO: 1712.96\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 2.94 .. Rec_loss: 1711.6 .. NELBO: 1714.54\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1699.47 .. NELBO: 1702.22\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1710.25 .. NELBO: 1712.85\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1711.43 .. NELBO: 1714.34\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1698.63 .. NELBO: 1701.54\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1709.66 .. NELBO: 1712.36\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1711.08 .. NELBO: 1714.06\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.94 .. Rec_loss: 1698.61 .. NELBO: 1701.55\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1709.29 .. NELBO: 1712.07\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 3.08 .. Rec_loss: 1710.56 .. NELBO: 1713.64\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1697.94 .. NELBO: 1700.98\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1708.83 .. NELBO: 1711.72\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 3.17 .. Rec_loss: 1710.26 .. NELBO: 1713.43\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.19 .. Rec_loss: 1697.25 .. NELBO: 1700.44\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1708.32 .. NELBO: 1711.36\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 3.29 .. Rec_loss: 1709.72 .. NELBO: 1713.01\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.4 .. Rec_loss: 1696.2 .. NELBO: 1699.6\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.19 .. Rec_loss: 1707.62 .. NELBO: 1710.81\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 3.45 .. Rec_loss: 1709.03 .. NELBO: 1712.48\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.48 .. Rec_loss: 1695.77 .. NELBO: 1699.25\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.29 .. Rec_loss: 1706.95 .. NELBO: 1710.24\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 3.66 .. Rec_loss: 1708.3 .. NELBO: 1711.96\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.76 .. Rec_loss: 1694.8 .. NELBO: 1698.56\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.32 .. Rec_loss: 1706.84 .. NELBO: 1710.16\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 3.62 .. Rec_loss: 1708.26 .. NELBO: 1711.88\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.88 .. Rec_loss: 1694.15 .. NELBO: 1698.03\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.58 .. Rec_loss: 1705.82 .. NELBO: 1709.4\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 3.88 .. Rec_loss: 1707.33 .. NELBO: 1711.21\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.03 .. Rec_loss: 1693.26 .. NELBO: 1697.29\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.57 .. Rec_loss: 1705.52 .. NELBO: 1709.09\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 3.89 .. Rec_loss: 1707.17 .. NELBO: 1711.06\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.9 .. Rec_loss: 1692.72 .. NELBO: 1696.62\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.58 .. Rec_loss: 1704.66 .. NELBO: 1708.24\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 3.88 .. Rec_loss: 1706.29 .. NELBO: 1710.17\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.17 .. Rec_loss: 1692.37 .. NELBO: 1696.54\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.76 .. Rec_loss: 1704.56 .. NELBO: 1708.32\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 4.04 .. Rec_loss: 1706.18 .. NELBO: 1710.22\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.98 .. Rec_loss: 1692.88 .. NELBO: 1696.86\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.69 .. Rec_loss: 1704.56 .. NELBO: 1708.25\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 4.08 .. Rec_loss: 1706.04 .. NELBO: 1710.12\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.16 .. Rec_loss: 1692.44 .. NELBO: 1696.6\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.85 .. Rec_loss: 1704.13 .. NELBO: 1707.98\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 1705.88 .. NELBO: 1710.07\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.29 .. Rec_loss: 1691.55 .. NELBO: 1695.84\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.94 .. Rec_loss: 1703.46 .. NELBO: 1707.4\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 4.28 .. Rec_loss: 1705.25 .. NELBO: 1709.53\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.39 .. Rec_loss: 1691.37 .. NELBO: 1695.76\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.03 .. Rec_loss: 1703.03 .. NELBO: 1707.06\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 4.26 .. Rec_loss: 1705.1 .. NELBO: 1709.36\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1691.15 .. NELBO: 1695.6\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.94 .. Rec_loss: 1703.36 .. NELBO: 1707.3\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 4.3 .. Rec_loss: 1705.03 .. NELBO: 1709.33\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.35 .. Rec_loss: 1691.44 .. NELBO: 1695.79\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.94 .. Rec_loss: 1703.19 .. NELBO: 1707.13\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 4.25 .. Rec_loss: 1705.08 .. NELBO: 1709.33\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.63 .. Rec_loss: 1691.38 .. NELBO: 1696.01\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.08 .. Rec_loss: 1703.11 .. NELBO: 1707.19\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 4.39 .. Rec_loss: 1704.75 .. NELBO: 1709.14\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.58 .. Rec_loss: 1691.76 .. NELBO: 1696.34\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.92 .. Rec_loss: 1704.14 .. NELBO: 1708.06\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 4.22 .. Rec_loss: 1705.92 .. NELBO: 1710.14\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.73 .. Rec_loss: 1692.14 .. NELBO: 1696.87\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.03 .. Rec_loss: 1704.75 .. NELBO: 1708.78\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 4.35 .. Rec_loss: 1706.74 .. NELBO: 1711.09\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.47 .. Rec_loss: 1694.45 .. NELBO: 1698.92\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.89 .. Rec_loss: 1707.15 .. NELBO: 1711.04\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1708.86 .. NELBO: 1713.0\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.55 .. Rec_loss: 1697.04 .. NELBO: 1701.59\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.84 .. Rec_loss: 1710.48 .. NELBO: 1714.32\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 4.04 .. Rec_loss: 1712.19 .. NELBO: 1716.23\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.18 .. Rec_loss: 1695.88 .. NELBO: 1700.06\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.74 .. Rec_loss: 1709.83 .. NELBO: 1713.57\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 3.96 .. Rec_loss: 1711.05 .. NELBO: 1715.01\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.25 .. Rec_loss: 1696.83 .. NELBO: 1701.08\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.83 .. Rec_loss: 1710.24 .. NELBO: 1714.07\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 4.17 .. Rec_loss: 1711.96 .. NELBO: 1716.13\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.47 .. Rec_loss: 1696.5 .. NELBO: 1699.97\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1708.57 .. NELBO: 1711.85\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 3.65 .. Rec_loss: 1710.0 .. NELBO: 1713.65\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.34 .. Rec_loss: 1694.44 .. NELBO: 1698.78\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.78 .. Rec_loss: 1706.89 .. NELBO: 1710.67\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 4.03 .. Rec_loss: 1708.71 .. NELBO: 1712.74\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.59 .. Rec_loss: 1693.73 .. NELBO: 1698.32\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.97 .. Rec_loss: 1706.55 .. NELBO: 1710.52\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 4.21 .. Rec_loss: 1708.1 .. NELBO: 1712.31\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.47 .. Rec_loss: 1693.59 .. NELBO: 1698.06\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.91 .. Rec_loss: 1705.99 .. NELBO: 1709.9\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 4.18 .. Rec_loss: 1707.56 .. NELBO: 1711.74\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.61 .. Rec_loss: 1693.36 .. NELBO: 1697.97\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.96 .. Rec_loss: 1705.83 .. NELBO: 1709.79\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 4.2 .. Rec_loss: 1707.2 .. NELBO: 1711.4\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.61 .. Rec_loss: 1693.08 .. NELBO: 1697.69\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.04 .. Rec_loss: 1705.24 .. NELBO: 1709.28\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 4.27 .. Rec_loss: 1706.67 .. NELBO: 1710.94\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.5 .. Rec_loss: 1692.8 .. NELBO: 1697.3\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.04 .. Rec_loss: 1704.65 .. NELBO: 1708.69\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 4.21 .. Rec_loss: 1706.3 .. NELBO: 1710.51\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.56 .. Rec_loss: 1692.84 .. NELBO: 1697.4\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.07 .. Rec_loss: 1704.58 .. NELBO: 1708.65\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 4.24 .. Rec_loss: 1706.11 .. NELBO: 1710.35\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.87 .. Rec_loss: 1691.64 .. NELBO: 1696.51\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.38 .. Rec_loss: 1703.73 .. NELBO: 1708.11\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 4.49 .. Rec_loss: 1705.57 .. NELBO: 1710.06\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.65 .. Rec_loss: 1691.88 .. NELBO: 1696.53\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.31 .. Rec_loss: 1703.38 .. NELBO: 1707.69\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 4.48 .. Rec_loss: 1705.03 .. NELBO: 1709.51\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.61 .. Rec_loss: 1691.31 .. NELBO: 1695.92\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.32 .. Rec_loss: 1703.23 .. NELBO: 1707.55\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 4.53 .. Rec_loss: 1704.77 .. NELBO: 1709.3\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.56 .. Rec_loss: 1691.63 .. NELBO: 1696.19\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.41 .. Rec_loss: 1703.23 .. NELBO: 1707.64\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 4.57 .. Rec_loss: 1704.79 .. NELBO: 1709.36\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.74 .. Rec_loss: 1691.4 .. NELBO: 1696.14\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.47 .. Rec_loss: 1703.09 .. NELBO: 1707.56\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 4.67 .. Rec_loss: 1704.64 .. NELBO: 1709.31\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.63 .. Rec_loss: 1691.31 .. NELBO: 1695.94\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.67 .. Rec_loss: 1702.67 .. NELBO: 1707.34\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1704.04 .. NELBO: 1708.88\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.29 .. Rec_loss: 1692.31 .. NELBO: 1696.6\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.53 .. Rec_loss: 1702.97 .. NELBO: 1707.5\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 4.74 .. Rec_loss: 1704.37 .. NELBO: 1709.11\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.47 .. Rec_loss: 1691.41 .. NELBO: 1695.88\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.6 .. Rec_loss: 1702.99 .. NELBO: 1707.59\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 4.88 .. Rec_loss: 1704.4 .. NELBO: 1709.28\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.55 .. Rec_loss: 1691.73 .. NELBO: 1696.28\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.75 .. Rec_loss: 1703.04 .. NELBO: 1707.79\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1704.44 .. NELBO: 1709.51\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 1691.1 .. NELBO: 1695.56\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.58 .. Rec_loss: 1702.43 .. NELBO: 1707.01\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1703.62 .. NELBO: 1708.65\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.6 .. Rec_loss: 1690.21 .. NELBO: 1694.81\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.58 .. Rec_loss: 1702.36 .. NELBO: 1706.94\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1703.61 .. NELBO: 1708.61\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.55 .. Rec_loss: 1690.5 .. NELBO: 1695.05\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.53 .. Rec_loss: 1702.22 .. NELBO: 1706.75\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1703.42 .. NELBO: 1708.39\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.61 .. Rec_loss: 1690.2 .. NELBO: 1694.81\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.43 .. Rec_loss: 1702.25 .. NELBO: 1706.68\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 4.92 .. Rec_loss: 1703.36 .. NELBO: 1708.28\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 1689.92 .. NELBO: 1694.62\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.47 .. Rec_loss: 1701.88 .. NELBO: 1706.35\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 4.82 .. Rec_loss: 1702.99 .. NELBO: 1707.81\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.79 .. Rec_loss: 1690.05 .. NELBO: 1694.84\n",
      "Epoch: 88 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.47 .. Rec_loss: 1702.2 .. NELBO: 1706.67\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 4.83 .. Rec_loss: 1703.25 .. NELBO: 1708.08\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1689.44 .. NELBO: 1694.52\n",
      "Epoch: 89 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.49 .. Rec_loss: 1701.81 .. NELBO: 1706.3\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 4.89 .. Rec_loss: 1702.75 .. NELBO: 1707.64\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.94 .. Rec_loss: 1688.68 .. NELBO: 1693.62\n",
      "Epoch: 90 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.52 .. Rec_loss: 1700.43 .. NELBO: 1704.95\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 1701.57 .. NELBO: 1706.48\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.94 .. Rec_loss: 1688.68 .. NELBO: 1693.62\n",
      "Epoch: 91 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.54 .. Rec_loss: 1700.41 .. NELBO: 1704.95\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1701.81 .. NELBO: 1706.78\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1688.77 .. NELBO: 1693.92\n",
      "Epoch: 92 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.6 .. Rec_loss: 1700.44 .. NELBO: 1705.04\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 4.89 .. Rec_loss: 1701.72 .. NELBO: 1706.61\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1687.48 .. NELBO: 1692.63\n",
      "Epoch: 93 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 1699.39 .. NELBO: 1704.09\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 5.09 .. Rec_loss: 1700.6 .. NELBO: 1705.69\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.4 .. Rec_loss: 1687.5 .. NELBO: 1692.9\n",
      "Epoch: 94 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.71 .. Rec_loss: 1699.71 .. NELBO: 1704.42\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 5.05 .. Rec_loss: 1700.95 .. NELBO: 1706.0\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.39 .. Rec_loss: 1686.99 .. NELBO: 1692.38\n",
      "Epoch: 95 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.69 .. Rec_loss: 1699.23 .. NELBO: 1703.92\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 5.04 .. Rec_loss: 1700.48 .. NELBO: 1705.52\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1686.84 .. NELBO: 1692.2\n",
      "Epoch: 96 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 1698.79 .. NELBO: 1703.49\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 5.02 .. Rec_loss: 1700.18 .. NELBO: 1705.2\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 1686.56 .. NELBO: 1692.04\n",
      "Epoch: 97 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.75 .. Rec_loss: 1698.84 .. NELBO: 1703.59\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1700.16 .. NELBO: 1705.22\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.7 .. Rec_loss: 1686.74 .. NELBO: 1692.44\n",
      "Epoch: 98 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.88 .. Rec_loss: 1698.91 .. NELBO: 1703.79\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 5.12 .. Rec_loss: 1700.29 .. NELBO: 1705.41\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 1686.97 .. NELBO: 1692.68\n",
      "Epoch: 99 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.87 .. Rec_loss: 1699.16 .. NELBO: 1704.03\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 5.16 .. Rec_loss: 1700.35 .. NELBO: 1705.51\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.6 .. Rec_loss: 1687.66 .. NELBO: 1693.26\n",
      "Epoch: 100 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.89 .. Rec_loss: 1699.11 .. NELBO: 1704.0\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1700.58 .. NELBO: 1705.68\n",
      "****************************************************************************************************\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.590628172569627, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.82 .. Rec_loss: 1994.15 .. NELBO: 1994.97\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.75 .. Rec_loss: 1878.59 .. NELBO: 1879.34\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.82 .. Rec_loss: 1867.2 .. NELBO: 1868.02\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1723.01 .. NELBO: 1723.08\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.9 .. NELBO: 1730.95\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.21 .. NELBO: 1734.26\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1720.0 .. NELBO: 1720.07\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1728.12 .. NELBO: 1728.18\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1731.34 .. NELBO: 1731.4\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1718.52 .. NELBO: 1718.54\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1726.72 .. NELBO: 1726.74\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1729.88 .. NELBO: 1729.9\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.75 .. NELBO: 1717.76\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.92 .. NELBO: 1725.93\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.06 .. NELBO: 1729.07\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.13 .. NELBO: 1717.14\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.28 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1728.43 .. NELBO: 1728.44\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1716.47 .. NELBO: 1716.5\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1724.69 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1727.83 .. NELBO: 1727.88\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 1715.58 .. NELBO: 1715.77\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 1723.75 .. NELBO: 1723.98\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 1726.81 .. NELBO: 1727.08\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 1714.11 .. NELBO: 1714.76\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 1722.47 .. NELBO: 1723.07\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1725.49 .. NELBO: 1726.13\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.86 .. Rec_loss: 1713.16 .. NELBO: 1714.02\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.85 .. Rec_loss: 1721.54 .. NELBO: 1722.39\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.9 .. Rec_loss: 1724.47 .. NELBO: 1725.37\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.53 .. Rec_loss: 1711.26 .. NELBO: 1712.79\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.34 .. Rec_loss: 1720.15 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.46 .. Rec_loss: 1722.51 .. NELBO: 1723.97\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.27 .. Rec_loss: 1708.86 .. NELBO: 1711.13\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.69 .. Rec_loss: 1719.06 .. NELBO: 1720.75\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 1.86 .. Rec_loss: 1721.1 .. NELBO: 1722.96\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 1706.05 .. NELBO: 1708.39\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.67 .. Rec_loss: 1717.06 .. NELBO: 1718.73\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 1.83 .. Rec_loss: 1718.91 .. NELBO: 1720.74\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1705.17 .. NELBO: 1707.86\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.97 .. Rec_loss: 1716.01 .. NELBO: 1717.98\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.12 .. Rec_loss: 1717.71 .. NELBO: 1719.83\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1704.9 .. NELBO: 1707.8\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.19 .. Rec_loss: 1715.4 .. NELBO: 1717.59\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.32 .. Rec_loss: 1717.08 .. NELBO: 1719.4\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1703.88 .. NELBO: 1706.6\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.18 .. Rec_loss: 1714.55 .. NELBO: 1716.73\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1716.13 .. NELBO: 1718.5\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1702.97 .. NELBO: 1705.75\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.28 .. Rec_loss: 1713.46 .. NELBO: 1715.74\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1714.85 .. NELBO: 1717.47\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1702.43 .. NELBO: 1704.87\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.19 .. Rec_loss: 1712.67 .. NELBO: 1714.86\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1714.21 .. NELBO: 1716.69\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1701.19 .. NELBO: 1703.88\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.26 .. Rec_loss: 1712.07 .. NELBO: 1714.33\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1713.59 .. NELBO: 1716.13\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1701.04 .. NELBO: 1703.63\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.22 .. Rec_loss: 1711.96 .. NELBO: 1714.18\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1713.37 .. NELBO: 1715.87\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1700.39 .. NELBO: 1703.08\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.32 .. Rec_loss: 1711.53 .. NELBO: 1713.85\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.55 .. Rec_loss: 1713.11 .. NELBO: 1715.66\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1701.43 .. NELBO: 1704.31\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1712.19 .. NELBO: 1714.73\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1713.73 .. NELBO: 1716.41\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1702.33 .. NELBO: 1705.12\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1713.28 .. NELBO: 1715.81\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1714.78 .. NELBO: 1717.52\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1702.83 .. NELBO: 1705.27\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1712.98 .. NELBO: 1715.22\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1714.19 .. NELBO: 1716.75\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1701.6 .. NELBO: 1704.14\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.23 .. Rec_loss: 1712.19 .. NELBO: 1714.42\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1713.44 .. NELBO: 1715.97\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1701.1 .. NELBO: 1703.89\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 1711.81 .. NELBO: 1714.24\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1713.14 .. NELBO: 1715.81\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.94 .. Rec_loss: 1701.19 .. NELBO: 1704.13\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1711.62 .. NELBO: 1714.15\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1712.97 .. NELBO: 1715.7\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1700.43 .. NELBO: 1703.39\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1711.1 .. NELBO: 1713.73\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.94 .. Rec_loss: 1712.25 .. NELBO: 1715.19\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1699.5 .. NELBO: 1702.52\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1710.01 .. NELBO: 1712.73\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1711.2 .. NELBO: 1714.24\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1698.36 .. NELBO: 1701.46\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1709.05 .. NELBO: 1711.77\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1710.2 .. NELBO: 1713.25\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.38 .. Rec_loss: 1697.08 .. NELBO: 1700.46\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1708.1 .. NELBO: 1711.1\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.3 .. Rec_loss: 1709.12 .. NELBO: 1712.42\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.75 .. Rec_loss: 1696.12 .. NELBO: 1699.87\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.36 .. Rec_loss: 1707.05 .. NELBO: 1710.41\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1708.35 .. NELBO: 1711.91\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.13 .. Rec_loss: 1695.25 .. NELBO: 1699.38\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.71 .. Rec_loss: 1706.42 .. NELBO: 1710.13\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.83 .. Rec_loss: 1707.78 .. NELBO: 1711.61\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.49 .. Rec_loss: 1694.04 .. NELBO: 1698.53\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.04 .. Rec_loss: 1705.14 .. NELBO: 1709.18\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1706.75 .. NELBO: 1710.89\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.61 .. Rec_loss: 1692.62 .. NELBO: 1697.23\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.17 .. Rec_loss: 1704.13 .. NELBO: 1708.3\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 4.3 .. Rec_loss: 1705.78 .. NELBO: 1710.08\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.56 .. Rec_loss: 1691.72 .. NELBO: 1696.28\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.26 .. Rec_loss: 1702.96 .. NELBO: 1707.22\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 1704.47 .. NELBO: 1708.93\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.33 .. Rec_loss: 1692.34 .. NELBO: 1696.67\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.17 .. Rec_loss: 1702.98 .. NELBO: 1707.15\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1704.45 .. NELBO: 1708.87\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 1690.91 .. NELBO: 1695.37\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.31 .. Rec_loss: 1702.1 .. NELBO: 1706.41\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 4.54 .. Rec_loss: 1703.55 .. NELBO: 1708.09\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.44 .. Rec_loss: 1691.44 .. NELBO: 1695.88\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.24 .. Rec_loss: 1702.56 .. NELBO: 1706.8\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 4.49 .. Rec_loss: 1703.94 .. NELBO: 1708.43\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.5 .. Rec_loss: 1692.35 .. NELBO: 1696.85\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.4 .. Rec_loss: 1702.85 .. NELBO: 1707.25\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 4.56 .. Rec_loss: 1704.3 .. NELBO: 1708.86\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 1691.77 .. NELBO: 1696.23\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1702.38 .. NELBO: 1706.83\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4.68 .. Rec_loss: 1703.72 .. NELBO: 1708.4\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.58 .. Rec_loss: 1691.16 .. NELBO: 1695.74\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.48 .. Rec_loss: 1702.56 .. NELBO: 1707.04\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4.77 .. Rec_loss: 1703.89 .. NELBO: 1708.66\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1691.7 .. NELBO: 1696.12\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.39 .. Rec_loss: 1702.51 .. NELBO: 1706.9\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 4.74 .. Rec_loss: 1703.9 .. NELBO: 1708.64\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.58 .. Rec_loss: 1691.02 .. NELBO: 1695.6\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.2 .. Rec_loss: 1702.42 .. NELBO: 1706.62\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 4.54 .. Rec_loss: 1704.02 .. NELBO: 1708.56\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.83 .. Rec_loss: 1691.1 .. NELBO: 1695.93\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.3 .. Rec_loss: 1703.05 .. NELBO: 1707.35\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.66 .. Rec_loss: 1703.95 .. NELBO: 1708.61\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.09 .. Rec_loss: 1689.93 .. NELBO: 1695.02\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.54 .. Rec_loss: 1701.54 .. NELBO: 1706.08\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1702.69 .. NELBO: 1707.64\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.76 .. Rec_loss: 1689.7 .. NELBO: 1694.46\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 1701.12 .. NELBO: 1705.58\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 4.87 .. Rec_loss: 1702.23 .. NELBO: 1707.1\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.83 .. Rec_loss: 1688.99 .. NELBO: 1693.82\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.53 .. Rec_loss: 1700.43 .. NELBO: 1704.96\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 4.92 .. Rec_loss: 1701.47 .. NELBO: 1706.39\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.82 .. Rec_loss: 1689.37 .. NELBO: 1694.19\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.56 .. Rec_loss: 1700.41 .. NELBO: 1704.97\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 4.89 .. Rec_loss: 1701.67 .. NELBO: 1706.56\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1688.02 .. NELBO: 1692.92\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.63 .. Rec_loss: 1699.75 .. NELBO: 1704.38\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1700.88 .. NELBO: 1705.78\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.88 .. Rec_loss: 1687.96 .. NELBO: 1692.84\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.72 .. Rec_loss: 1699.48 .. NELBO: 1704.2\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 5.09 .. Rec_loss: 1700.6 .. NELBO: 1705.69\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.99 .. Rec_loss: 1687.98 .. NELBO: 1692.97\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.89 .. Rec_loss: 1699.15 .. NELBO: 1704.04\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 5.21 .. Rec_loss: 1700.43 .. NELBO: 1705.64\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.88 .. Rec_loss: 1687.6 .. NELBO: 1692.48\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.75 .. Rec_loss: 1699.05 .. NELBO: 1703.8\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1700.29 .. NELBO: 1705.47\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.74 .. Rec_loss: 1687.51 .. NELBO: 1692.25\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 1698.97 .. NELBO: 1703.67\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 5.14 .. Rec_loss: 1700.05 .. NELBO: 1705.19\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1686.89 .. NELBO: 1691.96\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1698.36 .. NELBO: 1703.26\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1699.61 .. NELBO: 1704.78\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.92 .. Rec_loss: 1687.29 .. NELBO: 1692.21\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 1698.47 .. NELBO: 1703.38\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1699.57 .. NELBO: 1704.88\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.04 .. Rec_loss: 1686.99 .. NELBO: 1692.03\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1698.13 .. NELBO: 1703.1\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1699.5 .. NELBO: 1704.73\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.39 .. Rec_loss: 1686.72 .. NELBO: 1692.11\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.11 .. Rec_loss: 1698.14 .. NELBO: 1703.25\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 5.41 .. Rec_loss: 1699.49 .. NELBO: 1704.9\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 1686.37 .. NELBO: 1691.59\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.05 .. Rec_loss: 1697.71 .. NELBO: 1702.76\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 5.37 .. Rec_loss: 1699.03 .. NELBO: 1704.4\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.19 .. Rec_loss: 1686.57 .. NELBO: 1691.76\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1698.01 .. NELBO: 1703.07\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 5.37 .. Rec_loss: 1699.25 .. NELBO: 1704.62\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1686.09 .. NELBO: 1691.26\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1697.52 .. NELBO: 1702.59\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 5.39 .. Rec_loss: 1698.89 .. NELBO: 1704.28\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1685.47 .. NELBO: 1690.7\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1697.18 .. NELBO: 1702.26\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1698.47 .. NELBO: 1703.94\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1685.9 .. NELBO: 1691.07\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1697.2 .. NELBO: 1702.38\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1698.68 .. NELBO: 1704.17\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.45 .. Rec_loss: 1685.65 .. NELBO: 1691.1\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1696.98 .. NELBO: 1702.26\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 5.55 .. Rec_loss: 1698.63 .. NELBO: 1704.18\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.52 .. Rec_loss: 1685.31 .. NELBO: 1690.83\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.35 .. Rec_loss: 1696.8 .. NELBO: 1702.15\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 1698.24 .. NELBO: 1703.92\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1685.49 .. NELBO: 1690.91\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1696.91 .. NELBO: 1702.16\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 5.65 .. Rec_loss: 1698.28 .. NELBO: 1703.93\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.29 .. Rec_loss: 1685.62 .. NELBO: 1690.91\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.16 .. Rec_loss: 1696.85 .. NELBO: 1702.01\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1698.38 .. NELBO: 1703.85\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 1685.34 .. NELBO: 1690.82\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1696.7 .. NELBO: 1701.93\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1698.34 .. NELBO: 1703.83\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 1684.59 .. NELBO: 1690.59\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.46 .. Rec_loss: 1696.41 .. NELBO: 1701.87\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 5.74 .. Rec_loss: 1697.99 .. NELBO: 1703.73\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 1684.94 .. NELBO: 1690.42\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1696.5 .. NELBO: 1701.67\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 5.52 .. Rec_loss: 1697.96 .. NELBO: 1703.48\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1684.55 .. NELBO: 1690.13\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1696.57 .. NELBO: 1701.82\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 5.6 .. Rec_loss: 1698.03 .. NELBO: 1703.63\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.7 .. Rec_loss: 1684.68 .. NELBO: 1690.38\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1696.5 .. NELBO: 1701.86\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 5.64 .. Rec_loss: 1698.03 .. NELBO: 1703.67\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.76 .. Rec_loss: 1684.97 .. NELBO: 1690.73\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.19 .. Rec_loss: 1697.3 .. NELBO: 1702.49\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 5.51 .. Rec_loss: 1698.84 .. NELBO: 1704.35\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1684.94 .. NELBO: 1690.66\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.13 .. Rec_loss: 1697.15 .. NELBO: 1702.28\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 5.41 .. Rec_loss: 1698.82 .. NELBO: 1704.23\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.88 .. Rec_loss: 1686.88 .. NELBO: 1692.76\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.16 .. Rec_loss: 1699.46 .. NELBO: 1704.62\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1701.05 .. NELBO: 1706.47\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.11 .. Rec_loss: 1690.11 .. NELBO: 1695.22\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.82 .. Rec_loss: 1700.89 .. NELBO: 1705.71\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 5.34 .. Rec_loss: 1702.04 .. NELBO: 1707.38\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1688.08 .. NELBO: 1693.55\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.01 .. Rec_loss: 1699.7 .. NELBO: 1704.71\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 5.38 .. Rec_loss: 1701.21 .. NELBO: 1706.59\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.74 .. Rec_loss: 1687.65 .. NELBO: 1693.39\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1700.24 .. NELBO: 1705.47\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 5.41 .. Rec_loss: 1702.01 .. NELBO: 1707.42\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.61 .. Rec_loss: 1689.79 .. NELBO: 1695.4\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1701.55 .. NELBO: 1706.63\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 5.38 .. Rec_loss: 1702.88 .. NELBO: 1708.26\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.55 .. Rec_loss: 1690.08 .. NELBO: 1695.63\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1701.65 .. NELBO: 1706.83\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 5.37 .. Rec_loss: 1703.34 .. NELBO: 1708.71\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1691.19 .. NELBO: 1696.63\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.93 .. Rec_loss: 1701.67 .. NELBO: 1706.6\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 5.21 .. Rec_loss: 1703.34 .. NELBO: 1708.55\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1689.88 .. NELBO: 1695.3\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1700.71 .. NELBO: 1705.74\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1702.43 .. NELBO: 1707.66\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.7 .. Rec_loss: 1689.43 .. NELBO: 1695.13\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.09 .. Rec_loss: 1700.6 .. NELBO: 1705.69\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 5.33 .. Rec_loss: 1702.04 .. NELBO: 1707.37\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.46 .. Rec_loss: 1688.59 .. NELBO: 1694.05\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1700.01 .. NELBO: 1705.08\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 5.24 .. Rec_loss: 1701.51 .. NELBO: 1706.75\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.81 .. Rec_loss: 1687.06 .. NELBO: 1692.87\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1699.1 .. NELBO: 1704.52\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1700.62 .. NELBO: 1706.2\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 1687.16 .. NELBO: 1692.84\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.34 .. Rec_loss: 1698.8 .. NELBO: 1704.14\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 5.53 .. Rec_loss: 1700.16 .. NELBO: 1705.69\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1685.26 .. NELBO: 1691.16\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1697.26 .. NELBO: 1702.68\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1698.77 .. NELBO: 1704.39\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.6 .. Rec_loss: 1685.83 .. NELBO: 1691.43\n",
      "Epoch: 88 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.55 .. Rec_loss: 1697.14 .. NELBO: 1702.69\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1698.6 .. NELBO: 1704.4\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1685.01 .. NELBO: 1690.63\n",
      "Epoch: 89 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1696.61 .. NELBO: 1702.1\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1698.14 .. NELBO: 1703.86\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.52 .. Rec_loss: 1685.86 .. NELBO: 1691.38\n",
      "Epoch: 90 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.51 .. Rec_loss: 1696.99 .. NELBO: 1702.5\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1698.34 .. NELBO: 1704.18\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.65 .. Rec_loss: 1685.43 .. NELBO: 1691.08\n",
      "Epoch: 91 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1696.74 .. NELBO: 1702.3\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 5.79 .. Rec_loss: 1698.22 .. NELBO: 1704.01\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.57 .. Rec_loss: 1685.44 .. NELBO: 1691.01\n",
      "Epoch: 92 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1696.47 .. NELBO: 1702.03\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1697.84 .. NELBO: 1703.74\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.59 .. Rec_loss: 1685.09 .. NELBO: 1690.68\n",
      "Epoch: 93 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1696.26 .. NELBO: 1701.88\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 5.86 .. Rec_loss: 1697.74 .. NELBO: 1703.6\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.6 .. Rec_loss: 1685.06 .. NELBO: 1690.66\n",
      "Epoch: 94 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 1696.24 .. NELBO: 1701.92\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1697.58 .. NELBO: 1703.54\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.51 .. Rec_loss: 1684.7 .. NELBO: 1690.21\n",
      "Epoch: 95 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.53 .. Rec_loss: 1695.9 .. NELBO: 1701.43\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 5.81 .. Rec_loss: 1697.37 .. NELBO: 1703.18\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.64 .. Rec_loss: 1684.84 .. NELBO: 1690.48\n",
      "Epoch: 96 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.55 .. Rec_loss: 1696.18 .. NELBO: 1701.73\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 5.85 .. Rec_loss: 1697.55 .. NELBO: 1703.4\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 1684.46 .. NELBO: 1690.21\n",
      "Epoch: 97 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1695.98 .. NELBO: 1701.56\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1697.49 .. NELBO: 1703.33\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.81 .. Rec_loss: 1684.01 .. NELBO: 1689.82\n",
      "Epoch: 98 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1695.52 .. NELBO: 1701.14\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 5.95 .. Rec_loss: 1696.98 .. NELBO: 1702.93\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.66 .. Rec_loss: 1684.51 .. NELBO: 1690.17\n",
      "Epoch: 99 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.55 .. Rec_loss: 1696.0 .. NELBO: 1701.55\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 5.86 .. Rec_loss: 1697.43 .. NELBO: 1703.29\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.78 .. Rec_loss: 1684.07 .. NELBO: 1689.85\n",
      "Epoch: 100 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1695.75 .. NELBO: 1701.29\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 5.86 .. Rec_loss: 1697.18 .. NELBO: 1703.04\n",
      "****************************************************************************************************\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.590628172569627, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.43 .. Rec_loss: 1993.04 .. NELBO: 1993.47\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.44 .. Rec_loss: 1878.94 .. NELBO: 1879.38\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 1867.43 .. NELBO: 1867.93\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1722.96 .. NELBO: 1723.03\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.79 .. NELBO: 1730.84\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.1 .. NELBO: 1734.15\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1719.95 .. NELBO: 1720.01\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1728.08 .. NELBO: 1728.13\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1731.27 .. NELBO: 1731.32\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1718.53 .. NELBO: 1718.56\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1726.71 .. NELBO: 1726.74\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1729.86 .. NELBO: 1729.88\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.76 .. NELBO: 1717.77\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.92 .. NELBO: 1725.93\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.06 .. NELBO: 1729.06\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.13 .. NELBO: 1717.13\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.31 .. NELBO: 1725.31\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.45 .. NELBO: 1728.45\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.56 .. NELBO: 1716.56\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.95 .. NELBO: 1727.95\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.08 .. NELBO: 1716.08\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.38 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.53 .. NELBO: 1727.53\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.7 .. NELBO: 1715.7\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.02 .. NELBO: 1724.02\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.17 .. NELBO: 1727.17\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.39 .. NELBO: 1715.39\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.74 .. NELBO: 1723.74\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.87 .. NELBO: 1726.87\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.14 .. NELBO: 1715.14\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.51 .. NELBO: 1723.51\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.63 .. NELBO: 1726.63\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.96 .. NELBO: 1714.96\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.31 .. NELBO: 1723.31\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.43 .. NELBO: 1726.43\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.8 .. NELBO: 1714.8\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.16 .. NELBO: 1723.16\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.28 .. NELBO: 1726.28\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.63 .. NELBO: 1714.63\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.04 .. NELBO: 1723.04\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.13 .. NELBO: 1726.13\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.54 .. NELBO: 1714.54\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.92 .. NELBO: 1722.92\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.01 .. NELBO: 1726.01\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.45 .. NELBO: 1714.45\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.84 .. NELBO: 1722.84\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.92 .. NELBO: 1725.92\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.31 .. NELBO: 1714.31\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.7 .. NELBO: 1722.7\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.77 .. NELBO: 1725.77\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.21 .. NELBO: 1714.21\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.57 .. NELBO: 1722.57\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.64 .. NELBO: 1725.64\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.15 .. NELBO: 1714.15\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.53 .. NELBO: 1722.53\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.6 .. NELBO: 1725.6\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.99 .. NELBO: 1713.99\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.43 .. NELBO: 1722.43\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.49 .. NELBO: 1725.49\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.0 .. NELBO: 1714.0\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.38 .. NELBO: 1722.38\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.45 .. NELBO: 1725.45\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.9 .. NELBO: 1713.9\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.33 .. NELBO: 1722.33\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.38 .. NELBO: 1725.38\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.89 .. NELBO: 1713.89\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.28 .. NELBO: 1722.28\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.34 .. NELBO: 1725.34\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.82 .. NELBO: 1713.82\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.21 .. NELBO: 1722.21\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.67 .. NELBO: 1713.67\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.14 .. NELBO: 1725.14\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.58 .. NELBO: 1713.58\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.02 .. NELBO: 1722.02\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.99 .. NELBO: 1724.99\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.77 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.33 .. NELBO: 1713.33\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.37 .. NELBO: 1713.37\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.79 .. NELBO: 1724.79\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.37 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.33 .. NELBO: 1721.33\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.34 .. NELBO: 1721.34\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.39 .. NELBO: 1724.39\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.74 .. NELBO: 1712.74\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.29 .. NELBO: 1721.29\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.33 .. NELBO: 1721.33\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.81 .. NELBO: 1712.81\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.28 .. NELBO: 1721.28\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.76 .. NELBO: 1712.76\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.32 .. NELBO: 1721.32\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.27 .. NELBO: 1721.27\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.67 .. NELBO: 1712.67\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.3 .. NELBO: 1721.3\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.35 .. NELBO: 1721.35\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.39 .. NELBO: 1724.39\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.27 .. NELBO: 1721.27\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.75 .. NELBO: 1712.75\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.27 .. NELBO: 1721.27\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.7 .. NELBO: 1712.7\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.28 .. NELBO: 1721.28\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.26 .. NELBO: 1721.26\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.29 .. NELBO: 1724.29\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.76 .. NELBO: 1712.76\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.23 .. NELBO: 1721.23\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.29 .. NELBO: 1724.29\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.64 .. NELBO: 1712.64\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.2 .. NELBO: 1721.2\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.23 .. NELBO: 1724.23\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.7 .. NELBO: 1712.7\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.16 .. NELBO: 1721.16\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.19 .. NELBO: 1724.19\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.71 .. NELBO: 1712.71\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.16 .. NELBO: 1721.16\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.22 .. NELBO: 1724.22\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.61 .. NELBO: 1712.61\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.19 .. NELBO: 1721.19\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.22 .. NELBO: 1724.22\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.62 .. NELBO: 1712.62\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.16 .. NELBO: 1721.16\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.19 .. NELBO: 1724.19\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.75 .. NELBO: 1712.75\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.19 .. NELBO: 1721.19\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.24 .. NELBO: 1724.24\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.7 .. NELBO: 1712.7\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.21 .. NELBO: 1721.21\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.25 .. NELBO: 1724.25\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.71 .. NELBO: 1712.71\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.23 .. NELBO: 1721.23\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.24 .. NELBO: 1724.24\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.76 .. NELBO: 1712.76\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.24 .. NELBO: 1721.24\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.29 .. NELBO: 1724.29\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.77 .. NELBO: 1712.77\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.8 .. NELBO: 1712.8\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.29 .. NELBO: 1721.29\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.29 .. NELBO: 1724.29\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.77 .. NELBO: 1712.77\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.34 .. NELBO: 1721.34\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.85 .. NELBO: 1712.85\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.35 .. NELBO: 1721.35\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.35 .. NELBO: 1724.35\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.33 .. NELBO: 1721.33\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.82 .. NELBO: 1712.82\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.29 .. NELBO: 1721.29\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.72 .. NELBO: 1712.72\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.78 .. NELBO: 1712.78\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.3 .. NELBO: 1724.3\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.83 .. NELBO: 1712.83\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.3 .. NELBO: 1724.3\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.81 .. NELBO: 1712.81\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.34 .. NELBO: 1721.34\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.8 .. NELBO: 1712.8\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.35 .. NELBO: 1721.35\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.33 .. NELBO: 1721.33\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.82 .. NELBO: 1712.82\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.81 .. NELBO: 1712.81\n",
      "Epoch: 88 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 89 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 90 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.77 .. NELBO: 1712.77\n",
      "Epoch: 91 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.38 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 92 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 93 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.73 .. NELBO: 1712.73\n",
      "Epoch: 94 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.38 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 95 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 96 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.76 .. NELBO: 1712.76\n",
      "Epoch: 97 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.38 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 98 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 99 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 100 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.590628172569627, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.84 .. Rec_loss: 1994.35 .. NELBO: 1995.19\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.79 .. Rec_loss: 1878.74 .. NELBO: 1879.53\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.82 .. Rec_loss: 1867.23 .. NELBO: 1868.05\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1723.11 .. NELBO: 1723.18\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1730.83 .. NELBO: 1730.89\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1734.12 .. NELBO: 1734.18\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 1720.02 .. NELBO: 1720.14\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1728.12 .. NELBO: 1728.23\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1731.31 .. NELBO: 1731.42\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1718.72 .. NELBO: 1718.78\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1726.82 .. NELBO: 1726.87\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1729.96 .. NELBO: 1730.01\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1717.78 .. NELBO: 1717.81\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1725.91 .. NELBO: 1725.94\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1729.05 .. NELBO: 1729.08\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1716.96 .. NELBO: 1717.02\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1725.11 .. NELBO: 1725.19\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1728.21 .. NELBO: 1728.3\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 1715.79 .. NELBO: 1716.1\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 1723.9 .. NELBO: 1724.24\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 1727.0 .. NELBO: 1727.36\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.66 .. Rec_loss: 1714.64 .. NELBO: 1715.3\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 1722.84 .. NELBO: 1723.45\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 1725.91 .. NELBO: 1726.53\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1713.6 .. NELBO: 1714.41\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.78 .. Rec_loss: 1722.0 .. NELBO: 1722.78\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.8 .. Rec_loss: 1724.95 .. NELBO: 1725.75\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.17 .. Rec_loss: 1712.15 .. NELBO: 1713.32\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.14 .. Rec_loss: 1720.43 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 1.21 .. Rec_loss: 1723.14 .. NELBO: 1724.35\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.87 .. Rec_loss: 1710.29 .. NELBO: 1712.16\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.63 .. Rec_loss: 1719.33 .. NELBO: 1720.96\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.71 .. Rec_loss: 1721.67 .. NELBO: 1723.38\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1708.97 .. NELBO: 1711.26\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.83 .. Rec_loss: 1718.47 .. NELBO: 1720.3\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 2.12 .. Rec_loss: 1720.32 .. NELBO: 1722.44\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.42 .. Rec_loss: 1706.41 .. NELBO: 1708.83\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.88 .. Rec_loss: 1716.43 .. NELBO: 1718.31\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.13 .. Rec_loss: 1718.14 .. NELBO: 1720.27\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1704.41 .. NELBO: 1707.12\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.06 .. Rec_loss: 1715.07 .. NELBO: 1717.13\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.26 .. Rec_loss: 1716.77 .. NELBO: 1719.03\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1702.74 .. NELBO: 1705.5\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.16 .. Rec_loss: 1713.86 .. NELBO: 1716.02\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1715.53 .. NELBO: 1717.9\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1702.21 .. NELBO: 1704.87\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.14 .. Rec_loss: 1713.38 .. NELBO: 1715.52\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 1715.0 .. NELBO: 1717.34\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1702.61 .. NELBO: 1705.39\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.32 .. Rec_loss: 1713.44 .. NELBO: 1715.76\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.52 .. Rec_loss: 1714.87 .. NELBO: 1717.39\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1702.2 .. NELBO: 1704.8\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1713.04 .. NELBO: 1715.33\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1714.43 .. NELBO: 1716.93\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1702.22 .. NELBO: 1704.86\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1712.98 .. NELBO: 1715.35\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1714.31 .. NELBO: 1716.91\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1701.98 .. NELBO: 1704.64\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1712.57 .. NELBO: 1714.94\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1713.86 .. NELBO: 1716.47\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1701.79 .. NELBO: 1704.52\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 1712.09 .. NELBO: 1714.56\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1713.45 .. NELBO: 1716.15\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1701.16 .. NELBO: 1703.84\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.42 .. Rec_loss: 1711.64 .. NELBO: 1714.06\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1712.79 .. NELBO: 1715.51\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1700.27 .. NELBO: 1703.01\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 1711.15 .. NELBO: 1713.58\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1712.3 .. NELBO: 1715.09\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1699.54 .. NELBO: 1702.36\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.46 .. Rec_loss: 1710.51 .. NELBO: 1712.97\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1711.72 .. NELBO: 1714.5\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1699.22 .. NELBO: 1702.2\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.57 .. Rec_loss: 1710.29 .. NELBO: 1712.86\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1711.46 .. NELBO: 1714.31\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1698.72 .. NELBO: 1701.81\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1709.89 .. NELBO: 1712.57\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1710.94 .. NELBO: 1713.93\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1698.14 .. NELBO: 1701.29\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.81 .. Rec_loss: 1709.28 .. NELBO: 1712.09\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1710.47 .. NELBO: 1713.51\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1698.18 .. NELBO: 1701.16\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1709.37 .. NELBO: 1712.03\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1710.59 .. NELBO: 1713.51\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.08 .. Rec_loss: 1698.25 .. NELBO: 1701.33\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1709.26 .. NELBO: 1712.14\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.11 .. Rec_loss: 1710.52 .. NELBO: 1713.63\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1697.88 .. NELBO: 1700.98\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1709.3 .. NELBO: 1712.18\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1710.47 .. NELBO: 1713.62\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.22 .. Rec_loss: 1697.28 .. NELBO: 1700.5\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.94 .. Rec_loss: 1708.79 .. NELBO: 1711.73\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1710.04 .. NELBO: 1713.27\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.19 .. Rec_loss: 1697.15 .. NELBO: 1700.34\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1708.48 .. NELBO: 1711.45\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1709.77 .. NELBO: 1712.98\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.44 .. Rec_loss: 1696.28 .. NELBO: 1699.72\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1707.81 .. NELBO: 1710.96\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.44 .. Rec_loss: 1708.95 .. NELBO: 1712.39\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.54 .. Rec_loss: 1696.1 .. NELBO: 1699.64\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.24 .. Rec_loss: 1707.36 .. NELBO: 1710.6\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.61 .. Rec_loss: 1708.5 .. NELBO: 1712.11\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.62 .. Rec_loss: 1695.75 .. NELBO: 1699.37\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.37 .. Rec_loss: 1706.98 .. NELBO: 1710.35\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.76 .. Rec_loss: 1707.88 .. NELBO: 1711.64\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.03 .. Rec_loss: 1694.83 .. NELBO: 1698.86\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.58 .. Rec_loss: 1705.97 .. NELBO: 1709.55\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.84 .. Rec_loss: 1707.16 .. NELBO: 1711.0\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.27 .. Rec_loss: 1694.19 .. NELBO: 1698.46\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.74 .. Rec_loss: 1705.46 .. NELBO: 1709.2\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.95 .. Rec_loss: 1706.74 .. NELBO: 1710.69\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1692.55 .. NELBO: 1697.0\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.96 .. Rec_loss: 1704.1 .. NELBO: 1708.06\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 4.24 .. Rec_loss: 1705.27 .. NELBO: 1709.51\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1691.18 .. NELBO: 1696.02\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 1703.33 .. NELBO: 1707.52\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 4.39 .. Rec_loss: 1704.73 .. NELBO: 1709.12\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.77 .. Rec_loss: 1690.82 .. NELBO: 1695.59\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.24 .. Rec_loss: 1702.62 .. NELBO: 1706.86\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 4.5 .. Rec_loss: 1704.09 .. NELBO: 1708.59\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.94 .. Rec_loss: 1691.03 .. NELBO: 1695.97\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.4 .. Rec_loss: 1702.66 .. NELBO: 1707.06\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4.71 .. Rec_loss: 1703.81 .. NELBO: 1708.52\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.88 .. Rec_loss: 1690.75 .. NELBO: 1695.63\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.37 .. Rec_loss: 1702.43 .. NELBO: 1706.8\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4.65 .. Rec_loss: 1703.69 .. NELBO: 1708.34\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.11 .. Rec_loss: 1689.85 .. NELBO: 1694.96\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.52 .. Rec_loss: 1701.55 .. NELBO: 1706.07\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1702.99 .. NELBO: 1707.83\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 1690.27 .. NELBO: 1695.49\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.6 .. Rec_loss: 1701.93 .. NELBO: 1706.53\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 4.85 .. Rec_loss: 1703.41 .. NELBO: 1708.26\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1689.85 .. NELBO: 1695.0\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.6 .. Rec_loss: 1701.82 .. NELBO: 1706.42\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1703.11 .. NELBO: 1708.01\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1690.4 .. NELBO: 1695.58\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.68 .. Rec_loss: 1702.08 .. NELBO: 1706.76\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 4.88 .. Rec_loss: 1703.39 .. NELBO: 1708.27\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.4 .. Rec_loss: 1689.56 .. NELBO: 1694.96\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1701.7 .. NELBO: 1706.54\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 5.16 .. Rec_loss: 1702.93 .. NELBO: 1708.09\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.26 .. Rec_loss: 1690.03 .. NELBO: 1695.29\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.73 .. Rec_loss: 1702.06 .. NELBO: 1706.79\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1703.61 .. NELBO: 1708.61\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1689.21 .. NELBO: 1694.57\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.85 .. Rec_loss: 1701.76 .. NELBO: 1706.61\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 4.96 .. Rec_loss: 1703.47 .. NELBO: 1708.43\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.5 .. Rec_loss: 1688.76 .. NELBO: 1694.26\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1700.67 .. NELBO: 1705.74\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 5.24 .. Rec_loss: 1702.58 .. NELBO: 1707.82\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1687.7 .. NELBO: 1693.01\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.09 .. Rec_loss: 1700.28 .. NELBO: 1705.37\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 5.2 .. Rec_loss: 1702.02 .. NELBO: 1707.22\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1688.34 .. NELBO: 1693.52\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1700.0 .. NELBO: 1705.08\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1701.81 .. NELBO: 1707.12\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1687.88 .. NELBO: 1693.03\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1699.35 .. NELBO: 1704.41\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 5.29 .. Rec_loss: 1701.17 .. NELBO: 1706.46\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1687.23 .. NELBO: 1692.4\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.99 .. Rec_loss: 1698.85 .. NELBO: 1703.84\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 5.26 .. Rec_loss: 1700.63 .. NELBO: 1705.89\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1687.23 .. NELBO: 1692.48\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1698.88 .. NELBO: 1703.94\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 5.34 .. Rec_loss: 1700.45 .. NELBO: 1705.79\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.5 .. Rec_loss: 1686.46 .. NELBO: 1691.96\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.11 .. Rec_loss: 1698.22 .. NELBO: 1703.33\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 5.4 .. Rec_loss: 1699.92 .. NELBO: 1705.32\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.52 .. Rec_loss: 1686.07 .. NELBO: 1691.59\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1697.98 .. NELBO: 1703.04\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 5.34 .. Rec_loss: 1700.01 .. NELBO: 1705.35\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.51 .. Rec_loss: 1686.59 .. NELBO: 1692.1\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 1698.07 .. NELBO: 1703.29\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1700.13 .. NELBO: 1705.57\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.52 .. Rec_loss: 1687.16 .. NELBO: 1692.68\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1698.47 .. NELBO: 1703.62\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 5.45 .. Rec_loss: 1700.34 .. NELBO: 1705.79\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1687.04 .. NELBO: 1692.76\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1698.06 .. NELBO: 1703.42\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 5.59 .. Rec_loss: 1699.98 .. NELBO: 1705.57\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1686.54 .. NELBO: 1692.23\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1697.78 .. NELBO: 1703.03\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 1699.53 .. NELBO: 1705.01\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1685.9 .. NELBO: 1691.74\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1697.46 .. NELBO: 1702.9\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 5.63 .. Rec_loss: 1699.24 .. NELBO: 1704.87\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 1685.57 .. NELBO: 1691.5\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1697.2 .. NELBO: 1702.67\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1698.92 .. NELBO: 1704.54\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1685.39 .. NELBO: 1691.29\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.57 .. Rec_loss: 1696.86 .. NELBO: 1702.43\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 5.76 .. Rec_loss: 1698.57 .. NELBO: 1704.33\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.91 .. Rec_loss: 1684.64 .. NELBO: 1690.55\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1696.48 .. NELBO: 1702.04\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 5.78 .. Rec_loss: 1698.01 .. NELBO: 1703.79\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.98 .. Rec_loss: 1684.69 .. NELBO: 1690.67\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1696.21 .. NELBO: 1701.83\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1697.85 .. NELBO: 1703.69\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 1684.33 .. NELBO: 1690.46\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.79 .. Rec_loss: 1695.69 .. NELBO: 1701.48\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 6.01 .. Rec_loss: 1697.26 .. NELBO: 1703.27\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.97 .. Rec_loss: 1684.54 .. NELBO: 1690.51\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.7 .. Rec_loss: 1695.67 .. NELBO: 1701.37\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1697.25 .. NELBO: 1703.15\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.98 .. Rec_loss: 1683.88 .. NELBO: 1689.86\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.79 .. Rec_loss: 1695.06 .. NELBO: 1700.85\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 5.97 .. Rec_loss: 1696.99 .. NELBO: 1702.96\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 1683.95 .. NELBO: 1690.03\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.82 .. Rec_loss: 1694.99 .. NELBO: 1700.81\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 1696.64 .. NELBO: 1702.76\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.94 .. Rec_loss: 1684.26 .. NELBO: 1690.2\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1695.13 .. NELBO: 1700.82\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1696.69 .. NELBO: 1702.59\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.04 .. Rec_loss: 1683.7 .. NELBO: 1689.74\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 1694.59 .. NELBO: 1700.52\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 1696.21 .. NELBO: 1702.37\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 1683.86 .. NELBO: 1689.93\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.03 .. Rec_loss: 1694.69 .. NELBO: 1700.72\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 6.3 .. Rec_loss: 1696.2 .. NELBO: 1702.5\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1683.92 .. NELBO: 1689.76\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1694.92 .. NELBO: 1700.72\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 6.05 .. Rec_loss: 1696.59 .. NELBO: 1702.64\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1683.89 .. NELBO: 1689.69\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.89 .. Rec_loss: 1695.2 .. NELBO: 1701.09\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 1696.76 .. NELBO: 1702.9\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 1683.54 .. NELBO: 1689.47\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.91 .. Rec_loss: 1695.07 .. NELBO: 1700.98\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 1696.45 .. NELBO: 1702.6\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.89 .. Rec_loss: 1683.78 .. NELBO: 1689.67\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1695.33 .. NELBO: 1701.29\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 6.24 .. Rec_loss: 1696.68 .. NELBO: 1702.92\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.97 .. Rec_loss: 1683.66 .. NELBO: 1689.63\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.04 .. Rec_loss: 1694.98 .. NELBO: 1701.02\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 6.27 .. Rec_loss: 1696.52 .. NELBO: 1702.79\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.88 .. Rec_loss: 1683.18 .. NELBO: 1689.06\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.94 .. Rec_loss: 1694.97 .. NELBO: 1700.91\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 6.17 .. Rec_loss: 1696.31 .. NELBO: 1702.48\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 1683.54 .. NELBO: 1689.65\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.92 .. Rec_loss: 1695.47 .. NELBO: 1701.39\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 6.19 .. Rec_loss: 1696.87 .. NELBO: 1703.06\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.67 .. Rec_loss: 1685.89 .. NELBO: 1691.56\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.73 .. Rec_loss: 1696.99 .. NELBO: 1702.72\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1698.18 .. NELBO: 1704.14\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.78 .. Rec_loss: 1685.54 .. NELBO: 1691.32\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.7 .. Rec_loss: 1696.2 .. NELBO: 1701.9\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 6.02 .. Rec_loss: 1697.36 .. NELBO: 1703.38\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.66 .. Rec_loss: 1683.45 .. NELBO: 1689.11\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.64 .. Rec_loss: 1694.87 .. NELBO: 1700.51\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 5.99 .. Rec_loss: 1696.21 .. NELBO: 1702.2\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 1682.88 .. NELBO: 1688.59\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 1694.55 .. NELBO: 1700.23\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 5.91 .. Rec_loss: 1695.99 .. NELBO: 1701.9\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.86 .. Rec_loss: 1682.57 .. NELBO: 1688.43\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.83 .. Rec_loss: 1694.28 .. NELBO: 1700.11\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 1695.65 .. NELBO: 1701.76\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 1682.14 .. NELBO: 1688.14\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.88 .. Rec_loss: 1693.55 .. NELBO: 1699.43\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 1695.06 .. NELBO: 1701.14\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.83 .. Rec_loss: 1682.06 .. NELBO: 1687.89\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1693.58 .. NELBO: 1699.38\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 1694.93 .. NELBO: 1701.0\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 1681.86 .. NELBO: 1687.97\n",
      "Epoch: 88 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1693.81 .. NELBO: 1699.65\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 6.09 .. Rec_loss: 1695.13 .. NELBO: 1701.22\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.95 .. Rec_loss: 1682.08 .. NELBO: 1688.03\n",
      "Epoch: 89 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.76 .. Rec_loss: 1693.8 .. NELBO: 1699.56\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 5.94 .. Rec_loss: 1695.32 .. NELBO: 1701.26\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.34 .. Rec_loss: 1681.23 .. NELBO: 1687.57\n",
      "Epoch: 90 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.97 .. Rec_loss: 1693.55 .. NELBO: 1699.52\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 1695.16 .. NELBO: 1701.3\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.37 .. Rec_loss: 1681.01 .. NELBO: 1687.38\n",
      "Epoch: 91 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.99 .. Rec_loss: 1693.28 .. NELBO: 1699.27\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 6.19 .. Rec_loss: 1695.09 .. NELBO: 1701.28\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.3 .. Rec_loss: 1681.27 .. NELBO: 1687.57\n",
      "Epoch: 92 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.05 .. Rec_loss: 1693.33 .. NELBO: 1699.38\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 6.26 .. Rec_loss: 1695.09 .. NELBO: 1701.35\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.27 .. Rec_loss: 1681.57 .. NELBO: 1687.84\n",
      "Epoch: 93 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 1693.32 .. NELBO: 1699.32\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 6.23 .. Rec_loss: 1695.0 .. NELBO: 1701.23\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.41 .. Rec_loss: 1681.41 .. NELBO: 1687.82\n",
      "Epoch: 94 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.06 .. Rec_loss: 1693.43 .. NELBO: 1699.49\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 6.27 .. Rec_loss: 1694.96 .. NELBO: 1701.23\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.19 .. Rec_loss: 1682.07 .. NELBO: 1688.26\n",
      "Epoch: 95 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 1693.65 .. NELBO: 1699.65\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 6.19 .. Rec_loss: 1695.28 .. NELBO: 1701.47\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.56 .. Rec_loss: 1681.56 .. NELBO: 1688.12\n",
      "Epoch: 96 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 1693.15 .. NELBO: 1699.28\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 6.31 .. Rec_loss: 1694.82 .. NELBO: 1701.13\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.76 .. Rec_loss: 1681.36 .. NELBO: 1688.12\n",
      "Epoch: 97 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.25 .. Rec_loss: 1693.33 .. NELBO: 1699.58\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 6.39 .. Rec_loss: 1694.92 .. NELBO: 1701.31\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.61 .. Rec_loss: 1681.31 .. NELBO: 1687.92\n",
      "Epoch: 98 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 1693.25 .. NELBO: 1699.45\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 6.33 .. Rec_loss: 1694.96 .. NELBO: 1701.29\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.68 .. Rec_loss: 1680.64 .. NELBO: 1687.32\n",
      "Epoch: 99 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.33 .. Rec_loss: 1692.56 .. NELBO: 1698.89\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 6.45 .. Rec_loss: 1694.1 .. NELBO: 1700.55\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.61 .. Rec_loss: 1680.62 .. NELBO: 1687.23\n",
      "Epoch: 100 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.3 .. Rec_loss: 1692.75 .. NELBO: 1699.05\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 6.46 .. Rec_loss: 1694.41 .. NELBO: 1700.87\n",
      "****************************************************************************************************\n",
      "Current call:  32\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5465550502500912, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=10, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.75 .. Rec_loss: 1992.24 .. NELBO: 1992.99\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 1878.33 .. NELBO: 1878.81\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 1866.91 .. NELBO: 1867.41\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1723.2 .. NELBO: 1723.27\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.83 .. NELBO: 1730.88\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.12 .. NELBO: 1734.17\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1719.83 .. NELBO: 1719.9\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1727.99 .. NELBO: 1728.06\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1731.2 .. NELBO: 1731.27\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1718.59 .. NELBO: 1718.65\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1726.75 .. NELBO: 1726.8\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1729.9 .. NELBO: 1729.95\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1717.77 .. NELBO: 1717.82\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1725.91 .. NELBO: 1725.96\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1729.04 .. NELBO: 1729.09\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1717.04 .. NELBO: 1717.11\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1725.19 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1728.33 .. NELBO: 1728.41\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 1716.05 .. NELBO: 1716.24\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 1724.2 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 1727.25 .. NELBO: 1727.53\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1714.54 .. NELBO: 1715.18\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1722.77 .. NELBO: 1723.41\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.66 .. Rec_loss: 1725.85 .. NELBO: 1726.51\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.97 .. Rec_loss: 1713.34 .. NELBO: 1714.31\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.92 .. Rec_loss: 1721.75 .. NELBO: 1722.67\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.97 .. Rec_loss: 1724.68 .. NELBO: 1725.65\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.51 .. Rec_loss: 1711.43 .. NELBO: 1712.94\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.33 .. Rec_loss: 1720.31 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 1.4 .. Rec_loss: 1722.79 .. NELBO: 1724.19\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.11 .. Rec_loss: 1709.17 .. NELBO: 1711.28\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.57 .. Rec_loss: 1719.2 .. NELBO: 1720.77\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.71 .. Rec_loss: 1721.12 .. NELBO: 1722.83\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1706.17 .. NELBO: 1708.38\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.62 .. Rec_loss: 1717.08 .. NELBO: 1718.7\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 1.76 .. Rec_loss: 1718.91 .. NELBO: 1720.67\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 1705.36 .. NELBO: 1707.83\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.81 .. Rec_loss: 1716.01 .. NELBO: 1717.82\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.04 .. Rec_loss: 1717.54 .. NELBO: 1719.58\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1704.16 .. NELBO: 1706.67\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.98 .. Rec_loss: 1714.77 .. NELBO: 1716.75\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.15 .. Rec_loss: 1716.35 .. NELBO: 1718.5\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1703.56 .. NELBO: 1706.14\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.0 .. Rec_loss: 1714.21 .. NELBO: 1716.21\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1715.61 .. NELBO: 1717.85\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.55 .. Rec_loss: 1702.51 .. NELBO: 1705.06\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.1 .. Rec_loss: 1713.21 .. NELBO: 1715.31\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 1714.62 .. NELBO: 1716.95\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1701.65 .. NELBO: 1704.33\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.25 .. Rec_loss: 1712.4 .. NELBO: 1714.65\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1713.8 .. NELBO: 1716.34\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1700.85 .. NELBO: 1703.53\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1711.63 .. NELBO: 1713.92\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.46 .. Rec_loss: 1713.17 .. NELBO: 1715.63\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1700.39 .. NELBO: 1703.21\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.38 .. Rec_loss: 1711.17 .. NELBO: 1713.55\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1712.63 .. NELBO: 1715.28\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1699.95 .. NELBO: 1702.65\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1710.83 .. NELBO: 1713.19\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1712.4 .. NELBO: 1714.98\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1700.09 .. NELBO: 1702.94\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1710.81 .. NELBO: 1713.3\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1712.17 .. NELBO: 1714.92\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1699.87 .. NELBO: 1702.62\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 1710.69 .. NELBO: 1713.16\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1712.21 .. NELBO: 1714.9\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1699.5 .. NELBO: 1702.42\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1710.3 .. NELBO: 1712.93\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1711.76 .. NELBO: 1714.63\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1699.24 .. NELBO: 1702.12\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1709.99 .. NELBO: 1712.67\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1711.6 .. NELBO: 1714.48\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1699.56 .. NELBO: 1702.33\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1710.23 .. NELBO: 1712.87\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1711.79 .. NELBO: 1714.64\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1700.21 .. NELBO: 1702.98\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1710.26 .. NELBO: 1712.98\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1711.67 .. NELBO: 1714.68\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1700.27 .. NELBO: 1702.91\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1710.2 .. NELBO: 1712.82\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1711.6 .. NELBO: 1714.49\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1700.8 .. NELBO: 1703.46\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1710.94 .. NELBO: 1713.55\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1712.46 .. NELBO: 1715.35\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1700.48 .. NELBO: 1703.38\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1710.79 .. NELBO: 1713.49\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.03 .. Rec_loss: 1712.17 .. NELBO: 1715.2\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1700.06 .. NELBO: 1702.91\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1710.5 .. NELBO: 1713.13\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1711.74 .. NELBO: 1714.7\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1699.98 .. NELBO: 1703.03\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1710.48 .. NELBO: 1713.21\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1711.59 .. NELBO: 1714.64\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.08 .. Rec_loss: 1699.8 .. NELBO: 1702.88\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.81 .. Rec_loss: 1710.36 .. NELBO: 1713.17\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1711.51 .. NELBO: 1714.64\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.16 .. Rec_loss: 1699.2 .. NELBO: 1702.36\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1710.03 .. NELBO: 1712.85\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1711.29 .. NELBO: 1714.42\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1698.86 .. NELBO: 1702.01\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1709.24 .. NELBO: 1712.2\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.27 .. Rec_loss: 1710.34 .. NELBO: 1713.61\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.24 .. Rec_loss: 1697.46 .. NELBO: 1700.7\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1708.5 .. NELBO: 1711.46\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.35 .. Rec_loss: 1709.56 .. NELBO: 1712.91\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1697.11 .. NELBO: 1700.39\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1708.08 .. NELBO: 1711.13\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.48 .. Rec_loss: 1708.93 .. NELBO: 1712.41\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.38 .. Rec_loss: 1696.03 .. NELBO: 1699.41\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.12 .. Rec_loss: 1707.2 .. NELBO: 1710.32\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.52 .. Rec_loss: 1708.07 .. NELBO: 1711.59\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1694.71 .. NELBO: 1698.27\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 1706.25 .. NELBO: 1709.58\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.68 .. Rec_loss: 1707.1 .. NELBO: 1710.78\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.87 .. Rec_loss: 1694.04 .. NELBO: 1697.91\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.61 .. Rec_loss: 1705.48 .. NELBO: 1709.09\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 3.96 .. Rec_loss: 1706.23 .. NELBO: 1710.19\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.1 .. Rec_loss: 1692.8 .. NELBO: 1696.9\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.81 .. Rec_loss: 1704.22 .. NELBO: 1708.03\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 4.07 .. Rec_loss: 1705.39 .. NELBO: 1709.46\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.38 .. Rec_loss: 1692.2 .. NELBO: 1696.58\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.0 .. Rec_loss: 1703.63 .. NELBO: 1707.63\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4.3 .. Rec_loss: 1704.73 .. NELBO: 1709.03\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.35 .. Rec_loss: 1691.12 .. NELBO: 1695.47\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.05 .. Rec_loss: 1702.52 .. NELBO: 1706.57\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4.36 .. Rec_loss: 1703.74 .. NELBO: 1708.1\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.21 .. Rec_loss: 1691.07 .. NELBO: 1695.28\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.12 .. Rec_loss: 1702.13 .. NELBO: 1706.25\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 4.41 .. Rec_loss: 1703.42 .. NELBO: 1707.83\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.61 .. Rec_loss: 1689.84 .. NELBO: 1694.45\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.23 .. Rec_loss: 1701.51 .. NELBO: 1705.74\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 4.6 .. Rec_loss: 1702.54 .. NELBO: 1707.14\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.92 .. Rec_loss: 1689.11 .. NELBO: 1694.03\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1701.02 .. NELBO: 1705.47\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.72 .. Rec_loss: 1702.24 .. NELBO: 1706.96\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.75 .. Rec_loss: 1689.0 .. NELBO: 1693.75\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1700.39 .. NELBO: 1704.84\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 4.79 .. Rec_loss: 1701.52 .. NELBO: 1706.31\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.69 .. Rec_loss: 1688.57 .. NELBO: 1693.26\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.38 .. Rec_loss: 1700.16 .. NELBO: 1704.54\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 4.8 .. Rec_loss: 1701.24 .. NELBO: 1706.04\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.86 .. Rec_loss: 1688.06 .. NELBO: 1692.92\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.61 .. Rec_loss: 1699.56 .. NELBO: 1704.17\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 4.89 .. Rec_loss: 1700.85 .. NELBO: 1705.74\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1687.82 .. NELBO: 1692.92\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.68 .. Rec_loss: 1699.4 .. NELBO: 1704.08\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1700.49 .. NELBO: 1705.55\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1687.47 .. NELBO: 1692.57\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.66 .. Rec_loss: 1699.0 .. NELBO: 1703.66\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 5.05 .. Rec_loss: 1700.24 .. NELBO: 1705.29\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1687.02 .. NELBO: 1692.12\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.74 .. Rec_loss: 1698.62 .. NELBO: 1703.36\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1699.85 .. NELBO: 1704.93\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.45 .. Rec_loss: 1686.3 .. NELBO: 1691.75\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1698.09 .. NELBO: 1702.93\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 5.11 .. Rec_loss: 1699.4 .. NELBO: 1704.51\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.35 .. Rec_loss: 1686.18 .. NELBO: 1691.53\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 1697.87 .. NELBO: 1702.78\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1699.18 .. NELBO: 1704.41\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1685.56 .. NELBO: 1691.12\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1697.74 .. NELBO: 1702.69\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 5.19 .. Rec_loss: 1699.06 .. NELBO: 1704.25\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1686.24 .. NELBO: 1691.6\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.96 .. Rec_loss: 1697.82 .. NELBO: 1702.78\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1699.24 .. NELBO: 1704.47\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1685.83 .. NELBO: 1691.25\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.01 .. Rec_loss: 1697.34 .. NELBO: 1702.35\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 1698.8 .. NELBO: 1704.02\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.57 .. Rec_loss: 1685.44 .. NELBO: 1691.01\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.04 .. Rec_loss: 1697.14 .. NELBO: 1702.18\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 5.27 .. Rec_loss: 1698.56 .. NELBO: 1703.83\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1685.33 .. NELBO: 1690.91\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.2 .. Rec_loss: 1696.81 .. NELBO: 1702.01\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 5.5 .. Rec_loss: 1697.99 .. NELBO: 1703.49\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1685.53 .. NELBO: 1691.15\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1696.88 .. NELBO: 1702.16\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1698.25 .. NELBO: 1703.83\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.53 .. Rec_loss: 1685.34 .. NELBO: 1690.87\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1696.52 .. NELBO: 1701.77\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 1698.07 .. NELBO: 1703.55\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.61 .. Rec_loss: 1684.94 .. NELBO: 1690.55\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1696.1 .. NELBO: 1701.38\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 5.59 .. Rec_loss: 1697.51 .. NELBO: 1703.1\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1685.05 .. NELBO: 1690.59\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.27 .. Rec_loss: 1696.31 .. NELBO: 1701.58\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1697.83 .. NELBO: 1703.39\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.79 .. Rec_loss: 1685.6 .. NELBO: 1691.39\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 1697.11 .. NELBO: 1702.59\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 5.67 .. Rec_loss: 1698.56 .. NELBO: 1704.23\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.63 .. Rec_loss: 1685.54 .. NELBO: 1691.17\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.45 .. Rec_loss: 1697.78 .. NELBO: 1703.23\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 5.66 .. Rec_loss: 1699.28 .. NELBO: 1704.94\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.6 .. Rec_loss: 1685.83 .. NELBO: 1691.43\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.37 .. Rec_loss: 1698.51 .. NELBO: 1703.88\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 5.64 .. Rec_loss: 1699.7 .. NELBO: 1705.34\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 1686.08 .. NELBO: 1691.79\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.37 .. Rec_loss: 1699.07 .. NELBO: 1704.44\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 5.66 .. Rec_loss: 1700.09 .. NELBO: 1705.75\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 1687.26 .. NELBO: 1692.74\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1701.54 .. NELBO: 1706.69\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 5.43 .. Rec_loss: 1702.92 .. NELBO: 1708.35\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1691.97 .. NELBO: 1697.07\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.86 .. Rec_loss: 1703.74 .. NELBO: 1708.6\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1704.48 .. NELBO: 1709.65\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1687.77 .. NELBO: 1693.19\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1699.88 .. NELBO: 1705.11\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 5.67 .. Rec_loss: 1701.72 .. NELBO: 1707.39\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.34 .. Rec_loss: 1687.62 .. NELBO: 1692.96\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1700.94 .. NELBO: 1706.0\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 5.29 .. Rec_loss: 1702.48 .. NELBO: 1707.77\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1688.56 .. NELBO: 1694.14\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1700.27 .. NELBO: 1705.44\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1701.83 .. NELBO: 1707.41\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.67 .. Rec_loss: 1686.75 .. NELBO: 1692.42\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1698.78 .. NELBO: 1704.09\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1700.43 .. NELBO: 1706.05\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.83 .. Rec_loss: 1686.33 .. NELBO: 1692.16\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.39 .. Rec_loss: 1698.25 .. NELBO: 1703.64\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 5.73 .. Rec_loss: 1699.65 .. NELBO: 1705.38\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.83 .. Rec_loss: 1686.02 .. NELBO: 1691.85\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.51 .. Rec_loss: 1697.33 .. NELBO: 1702.84\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 5.85 .. Rec_loss: 1698.7 .. NELBO: 1704.55\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1685.58 .. NELBO: 1691.48\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.51 .. Rec_loss: 1696.87 .. NELBO: 1702.38\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 5.85 .. Rec_loss: 1698.22 .. NELBO: 1704.07\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 1684.94 .. NELBO: 1690.87\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.63 .. Rec_loss: 1696.14 .. NELBO: 1701.77\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 5.92 .. Rec_loss: 1697.63 .. NELBO: 1703.55\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.01 .. Rec_loss: 1684.61 .. NELBO: 1690.62\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.66 .. Rec_loss: 1696.2 .. NELBO: 1701.86\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 5.97 .. Rec_loss: 1697.36 .. NELBO: 1703.33\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.25 .. Rec_loss: 1683.59 .. NELBO: 1689.84\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.91 .. Rec_loss: 1695.33 .. NELBO: 1701.24\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 1696.61 .. NELBO: 1702.81\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 1683.35 .. NELBO: 1689.51\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1695.03 .. NELBO: 1700.93\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 1696.24 .. NELBO: 1702.38\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.21 .. Rec_loss: 1683.42 .. NELBO: 1689.63\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.89 .. Rec_loss: 1695.19 .. NELBO: 1701.08\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 1696.28 .. NELBO: 1702.43\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.32 .. Rec_loss: 1683.25 .. NELBO: 1689.57\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.97 .. Rec_loss: 1694.58 .. NELBO: 1700.55\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 6.24 .. Rec_loss: 1695.74 .. NELBO: 1701.98\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.42 .. Rec_loss: 1682.07 .. NELBO: 1688.49\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 1694.15 .. NELBO: 1700.15\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 6.22 .. Rec_loss: 1695.3 .. NELBO: 1701.52\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.38 .. Rec_loss: 1682.01 .. NELBO: 1688.39\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.13 .. Rec_loss: 1693.66 .. NELBO: 1699.79\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 6.38 .. Rec_loss: 1694.64 .. NELBO: 1701.02\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.45 .. Rec_loss: 1682.46 .. NELBO: 1688.91\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 1693.46 .. NELBO: 1699.62\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 6.4 .. Rec_loss: 1694.45 .. NELBO: 1700.85\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.31 .. Rec_loss: 1681.96 .. NELBO: 1688.27\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 1693.42 .. NELBO: 1699.54\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 6.37 .. Rec_loss: 1694.42 .. NELBO: 1700.79\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.16 .. Rec_loss: 1682.29 .. NELBO: 1688.45\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1693.38 .. NELBO: 1699.34\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 1694.46 .. NELBO: 1700.66\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.47 .. Rec_loss: 1681.36 .. NELBO: 1687.83\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.31 .. Rec_loss: 1692.86 .. NELBO: 1699.17\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 6.49 .. Rec_loss: 1693.95 .. NELBO: 1700.44\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.6 .. Rec_loss: 1681.37 .. NELBO: 1687.97\n",
      "Epoch: 88 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.28 .. Rec_loss: 1692.52 .. NELBO: 1698.8\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 6.52 .. Rec_loss: 1693.68 .. NELBO: 1700.2\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.33 .. Rec_loss: 1681.7 .. NELBO: 1688.03\n",
      "Epoch: 89 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.38 .. Rec_loss: 1692.47 .. NELBO: 1698.85\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 6.52 .. Rec_loss: 1693.69 .. NELBO: 1700.21\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.24 .. Rec_loss: 1681.65 .. NELBO: 1687.89\n",
      "Epoch: 90 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 1692.53 .. NELBO: 1698.73\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 6.42 .. Rec_loss: 1693.61 .. NELBO: 1700.03\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.29 .. Rec_loss: 1681.91 .. NELBO: 1688.2\n",
      "Epoch: 91 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.01 .. Rec_loss: 1692.77 .. NELBO: 1698.78\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 6.23 .. Rec_loss: 1693.85 .. NELBO: 1700.08\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.59 .. Rec_loss: 1680.75 .. NELBO: 1687.34\n",
      "Epoch: 92 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.49 .. Rec_loss: 1691.88 .. NELBO: 1698.37\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 6.69 .. Rec_loss: 1693.11 .. NELBO: 1699.8\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.3 .. Rec_loss: 1681.29 .. NELBO: 1687.59\n",
      "Epoch: 93 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.33 .. Rec_loss: 1692.16 .. NELBO: 1698.49\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 6.53 .. Rec_loss: 1693.28 .. NELBO: 1699.81\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 1682.08 .. NELBO: 1688.28\n",
      "Epoch: 94 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.25 .. Rec_loss: 1692.62 .. NELBO: 1698.87\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 6.47 .. Rec_loss: 1693.8 .. NELBO: 1700.27\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.43 .. Rec_loss: 1680.59 .. NELBO: 1687.02\n",
      "Epoch: 95 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.26 .. Rec_loss: 1691.57 .. NELBO: 1697.83\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 6.5 .. Rec_loss: 1692.96 .. NELBO: 1699.46\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.44 .. Rec_loss: 1680.66 .. NELBO: 1687.1\n",
      "Epoch: 96 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.41 .. Rec_loss: 1691.78 .. NELBO: 1698.19\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 6.55 .. Rec_loss: 1693.04 .. NELBO: 1699.59\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.18 .. Rec_loss: 1681.68 .. NELBO: 1687.86\n",
      "Epoch: 97 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.12 .. Rec_loss: 1692.55 .. NELBO: 1698.67\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 6.32 .. Rec_loss: 1693.7 .. NELBO: 1700.02\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.03 .. Rec_loss: 1682.62 .. NELBO: 1688.65\n",
      "Epoch: 98 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.99 .. Rec_loss: 1693.74 .. NELBO: 1699.73\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 6.37 .. Rec_loss: 1694.76 .. NELBO: 1701.13\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 1683.38 .. NELBO: 1689.31\n",
      "Epoch: 99 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.99 .. Rec_loss: 1693.79 .. NELBO: 1699.78\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 6.17 .. Rec_loss: 1695.09 .. NELBO: 1701.26\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.24 .. Rec_loss: 1681.19 .. NELBO: 1687.43\n",
      "Epoch: 100 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.19 .. Rec_loss: 1692.67 .. NELBO: 1698.86\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 6.44 .. Rec_loss: 1693.75 .. NELBO: 1700.19\n",
      "****************************************************************************************************\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5465550502500912, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=10, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 1993.81 .. NELBO: 1994.28\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 1879.29 .. NELBO: 1879.81\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 1867.72 .. NELBO: 1868.25\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1722.98 .. NELBO: 1723.02\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.84 .. NELBO: 1730.89\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1734.16 .. NELBO: 1734.22\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1719.81 .. NELBO: 1719.89\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1727.99 .. NELBO: 1728.06\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1731.18 .. NELBO: 1731.25\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1718.56 .. NELBO: 1718.6\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1726.69 .. NELBO: 1726.73\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1729.83 .. NELBO: 1729.88\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1717.59 .. NELBO: 1717.66\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1725.74 .. NELBO: 1725.82\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1728.84 .. NELBO: 1728.93\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 1716.54 .. NELBO: 1716.78\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 1724.55 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 1727.59 .. NELBO: 1727.96\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 1714.92 .. NELBO: 1715.54\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.68 .. Rec_loss: 1723.1 .. NELBO: 1723.78\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 1726.13 .. NELBO: 1726.87\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.1 .. Rec_loss: 1713.08 .. NELBO: 1714.18\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.08 .. Rec_loss: 1721.5 .. NELBO: 1722.58\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 1.18 .. Rec_loss: 1724.17 .. NELBO: 1725.35\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.89 .. Rec_loss: 1710.17 .. NELBO: 1712.06\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.41 .. Rec_loss: 1719.89 .. NELBO: 1721.3\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 1.55 .. Rec_loss: 1721.85 .. NELBO: 1723.4\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.25 .. Rec_loss: 1707.37 .. NELBO: 1709.62\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.57 .. Rec_loss: 1718.02 .. NELBO: 1719.59\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 1.78 .. Rec_loss: 1719.72 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.22 .. Rec_loss: 1705.11 .. NELBO: 1707.33\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.62 .. Rec_loss: 1716.2 .. NELBO: 1717.82\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.8 .. Rec_loss: 1717.87 .. NELBO: 1719.67\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.55 .. Rec_loss: 1704.14 .. NELBO: 1706.69\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.84 .. Rec_loss: 1715.35 .. NELBO: 1717.19\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 2.0 .. Rec_loss: 1716.95 .. NELBO: 1718.95\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1704.08 .. NELBO: 1706.75\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.95 .. Rec_loss: 1715.1 .. NELBO: 1717.05\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.13 .. Rec_loss: 1716.62 .. NELBO: 1718.75\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1703.18 .. NELBO: 1705.92\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.03 .. Rec_loss: 1714.23 .. NELBO: 1716.26\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1715.7 .. NELBO: 1717.94\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1702.24 .. NELBO: 1705.01\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.14 .. Rec_loss: 1713.33 .. NELBO: 1715.47\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 1714.88 .. NELBO: 1717.21\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1702.07 .. NELBO: 1704.98\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1712.89 .. NELBO: 1715.1\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.41 .. Rec_loss: 1714.39 .. NELBO: 1716.8\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1701.47 .. NELBO: 1704.21\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1712.11 .. NELBO: 1714.32\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 1713.65 .. NELBO: 1716.08\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1700.94 .. NELBO: 1703.68\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.27 .. Rec_loss: 1711.47 .. NELBO: 1713.74\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1713.07 .. NELBO: 1715.55\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.81 .. Rec_loss: 1700.36 .. NELBO: 1703.17\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.31 .. Rec_loss: 1711.12 .. NELBO: 1713.43\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1712.73 .. NELBO: 1715.24\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1699.83 .. NELBO: 1702.65\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1710.54 .. NELBO: 1712.91\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1712.11 .. NELBO: 1714.72\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1699.86 .. NELBO: 1702.7\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.41 .. Rec_loss: 1710.45 .. NELBO: 1712.86\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1712.0 .. NELBO: 1714.63\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1699.53 .. NELBO: 1702.44\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1710.16 .. NELBO: 1712.6\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1711.71 .. NELBO: 1714.43\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1699.46 .. NELBO: 1702.31\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1709.68 .. NELBO: 1712.16\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1711.43 .. NELBO: 1714.11\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1698.91 .. NELBO: 1701.88\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1709.36 .. NELBO: 1711.87\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1711.07 .. NELBO: 1713.83\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1698.63 .. NELBO: 1701.51\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.42 .. Rec_loss: 1709.36 .. NELBO: 1711.78\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1711.11 .. NELBO: 1713.78\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1698.51 .. NELBO: 1701.51\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1708.85 .. NELBO: 1711.44\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1710.58 .. NELBO: 1713.4\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 1698.57 .. NELBO: 1701.63\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1708.81 .. NELBO: 1711.42\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1710.47 .. NELBO: 1713.34\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.31 .. Rec_loss: 1698.58 .. NELBO: 1701.89\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1708.81 .. NELBO: 1711.54\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1710.7 .. NELBO: 1713.58\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.36 .. Rec_loss: 1698.66 .. NELBO: 1702.02\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1708.69 .. NELBO: 1711.54\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1710.42 .. NELBO: 1713.42\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1697.86 .. NELBO: 1701.28\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1708.32 .. NELBO: 1711.29\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1710.14 .. NELBO: 1713.24\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.45 .. Rec_loss: 1697.4 .. NELBO: 1700.85\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1707.9 .. NELBO: 1710.95\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1709.82 .. NELBO: 1713.05\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.48 .. Rec_loss: 1696.24 .. NELBO: 1699.72\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.22 .. Rec_loss: 1707.03 .. NELBO: 1710.25\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1708.9 .. NELBO: 1712.32\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.71 .. Rec_loss: 1695.89 .. NELBO: 1699.6\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.46 .. Rec_loss: 1707.26 .. NELBO: 1710.72\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.64 .. Rec_loss: 1709.07 .. NELBO: 1712.71\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.72 .. Rec_loss: 1696.88 .. NELBO: 1700.6\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.48 .. Rec_loss: 1708.3 .. NELBO: 1711.78\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.65 .. Rec_loss: 1709.63 .. NELBO: 1713.28\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.08 .. Rec_loss: 1698.08 .. NELBO: 1702.16\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.66 .. Rec_loss: 1708.63 .. NELBO: 1712.29\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.86 .. Rec_loss: 1709.97 .. NELBO: 1713.83\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.5 .. Rec_loss: 1697.4 .. NELBO: 1701.9\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.84 .. Rec_loss: 1708.02 .. NELBO: 1711.86\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.95 .. Rec_loss: 1709.75 .. NELBO: 1713.7\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.82 .. Rec_loss: 1695.39 .. NELBO: 1700.21\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1706.81 .. NELBO: 1710.95\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 4.15 .. Rec_loss: 1709.04 .. NELBO: 1713.19\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.86 .. Rec_loss: 1693.48 .. NELBO: 1698.34\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.3 .. Rec_loss: 1704.52 .. NELBO: 1708.82\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1706.27 .. NELBO: 1710.69\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.62 .. Rec_loss: 1692.12 .. NELBO: 1696.74\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.22 .. Rec_loss: 1703.61 .. NELBO: 1707.83\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 4.41 .. Rec_loss: 1705.11 .. NELBO: 1709.52\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.69 .. Rec_loss: 1690.73 .. NELBO: 1695.42\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1701.98 .. NELBO: 1706.4\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 4.62 .. Rec_loss: 1703.6 .. NELBO: 1708.22\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.71 .. Rec_loss: 1689.53 .. NELBO: 1694.24\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.36 .. Rec_loss: 1701.04 .. NELBO: 1705.4\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4.6 .. Rec_loss: 1702.47 .. NELBO: 1707.07\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1688.36 .. NELBO: 1693.36\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 1699.99 .. NELBO: 1704.69\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1701.64 .. NELBO: 1706.48\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.89 .. Rec_loss: 1687.79 .. NELBO: 1692.68\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.67 .. Rec_loss: 1699.21 .. NELBO: 1703.88\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 4.94 .. Rec_loss: 1700.7 .. NELBO: 1705.64\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 1688.39 .. NELBO: 1693.09\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.51 .. Rec_loss: 1699.46 .. NELBO: 1703.97\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 4.73 .. Rec_loss: 1700.92 .. NELBO: 1705.65\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1686.74 .. NELBO: 1692.05\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.02 .. Rec_loss: 1698.11 .. NELBO: 1703.13\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 1699.65 .. NELBO: 1704.87\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1686.82 .. NELBO: 1691.72\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.85 .. Rec_loss: 1698.02 .. NELBO: 1702.87\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1699.58 .. NELBO: 1704.65\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.85 .. Rec_loss: 1686.43 .. NELBO: 1691.28\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.79 .. Rec_loss: 1697.71 .. NELBO: 1702.5\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1699.15 .. NELBO: 1704.21\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.38 .. Rec_loss: 1685.96 .. NELBO: 1691.34\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1696.98 .. NELBO: 1702.16\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 5.4 .. Rec_loss: 1698.56 .. NELBO: 1703.96\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.24 .. Rec_loss: 1686.16 .. NELBO: 1691.4\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.16 .. Rec_loss: 1697.1 .. NELBO: 1702.26\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 5.38 .. Rec_loss: 1698.57 .. NELBO: 1703.95\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.94 .. Rec_loss: 1685.68 .. NELBO: 1690.62\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.99 .. Rec_loss: 1696.51 .. NELBO: 1701.5\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1697.94 .. NELBO: 1703.22\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.12 .. Rec_loss: 1685.39 .. NELBO: 1690.51\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.11 .. Rec_loss: 1696.38 .. NELBO: 1701.49\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 5.4 .. Rec_loss: 1697.8 .. NELBO: 1703.2\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1685.37 .. NELBO: 1690.47\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.13 .. Rec_loss: 1696.47 .. NELBO: 1701.6\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1697.94 .. NELBO: 1703.3\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.01 .. Rec_loss: 1685.48 .. NELBO: 1690.49\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1696.23 .. NELBO: 1701.31\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 5.35 .. Rec_loss: 1697.66 .. NELBO: 1703.01\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.53 .. Rec_loss: 1685.04 .. NELBO: 1690.57\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1696.11 .. NELBO: 1701.39\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1697.59 .. NELBO: 1703.15\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1685.0 .. NELBO: 1690.28\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.2 .. Rec_loss: 1696.18 .. NELBO: 1701.38\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1697.45 .. NELBO: 1702.99\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1684.6 .. NELBO: 1689.88\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1695.63 .. NELBO: 1700.86\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1697.11 .. NELBO: 1702.58\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.24 .. Rec_loss: 1685.26 .. NELBO: 1690.5\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 1695.93 .. NELBO: 1701.15\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 5.52 .. Rec_loss: 1697.34 .. NELBO: 1702.86\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.32 .. Rec_loss: 1685.19 .. NELBO: 1690.51\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 1695.77 .. NELBO: 1700.99\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1697.22 .. NELBO: 1702.76\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1684.71 .. NELBO: 1689.96\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.19 .. Rec_loss: 1696.07 .. NELBO: 1701.26\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1697.48 .. NELBO: 1702.92\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.43 .. Rec_loss: 1684.3 .. NELBO: 1689.73\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.33 .. Rec_loss: 1695.94 .. NELBO: 1701.27\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 5.6 .. Rec_loss: 1697.37 .. NELBO: 1702.97\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.37 .. Rec_loss: 1684.43 .. NELBO: 1689.8\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.27 .. Rec_loss: 1696.19 .. NELBO: 1701.46\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 5.61 .. Rec_loss: 1697.73 .. NELBO: 1703.34\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1685.37 .. NELBO: 1690.6\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.04 .. Rec_loss: 1697.42 .. NELBO: 1702.46\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1698.8 .. NELBO: 1704.24\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1686.18 .. NELBO: 1691.46\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.99 .. Rec_loss: 1698.62 .. NELBO: 1703.61\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1700.13 .. NELBO: 1705.44\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.11 .. Rec_loss: 1689.64 .. NELBO: 1694.75\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.96 .. Rec_loss: 1699.66 .. NELBO: 1704.62\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 5.26 .. Rec_loss: 1701.05 .. NELBO: 1706.31\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1687.33 .. NELBO: 1692.69\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1699.62 .. NELBO: 1704.77\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 5.51 .. Rec_loss: 1700.85 .. NELBO: 1706.36\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.3 .. Rec_loss: 1687.23 .. NELBO: 1692.53\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1700.07 .. NELBO: 1705.1\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1701.23 .. NELBO: 1706.65\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1688.02 .. NELBO: 1693.38\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.21 .. Rec_loss: 1699.84 .. NELBO: 1705.05\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1701.39 .. NELBO: 1706.83\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1686.56 .. NELBO: 1692.12\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1700.06 .. NELBO: 1705.31\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1701.66 .. NELBO: 1707.2\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1688.42 .. NELBO: 1693.39\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.86 .. Rec_loss: 1700.42 .. NELBO: 1705.28\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1702.02 .. NELBO: 1707.19\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.14 .. Rec_loss: 1686.42 .. NELBO: 1691.56\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1699.3 .. NELBO: 1704.27\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 5.35 .. Rec_loss: 1700.53 .. NELBO: 1705.88\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.53 .. Rec_loss: 1685.98 .. NELBO: 1691.51\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1698.38 .. NELBO: 1703.48\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1699.77 .. NELBO: 1705.19\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.64 .. Rec_loss: 1685.19 .. NELBO: 1690.83\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.14 .. Rec_loss: 1697.73 .. NELBO: 1702.87\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1699.07 .. NELBO: 1704.49\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 1684.73 .. NELBO: 1690.44\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.26 .. Rec_loss: 1697.06 .. NELBO: 1702.32\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 5.59 .. Rec_loss: 1698.33 .. NELBO: 1703.92\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1684.62 .. NELBO: 1690.16\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1696.73 .. NELBO: 1701.91\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 5.52 .. Rec_loss: 1697.98 .. NELBO: 1703.5\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.67 .. Rec_loss: 1683.9 .. NELBO: 1689.57\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.3 .. Rec_loss: 1696.44 .. NELBO: 1701.74\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1697.63 .. NELBO: 1703.17\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.79 .. Rec_loss: 1683.75 .. NELBO: 1689.54\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.39 .. Rec_loss: 1696.08 .. NELBO: 1701.47\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 5.64 .. Rec_loss: 1697.29 .. NELBO: 1702.93\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1683.46 .. NELBO: 1689.3\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.43 .. Rec_loss: 1695.73 .. NELBO: 1701.16\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 1696.92 .. NELBO: 1702.63\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.78 .. Rec_loss: 1683.31 .. NELBO: 1689.09\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.51 .. Rec_loss: 1695.55 .. NELBO: 1701.06\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1696.88 .. NELBO: 1702.57\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 1683.29 .. NELBO: 1689.22\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1695.2 .. NELBO: 1700.69\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 1696.62 .. NELBO: 1702.3\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.38 .. Rec_loss: 1681.77 .. NELBO: 1688.15\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1694.3 .. NELBO: 1700.02\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 5.87 .. Rec_loss: 1695.82 .. NELBO: 1701.69\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 1682.32 .. NELBO: 1688.52\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 1694.83 .. NELBO: 1700.58\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 1696.25 .. NELBO: 1702.18\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1683.79 .. NELBO: 1689.41\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1695.33 .. NELBO: 1700.87\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 5.74 .. Rec_loss: 1696.72 .. NELBO: 1702.46\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.81 .. Rec_loss: 1682.84 .. NELBO: 1688.65\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1694.42 .. NELBO: 1700.22\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 1696.06 .. NELBO: 1701.99\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1682.84 .. NELBO: 1688.68\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1694.32 .. NELBO: 1700.12\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 5.98 .. Rec_loss: 1695.77 .. NELBO: 1701.75\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.77 .. Rec_loss: 1682.8 .. NELBO: 1688.57\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 1694.46 .. NELBO: 1700.21\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1695.9 .. NELBO: 1701.86\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 1682.89 .. NELBO: 1688.64\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.79 .. Rec_loss: 1694.48 .. NELBO: 1700.27\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 1695.87 .. NELBO: 1701.87\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1682.47 .. NELBO: 1688.37\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.73 .. Rec_loss: 1694.25 .. NELBO: 1699.98\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 5.89 .. Rec_loss: 1695.93 .. NELBO: 1701.82\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.89 .. Rec_loss: 1682.62 .. NELBO: 1688.51\n",
      "Epoch: 88 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.86 .. Rec_loss: 1694.15 .. NELBO: 1700.01\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 1695.69 .. NELBO: 1701.69\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.91 .. Rec_loss: 1682.88 .. NELBO: 1688.79\n",
      "Epoch: 89 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.82 .. Rec_loss: 1694.3 .. NELBO: 1700.12\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 5.99 .. Rec_loss: 1695.77 .. NELBO: 1701.76\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1682.58 .. NELBO: 1688.42\n",
      "Epoch: 90 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.83 .. Rec_loss: 1693.99 .. NELBO: 1699.82\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 5.97 .. Rec_loss: 1695.45 .. NELBO: 1701.42\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1682.71 .. NELBO: 1688.61\n",
      "Epoch: 91 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.83 .. Rec_loss: 1693.84 .. NELBO: 1699.67\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 6.01 .. Rec_loss: 1695.32 .. NELBO: 1701.33\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 1681.99 .. NELBO: 1688.19\n",
      "Epoch: 92 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.86 .. Rec_loss: 1693.44 .. NELBO: 1699.3\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 6.06 .. Rec_loss: 1695.02 .. NELBO: 1701.08\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.26 .. Rec_loss: 1682.09 .. NELBO: 1688.35\n",
      "Epoch: 93 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1693.58 .. NELBO: 1699.54\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 6.04 .. Rec_loss: 1695.25 .. NELBO: 1701.29\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.1 .. Rec_loss: 1682.7 .. NELBO: 1688.8\n",
      "Epoch: 94 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.99 .. Rec_loss: 1693.74 .. NELBO: 1699.73\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 6.05 .. Rec_loss: 1695.3 .. NELBO: 1701.35\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 1682.46 .. NELBO: 1688.6\n",
      "Epoch: 95 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.87 .. Rec_loss: 1693.53 .. NELBO: 1699.4\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 6.01 .. Rec_loss: 1695.1 .. NELBO: 1701.11\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.29 .. Rec_loss: 1682.03 .. NELBO: 1688.32\n",
      "Epoch: 96 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1693.59 .. NELBO: 1699.55\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 1695.33 .. NELBO: 1701.33\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.37 .. Rec_loss: 1681.8 .. NELBO: 1688.17\n",
      "Epoch: 97 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1693.49 .. NELBO: 1699.45\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 6.05 .. Rec_loss: 1695.15 .. NELBO: 1701.2\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.52 .. Rec_loss: 1681.1 .. NELBO: 1687.62\n",
      "Epoch: 98 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.92 .. Rec_loss: 1693.1 .. NELBO: 1699.02\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1694.89 .. NELBO: 1700.85\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.39 .. Rec_loss: 1680.91 .. NELBO: 1687.3\n",
      "Epoch: 99 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.89 .. Rec_loss: 1692.95 .. NELBO: 1698.84\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1695.03 .. NELBO: 1700.93\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.59 .. Rec_loss: 1681.51 .. NELBO: 1688.1\n",
      "Epoch: 100 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 6.06 .. Rec_loss: 1693.04 .. NELBO: 1699.1\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 6.06 .. Rec_loss: 1695.22 .. NELBO: 1701.28\n",
      "****************************************************************************************************\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5465550502500912, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=10, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 1993.38 .. NELBO: 1993.61\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.58 .. Rec_loss: 1878.84 .. NELBO: 1879.42\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 1867.39 .. NELBO: 1867.99\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.38 .. NELBO: 1723.43\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1731.19 .. NELBO: 1731.24\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.49 .. NELBO: 1734.54\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1719.92 .. NELBO: 1719.96\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1728.11 .. NELBO: 1728.15\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1731.31 .. NELBO: 1731.34\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1718.52 .. NELBO: 1718.54\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.71 .. NELBO: 1726.72\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.86 .. NELBO: 1729.87\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.71 .. NELBO: 1717.72\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.89 .. NELBO: 1725.9\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.03 .. NELBO: 1729.04\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.09 .. NELBO: 1717.1\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1725.26 .. NELBO: 1725.28\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1728.4 .. NELBO: 1728.42\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1716.38 .. NELBO: 1716.47\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1724.55 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1727.66 .. NELBO: 1727.77\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 1715.29 .. NELBO: 1715.66\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 1723.51 .. NELBO: 1723.89\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 1726.62 .. NELBO: 1727.02\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.68 .. Rec_loss: 1714.55 .. NELBO: 1715.23\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.66 .. Rec_loss: 1723.01 .. NELBO: 1723.67\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1726.05 .. NELBO: 1726.69\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.87 .. Rec_loss: 1713.3 .. NELBO: 1714.17\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.72 .. Rec_loss: 1721.99 .. NELBO: 1722.71\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.79 .. Rec_loss: 1724.91 .. NELBO: 1725.7\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.33 .. Rec_loss: 1711.54 .. NELBO: 1712.87\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.19 .. Rec_loss: 1720.18 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.33 .. Rec_loss: 1722.75 .. NELBO: 1724.08\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.92 .. Rec_loss: 1709.44 .. NELBO: 1711.36\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.58 .. Rec_loss: 1719.2 .. NELBO: 1720.78\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 1.81 .. Rec_loss: 1721.17 .. NELBO: 1722.98\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.31 .. Rec_loss: 1707.58 .. NELBO: 1709.89\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.97 .. Rec_loss: 1717.28 .. NELBO: 1719.25\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.16 .. Rec_loss: 1718.87 .. NELBO: 1721.03\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 1705.33 .. NELBO: 1707.67\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.95 .. Rec_loss: 1715.53 .. NELBO: 1717.48\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.27 .. Rec_loss: 1716.87 .. NELBO: 1719.14\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1704.24 .. NELBO: 1706.78\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.16 .. Rec_loss: 1714.56 .. NELBO: 1716.72\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1715.86 .. NELBO: 1718.26\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1702.16 .. NELBO: 1704.8\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.3 .. Rec_loss: 1712.96 .. NELBO: 1715.26\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.45 .. Rec_loss: 1714.42 .. NELBO: 1716.87\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1701.07 .. NELBO: 1703.98\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1712.22 .. NELBO: 1714.66\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1713.53 .. NELBO: 1716.23\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1701.18 .. NELBO: 1704.16\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1712.33 .. NELBO: 1714.83\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1713.62 .. NELBO: 1716.35\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1700.68 .. NELBO: 1703.64\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1711.77 .. NELBO: 1714.3\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1712.91 .. NELBO: 1715.74\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1700.06 .. NELBO: 1703.02\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1711.28 .. NELBO: 1713.77\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1712.42 .. NELBO: 1715.19\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1699.43 .. NELBO: 1702.41\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1710.72 .. NELBO: 1713.34\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1711.92 .. NELBO: 1714.76\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1698.99 .. NELBO: 1702.08\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1710.45 .. NELBO: 1713.15\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1711.93 .. NELBO: 1714.78\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.19 .. Rec_loss: 1698.85 .. NELBO: 1702.04\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1710.34 .. NELBO: 1713.16\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1711.72 .. NELBO: 1714.69\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.07 .. Rec_loss: 1699.62 .. NELBO: 1702.69\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1710.89 .. NELBO: 1713.64\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1712.44 .. NELBO: 1715.31\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 1699.61 .. NELBO: 1702.67\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.8 .. Rec_loss: 1710.88 .. NELBO: 1713.68\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1712.53 .. NELBO: 1715.41\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1700.43 .. NELBO: 1703.38\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1711.13 .. NELBO: 1713.91\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.94 .. Rec_loss: 1712.54 .. NELBO: 1715.48\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1698.81 .. NELBO: 1701.63\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.8 .. Rec_loss: 1709.59 .. NELBO: 1712.39\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 3.03 .. Rec_loss: 1711.01 .. NELBO: 1714.04\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1698.23 .. NELBO: 1701.09\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1709.27 .. NELBO: 1712.14\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1710.59 .. NELBO: 1713.72\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1698.41 .. NELBO: 1701.16\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1708.99 .. NELBO: 1711.77\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1710.18 .. NELBO: 1713.28\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1697.81 .. NELBO: 1700.48\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1708.65 .. NELBO: 1711.43\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3.07 .. Rec_loss: 1709.9 .. NELBO: 1712.97\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1697.95 .. NELBO: 1700.74\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1708.77 .. NELBO: 1711.73\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1709.99 .. NELBO: 1713.22\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1697.51 .. NELBO: 1700.36\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1708.31 .. NELBO: 1711.4\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.29 .. Rec_loss: 1709.81 .. NELBO: 1713.1\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1697.34 .. NELBO: 1700.17\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.08 .. Rec_loss: 1708.11 .. NELBO: 1711.19\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.4 .. Rec_loss: 1709.49 .. NELBO: 1712.89\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1697.15 .. NELBO: 1700.02\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.07 .. Rec_loss: 1707.88 .. NELBO: 1710.95\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.45 .. Rec_loss: 1709.13 .. NELBO: 1712.58\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1696.66 .. NELBO: 1699.5\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1707.47 .. NELBO: 1710.56\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.53 .. Rec_loss: 1708.65 .. NELBO: 1712.18\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1696.92 .. NELBO: 1699.82\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1707.43 .. NELBO: 1710.66\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.49 .. Rec_loss: 1708.75 .. NELBO: 1712.24\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1696.05 .. NELBO: 1699.2\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.44 .. Rec_loss: 1706.9 .. NELBO: 1710.34\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.74 .. Rec_loss: 1708.5 .. NELBO: 1712.24\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.2 .. Rec_loss: 1695.51 .. NELBO: 1698.71\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.48 .. Rec_loss: 1706.45 .. NELBO: 1709.93\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.79 .. Rec_loss: 1707.89 .. NELBO: 1711.68\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.31 .. Rec_loss: 1695.23 .. NELBO: 1698.54\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.65 .. Rec_loss: 1706.13 .. NELBO: 1709.78\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 4.02 .. Rec_loss: 1707.67 .. NELBO: 1711.69\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1694.95 .. NELBO: 1698.51\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.73 .. Rec_loss: 1705.76 .. NELBO: 1709.49\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 4.17 .. Rec_loss: 1707.19 .. NELBO: 1711.36\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.57 .. Rec_loss: 1695.12 .. NELBO: 1698.69\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.71 .. Rec_loss: 1705.83 .. NELBO: 1709.54\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4.15 .. Rec_loss: 1707.16 .. NELBO: 1711.31\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.88 .. Rec_loss: 1693.96 .. NELBO: 1697.84\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.03 .. Rec_loss: 1704.69 .. NELBO: 1708.72\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4.39 .. Rec_loss: 1706.12 .. NELBO: 1710.51\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.89 .. Rec_loss: 1693.55 .. NELBO: 1697.44\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.01 .. Rec_loss: 1704.76 .. NELBO: 1708.77\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 4.32 .. Rec_loss: 1706.06 .. NELBO: 1710.38\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.97 .. Rec_loss: 1692.83 .. NELBO: 1696.8\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1703.85 .. NELBO: 1707.99\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1705.12 .. NELBO: 1709.57\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.07 .. Rec_loss: 1692.29 .. NELBO: 1696.36\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.15 .. Rec_loss: 1703.34 .. NELBO: 1707.49\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.54 .. Rec_loss: 1704.67 .. NELBO: 1709.21\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.0 .. Rec_loss: 1691.49 .. NELBO: 1695.49\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 1702.79 .. NELBO: 1706.98\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 4.56 .. Rec_loss: 1704.09 .. NELBO: 1708.65\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.86 .. Rec_loss: 1691.17 .. NELBO: 1695.03\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.12 .. Rec_loss: 1702.48 .. NELBO: 1706.6\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 4.34 .. Rec_loss: 1704.07 .. NELBO: 1708.41\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.52 .. Rec_loss: 1690.7 .. NELBO: 1695.22\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.56 .. Rec_loss: 1701.96 .. NELBO: 1706.52\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 4.96 .. Rec_loss: 1703.51 .. NELBO: 1708.47\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.71 .. Rec_loss: 1691.14 .. NELBO: 1694.85\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.11 .. Rec_loss: 1701.96 .. NELBO: 1706.07\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 4.65 .. Rec_loss: 1703.32 .. NELBO: 1707.97\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.09 .. Rec_loss: 1691.03 .. NELBO: 1695.12\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.07 .. Rec_loss: 1701.92 .. NELBO: 1705.99\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 4.68 .. Rec_loss: 1703.12 .. NELBO: 1707.8\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.25 .. Rec_loss: 1689.91 .. NELBO: 1694.16\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.0 .. Rec_loss: 1701.47 .. NELBO: 1705.47\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 4.53 .. Rec_loss: 1702.74 .. NELBO: 1707.27\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1688.92 .. NELBO: 1693.37\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.06 .. Rec_loss: 1700.86 .. NELBO: 1704.92\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 4.62 .. Rec_loss: 1702.1 .. NELBO: 1706.72\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.64 .. Rec_loss: 1688.72 .. NELBO: 1693.36\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.21 .. Rec_loss: 1700.95 .. NELBO: 1705.16\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 4.78 .. Rec_loss: 1702.3 .. NELBO: 1707.08\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.51 .. Rec_loss: 1689.97 .. NELBO: 1694.48\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.22 .. Rec_loss: 1701.66 .. NELBO: 1705.88\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 4.79 .. Rec_loss: 1702.76 .. NELBO: 1707.55\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.62 .. Rec_loss: 1689.12 .. NELBO: 1693.74\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.26 .. Rec_loss: 1701.2 .. NELBO: 1705.46\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 4.81 .. Rec_loss: 1702.26 .. NELBO: 1707.07\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.8 .. Rec_loss: 1689.09 .. NELBO: 1693.89\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.34 .. Rec_loss: 1701.36 .. NELBO: 1705.7\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1702.35 .. NELBO: 1707.19\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.11 .. Rec_loss: 1688.78 .. NELBO: 1693.89\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.67 .. Rec_loss: 1700.9 .. NELBO: 1705.57\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1702.08 .. NELBO: 1707.15\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.68 .. Rec_loss: 1688.81 .. NELBO: 1693.49\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.49 .. Rec_loss: 1700.79 .. NELBO: 1705.28\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 4.98 .. Rec_loss: 1701.83 .. NELBO: 1706.81\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1688.59 .. NELBO: 1693.76\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.65 .. Rec_loss: 1700.73 .. NELBO: 1705.38\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1701.84 .. NELBO: 1706.92\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.24 .. Rec_loss: 1688.22 .. NELBO: 1693.46\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.64 .. Rec_loss: 1700.35 .. NELBO: 1704.99\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1701.58 .. NELBO: 1706.65\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1688.46 .. NELBO: 1693.93\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.74 .. Rec_loss: 1700.2 .. NELBO: 1704.94\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 5.12 .. Rec_loss: 1701.36 .. NELBO: 1706.48\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.5 .. Rec_loss: 1687.4 .. NELBO: 1692.9\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.88 .. Rec_loss: 1699.04 .. NELBO: 1703.92\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 1700.43 .. NELBO: 1705.65\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1686.54 .. NELBO: 1692.12\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1698.29 .. NELBO: 1703.19\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1699.72 .. NELBO: 1705.0\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1686.34 .. NELBO: 1692.14\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1697.61 .. NELBO: 1702.76\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 5.39 .. Rec_loss: 1699.24 .. NELBO: 1704.63\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.65 .. Rec_loss: 1685.35 .. NELBO: 1691.0\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1697.0 .. NELBO: 1702.03\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 5.4 .. Rec_loss: 1698.65 .. NELBO: 1704.05\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 1685.28 .. NELBO: 1690.99\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1696.53 .. NELBO: 1701.61\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 5.33 .. Rec_loss: 1698.16 .. NELBO: 1703.49\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.6 .. Rec_loss: 1685.24 .. NELBO: 1690.84\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.21 .. Rec_loss: 1696.02 .. NELBO: 1701.23\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1697.7 .. NELBO: 1703.19\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1684.72 .. NELBO: 1690.41\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1695.89 .. NELBO: 1701.14\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 5.46 .. Rec_loss: 1697.73 .. NELBO: 1703.19\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1684.97 .. NELBO: 1690.66\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.35 .. Rec_loss: 1695.82 .. NELBO: 1701.17\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1697.61 .. NELBO: 1703.17\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.74 .. Rec_loss: 1684.62 .. NELBO: 1690.36\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.35 .. Rec_loss: 1695.64 .. NELBO: 1700.99\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 5.57 .. Rec_loss: 1697.49 .. NELBO: 1703.06\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 1683.63 .. NELBO: 1689.7\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.48 .. Rec_loss: 1695.14 .. NELBO: 1700.62\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 5.65 .. Rec_loss: 1696.94 .. NELBO: 1702.59\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.73 .. Rec_loss: 1683.62 .. NELBO: 1689.35\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.52 .. Rec_loss: 1694.77 .. NELBO: 1700.29\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1696.58 .. NELBO: 1702.27\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1683.73 .. NELBO: 1689.35\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.4 .. Rec_loss: 1694.8 .. NELBO: 1700.2\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 5.53 .. Rec_loss: 1696.63 .. NELBO: 1702.16\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 1683.32 .. NELBO: 1689.03\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.5 .. Rec_loss: 1694.55 .. NELBO: 1700.05\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1696.47 .. NELBO: 1702.16\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1683.33 .. NELBO: 1689.05\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1694.56 .. NELBO: 1699.92\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 5.66 .. Rec_loss: 1696.17 .. NELBO: 1701.83\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.67 .. Rec_loss: 1682.89 .. NELBO: 1688.56\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.39 .. Rec_loss: 1694.2 .. NELBO: 1699.59\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 5.55 .. Rec_loss: 1696.04 .. NELBO: 1701.59\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.88 .. Rec_loss: 1682.67 .. NELBO: 1688.55\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1693.89 .. NELBO: 1699.45\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 1695.78 .. NELBO: 1701.53\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.7 .. Rec_loss: 1682.71 .. NELBO: 1688.41\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.51 .. Rec_loss: 1693.95 .. NELBO: 1699.46\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1695.77 .. NELBO: 1701.46\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 1683.01 .. NELBO: 1688.76\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.46 .. Rec_loss: 1694.3 .. NELBO: 1699.76\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 5.74 .. Rec_loss: 1695.85 .. NELBO: 1701.59\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.67 .. Rec_loss: 1682.72 .. NELBO: 1688.39\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.41 .. Rec_loss: 1693.97 .. NELBO: 1699.38\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 5.57 .. Rec_loss: 1695.61 .. NELBO: 1701.18\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.92 .. Rec_loss: 1682.45 .. NELBO: 1688.37\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1693.8 .. NELBO: 1699.34\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 5.79 .. Rec_loss: 1695.51 .. NELBO: 1701.3\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1682.31 .. NELBO: 1688.27\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.43 .. Rec_loss: 1694.11 .. NELBO: 1699.54\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 5.67 .. Rec_loss: 1695.79 .. NELBO: 1701.46\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.73 .. Rec_loss: 1682.24 .. NELBO: 1687.97\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.39 .. Rec_loss: 1693.77 .. NELBO: 1699.16\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 5.59 .. Rec_loss: 1695.43 .. NELBO: 1701.02\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.11 .. Rec_loss: 1682.0 .. NELBO: 1688.11\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1693.75 .. NELBO: 1699.31\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 5.78 .. Rec_loss: 1695.48 .. NELBO: 1701.26\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.91 .. Rec_loss: 1682.26 .. NELBO: 1688.17\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1694.16 .. NELBO: 1699.6\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 1695.74 .. NELBO: 1701.42\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.95 .. Rec_loss: 1681.92 .. NELBO: 1687.87\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.46 .. Rec_loss: 1693.75 .. NELBO: 1699.21\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 1695.26 .. NELBO: 1700.97\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.05 .. Rec_loss: 1682.36 .. NELBO: 1688.41\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.5 .. Rec_loss: 1693.97 .. NELBO: 1699.47\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 1695.61 .. NELBO: 1701.32\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.03 .. Rec_loss: 1682.43 .. NELBO: 1688.46\n",
      "Epoch: 88 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1693.72 .. NELBO: 1699.3\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1695.48 .. NELBO: 1701.32\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.93 .. Rec_loss: 1683.17 .. NELBO: 1689.1\n",
      "Epoch: 89 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.55 .. Rec_loss: 1694.0 .. NELBO: 1699.55\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 5.77 .. Rec_loss: 1695.57 .. NELBO: 1701.34\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.07 .. Rec_loss: 1682.35 .. NELBO: 1688.42\n",
      "Epoch: 90 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.73 .. Rec_loss: 1693.4 .. NELBO: 1699.13\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 5.88 .. Rec_loss: 1695.11 .. NELBO: 1700.99\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 1683.06 .. NELBO: 1689.14\n",
      "Epoch: 91 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1693.5 .. NELBO: 1699.3\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 5.97 .. Rec_loss: 1695.16 .. NELBO: 1701.13\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.87 .. Rec_loss: 1682.88 .. NELBO: 1688.75\n",
      "Epoch: 92 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1693.74 .. NELBO: 1699.43\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1695.32 .. NELBO: 1701.22\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.06 .. Rec_loss: 1682.61 .. NELBO: 1688.67\n",
      "Epoch: 93 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.86 .. Rec_loss: 1693.55 .. NELBO: 1699.41\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 5.99 .. Rec_loss: 1695.27 .. NELBO: 1701.26\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.21 .. Rec_loss: 1682.1 .. NELBO: 1688.31\n",
      "Epoch: 94 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.89 .. Rec_loss: 1693.43 .. NELBO: 1699.32\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 6.08 .. Rec_loss: 1695.18 .. NELBO: 1701.26\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.15 .. Rec_loss: 1682.16 .. NELBO: 1688.31\n",
      "Epoch: 95 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.9 .. Rec_loss: 1693.48 .. NELBO: 1699.38\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 6.06 .. Rec_loss: 1695.1 .. NELBO: 1701.16\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.76 .. Rec_loss: 1682.9 .. NELBO: 1688.66\n",
      "Epoch: 96 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.67 .. Rec_loss: 1694.11 .. NELBO: 1699.78\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 5.86 .. Rec_loss: 1695.7 .. NELBO: 1701.56\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.05 .. Rec_loss: 1682.98 .. NELBO: 1689.03\n",
      "Epoch: 97 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.87 .. Rec_loss: 1694.32 .. NELBO: 1700.19\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 6.03 .. Rec_loss: 1695.95 .. NELBO: 1701.98\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.73 .. Rec_loss: 1685.04 .. NELBO: 1690.77\n",
      "Epoch: 98 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1696.13 .. NELBO: 1701.85\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 6.04 .. Rec_loss: 1697.41 .. NELBO: 1703.45\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.14 .. Rec_loss: 1686.03 .. NELBO: 1691.17\n",
      "Epoch: 99 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.3 .. Rec_loss: 1696.64 .. NELBO: 1701.94\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 5.64 .. Rec_loss: 1697.94 .. NELBO: 1703.58\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.65 .. Rec_loss: 1683.37 .. NELBO: 1689.02\n",
      "Epoch: 100 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.77 .. Rec_loss: 1694.33 .. NELBO: 1700.1\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1695.82 .. NELBO: 1701.78\n",
      "****************************************************************************************************\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5465550502500912, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=10, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 1991.6 .. NELBO: 1992.29\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.87 .. Rec_loss: 1877.24 .. NELBO: 1878.11\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.82 .. Rec_loss: 1865.94 .. NELBO: 1866.76\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1723.1 .. NELBO: 1723.14\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1730.87 .. NELBO: 1730.94\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1734.15 .. NELBO: 1734.24\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 1719.88 .. NELBO: 1720.03\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 1727.99 .. NELBO: 1728.13\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 1731.16 .. NELBO: 1731.32\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 1718.21 .. NELBO: 1718.39\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 1726.32 .. NELBO: 1726.52\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 1729.44 .. NELBO: 1729.67\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.33 .. Rec_loss: 1717.01 .. NELBO: 1717.34\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 1725.11 .. NELBO: 1725.47\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 1728.14 .. NELBO: 1728.54\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 1715.89 .. NELBO: 1716.49\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 1723.9 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 1726.93 .. NELBO: 1727.58\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1714.43 .. NELBO: 1715.24\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.8 .. Rec_loss: 1722.67 .. NELBO: 1723.47\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.85 .. Rec_loss: 1725.62 .. NELBO: 1726.47\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.4 .. Rec_loss: 1712.83 .. NELBO: 1714.23\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.22 .. Rec_loss: 1721.41 .. NELBO: 1722.63\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 1.36 .. Rec_loss: 1723.71 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.18 .. Rec_loss: 1711.05 .. NELBO: 1713.23\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.43 .. Rec_loss: 1721.57 .. NELBO: 1723.0\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 1.37 .. Rec_loss: 1724.27 .. NELBO: 1725.64\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.01 .. Rec_loss: 1710.28 .. NELBO: 1712.29\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.3 .. Rec_loss: 1720.51 .. NELBO: 1721.81\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 1.43 .. Rec_loss: 1722.32 .. NELBO: 1723.75\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.19 .. Rec_loss: 1705.86 .. NELBO: 1708.05\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.54 .. Rec_loss: 1717.02 .. NELBO: 1718.56\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.79 .. Rec_loss: 1718.82 .. NELBO: 1720.61\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1704.42 .. NELBO: 1707.0\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.82 .. Rec_loss: 1715.43 .. NELBO: 1717.25\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 2.03 .. Rec_loss: 1717.18 .. NELBO: 1719.21\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.52 .. Rec_loss: 1703.45 .. NELBO: 1705.97\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.82 .. Rec_loss: 1714.54 .. NELBO: 1716.36\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.14 .. Rec_loss: 1716.22 .. NELBO: 1718.36\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.35 .. Rec_loss: 1703.06 .. NELBO: 1705.41\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.71 .. Rec_loss: 1714.32 .. NELBO: 1716.03\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 1.95 .. Rec_loss: 1715.95 .. NELBO: 1717.9\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1703.63 .. NELBO: 1706.16\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.67 .. Rec_loss: 1715.34 .. NELBO: 1717.01\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 1.88 .. Rec_loss: 1716.79 .. NELBO: 1718.67\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1705.9 .. NELBO: 1708.11\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.56 .. Rec_loss: 1716.95 .. NELBO: 1718.51\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 1.64 .. Rec_loss: 1718.47 .. NELBO: 1720.11\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1705.67 .. NELBO: 1708.18\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.87 .. Rec_loss: 1716.55 .. NELBO: 1718.42\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 1.9 .. Rec_loss: 1718.31 .. NELBO: 1720.21\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1703.63 .. NELBO: 1706.22\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.95 .. Rec_loss: 1714.57 .. NELBO: 1716.52\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.06 .. Rec_loss: 1716.15 .. NELBO: 1718.21\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1701.99 .. NELBO: 1704.59\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.02 .. Rec_loss: 1712.9 .. NELBO: 1714.92\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.25 .. Rec_loss: 1714.33 .. NELBO: 1716.58\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1701.34 .. NELBO: 1703.83\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.01 .. Rec_loss: 1712.29 .. NELBO: 1714.3\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1713.82 .. NELBO: 1716.06\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1700.71 .. NELBO: 1703.32\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.06 .. Rec_loss: 1711.76 .. NELBO: 1713.82\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.28 .. Rec_loss: 1713.35 .. NELBO: 1715.63\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1700.58 .. NELBO: 1703.23\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.08 .. Rec_loss: 1711.73 .. NELBO: 1713.81\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 1713.3 .. NELBO: 1715.63\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1700.48 .. NELBO: 1703.14\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.09 .. Rec_loss: 1711.56 .. NELBO: 1713.65\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 1713.19 .. NELBO: 1715.53\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1700.96 .. NELBO: 1703.58\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.07 .. Rec_loss: 1711.8 .. NELBO: 1713.87\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.3 .. Rec_loss: 1713.38 .. NELBO: 1715.68\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1700.67 .. NELBO: 1703.37\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1711.87 .. NELBO: 1714.04\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1713.32 .. NELBO: 1715.82\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1700.29 .. NELBO: 1702.93\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1711.35 .. NELBO: 1713.56\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1712.92 .. NELBO: 1715.29\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1700.13 .. NELBO: 1703.02\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 1711.02 .. NELBO: 1713.36\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1712.44 .. NELBO: 1715.09\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1699.72 .. NELBO: 1702.45\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.31 .. Rec_loss: 1711.08 .. NELBO: 1713.39\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1712.41 .. NELBO: 1714.99\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1699.77 .. NELBO: 1702.76\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1711.35 .. NELBO: 1713.86\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1712.53 .. NELBO: 1715.32\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.38 .. Rec_loss: 1699.01 .. NELBO: 1702.39\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1711.16 .. NELBO: 1713.88\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1712.36 .. NELBO: 1715.22\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.64 .. Rec_loss: 1698.18 .. NELBO: 1701.82\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1709.95 .. NELBO: 1712.88\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1711.23 .. NELBO: 1714.32\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.25 .. Rec_loss: 1696.24 .. NELBO: 1700.49\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.44 .. Rec_loss: 1708.27 .. NELBO: 1711.71\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.51 .. Rec_loss: 1709.45 .. NELBO: 1712.96\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.3 .. Rec_loss: 1693.18 .. NELBO: 1697.48\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.68 .. Rec_loss: 1705.7 .. NELBO: 1709.38\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.82 .. Rec_loss: 1706.91 .. NELBO: 1710.73\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.4 .. Rec_loss: 1691.94 .. NELBO: 1696.34\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.92 .. Rec_loss: 1704.04 .. NELBO: 1707.96\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 4.06 .. Rec_loss: 1705.39 .. NELBO: 1709.45\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.54 .. Rec_loss: 1690.69 .. NELBO: 1695.23\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.92 .. Rec_loss: 1703.17 .. NELBO: 1707.09\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 4.12 .. Rec_loss: 1704.36 .. NELBO: 1708.48\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1690.02 .. NELBO: 1694.86\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.0 .. Rec_loss: 1702.77 .. NELBO: 1706.77\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 4.17 .. Rec_loss: 1703.97 .. NELBO: 1708.14\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.81 .. Rec_loss: 1688.89 .. NELBO: 1693.7\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1701.54 .. NELBO: 1705.68\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 4.24 .. Rec_loss: 1703.06 .. NELBO: 1707.3\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.86 .. Rec_loss: 1688.26 .. NELBO: 1693.12\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 1701.14 .. NELBO: 1705.33\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 4.24 .. Rec_loss: 1702.64 .. NELBO: 1706.88\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1687.79 .. NELBO: 1692.69\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.29 .. Rec_loss: 1700.37 .. NELBO: 1704.66\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 4.38 .. Rec_loss: 1702.0 .. NELBO: 1706.38\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1687.78 .. NELBO: 1692.84\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.38 .. Rec_loss: 1700.38 .. NELBO: 1704.76\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 4.53 .. Rec_loss: 1701.9 .. NELBO: 1706.43\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.98 .. Rec_loss: 1688.09 .. NELBO: 1693.07\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.36 .. Rec_loss: 1700.6 .. NELBO: 1704.96\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4.58 .. Rec_loss: 1702.04 .. NELBO: 1706.62\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.01 .. Rec_loss: 1687.88 .. NELBO: 1692.89\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1700.11 .. NELBO: 1704.53\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4.72 .. Rec_loss: 1701.59 .. NELBO: 1706.31\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1688.24 .. NELBO: 1693.27\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.44 .. Rec_loss: 1700.15 .. NELBO: 1704.59\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 4.66 .. Rec_loss: 1701.63 .. NELBO: 1706.29\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.27 .. Rec_loss: 1687.79 .. NELBO: 1693.06\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.64 .. Rec_loss: 1700.1 .. NELBO: 1704.74\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 4.83 .. Rec_loss: 1701.63 .. NELBO: 1706.46\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1688.42 .. NELBO: 1693.45\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.71 .. Rec_loss: 1700.17 .. NELBO: 1704.88\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1701.72 .. NELBO: 1706.62\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1687.84 .. NELBO: 1692.79\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.8 .. Rec_loss: 1700.31 .. NELBO: 1705.11\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1701.89 .. NELBO: 1706.84\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1687.85 .. NELBO: 1692.91\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1701.0 .. NELBO: 1705.84\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 4.93 .. Rec_loss: 1702.64 .. NELBO: 1707.57\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.13 .. Rec_loss: 1687.84 .. NELBO: 1692.97\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1701.63 .. NELBO: 1706.6\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1703.24 .. NELBO: 1708.31\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.05 .. Rec_loss: 1687.88 .. NELBO: 1692.93\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.79 .. Rec_loss: 1700.55 .. NELBO: 1705.34\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 4.96 .. Rec_loss: 1702.4 .. NELBO: 1707.36\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.53 .. Rec_loss: 1689.94 .. NELBO: 1694.47\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.61 .. Rec_loss: 1700.48 .. NELBO: 1705.09\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 4.89 .. Rec_loss: 1702.08 .. NELBO: 1706.97\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.76 .. Rec_loss: 1687.38 .. NELBO: 1692.14\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.72 .. Rec_loss: 1699.58 .. NELBO: 1704.3\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1701.48 .. NELBO: 1706.43\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.75 .. Rec_loss: 1687.09 .. NELBO: 1691.84\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 1699.41 .. NELBO: 1704.11\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1701.43 .. NELBO: 1706.4\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1686.19 .. NELBO: 1691.37\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1698.47 .. NELBO: 1703.42\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 5.2 .. Rec_loss: 1700.34 .. NELBO: 1705.54\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.21 .. Rec_loss: 1686.22 .. NELBO: 1691.43\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1698.27 .. NELBO: 1703.22\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 5.17 .. Rec_loss: 1699.94 .. NELBO: 1705.11\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.71 .. Rec_loss: 1686.47 .. NELBO: 1692.18\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.16 .. Rec_loss: 1698.46 .. NELBO: 1703.62\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 5.39 .. Rec_loss: 1700.1 .. NELBO: 1705.49\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1686.41 .. NELBO: 1691.9\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1698.48 .. NELBO: 1703.48\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1699.82 .. NELBO: 1705.05\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.82 .. Rec_loss: 1686.05 .. NELBO: 1691.87\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.25 .. Rec_loss: 1698.17 .. NELBO: 1703.42\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1699.54 .. NELBO: 1705.01\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1685.89 .. NELBO: 1691.61\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.26 .. Rec_loss: 1698.07 .. NELBO: 1703.33\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 5.46 .. Rec_loss: 1699.33 .. NELBO: 1704.79\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1685.61 .. NELBO: 1691.33\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.31 .. Rec_loss: 1697.86 .. NELBO: 1703.17\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1699.23 .. NELBO: 1704.7\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 1685.73 .. NELBO: 1691.48\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1697.38 .. NELBO: 1702.82\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1698.8 .. NELBO: 1704.36\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1685.0 .. NELBO: 1690.8\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1696.92 .. NELBO: 1702.39\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 5.59 .. Rec_loss: 1698.38 .. NELBO: 1703.97\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.74 .. Rec_loss: 1683.97 .. NELBO: 1689.71\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.46 .. Rec_loss: 1696.28 .. NELBO: 1701.74\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 5.55 .. Rec_loss: 1697.91 .. NELBO: 1703.46\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 1683.79 .. NELBO: 1689.47\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.52 .. Rec_loss: 1696.17 .. NELBO: 1701.69\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 5.61 .. Rec_loss: 1697.7 .. NELBO: 1703.31\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1683.8 .. NELBO: 1689.36\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.53 .. Rec_loss: 1696.11 .. NELBO: 1701.64\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 5.62 .. Rec_loss: 1697.75 .. NELBO: 1703.37\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.65 .. Rec_loss: 1684.1 .. NELBO: 1689.75\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.38 .. Rec_loss: 1696.11 .. NELBO: 1701.49\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 5.59 .. Rec_loss: 1697.63 .. NELBO: 1703.22\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.14 .. Rec_loss: 1682.61 .. NELBO: 1688.75\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1694.74 .. NELBO: 1700.54\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 5.87 .. Rec_loss: 1696.35 .. NELBO: 1702.22\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.61 .. Rec_loss: 1683.81 .. NELBO: 1689.42\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.73 .. Rec_loss: 1695.34 .. NELBO: 1701.07\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 5.82 .. Rec_loss: 1696.87 .. NELBO: 1702.69\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1683.34 .. NELBO: 1688.83\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.7 .. Rec_loss: 1695.09 .. NELBO: 1700.79\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1696.62 .. NELBO: 1702.42\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.41 .. Rec_loss: 1684.29 .. NELBO: 1689.7\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.6 .. Rec_loss: 1695.29 .. NELBO: 1700.89\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 1696.72 .. NELBO: 1702.47\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.34 .. Rec_loss: 1684.11 .. NELBO: 1689.45\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.56 .. Rec_loss: 1695.25 .. NELBO: 1700.81\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 5.76 .. Rec_loss: 1696.82 .. NELBO: 1702.58\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.2 .. Rec_loss: 1685.32 .. NELBO: 1690.52\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.36 .. Rec_loss: 1696.67 .. NELBO: 1702.03\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1698.08 .. NELBO: 1703.62\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1688.78 .. NELBO: 1693.78\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1698.86 .. NELBO: 1704.09\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 5.42 .. Rec_loss: 1700.0 .. NELBO: 1705.42\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1684.83 .. NELBO: 1689.83\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.55 .. Rec_loss: 1695.93 .. NELBO: 1701.48\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 5.74 .. Rec_loss: 1697.38 .. NELBO: 1703.12\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.16 .. Rec_loss: 1684.78 .. NELBO: 1689.94\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.55 .. Rec_loss: 1696.72 .. NELBO: 1702.27\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 5.78 .. Rec_loss: 1698.05 .. NELBO: 1703.83\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1687.88 .. NELBO: 1692.88\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1698.5 .. NELBO: 1703.94\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1699.61 .. NELBO: 1705.33\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 1685.34 .. NELBO: 1690.25\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.65 .. Rec_loss: 1696.94 .. NELBO: 1702.59\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 5.98 .. Rec_loss: 1698.17 .. NELBO: 1704.15\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.87 .. Rec_loss: 1686.96 .. NELBO: 1691.83\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.59 .. Rec_loss: 1697.81 .. NELBO: 1703.4\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 5.94 .. Rec_loss: 1698.89 .. NELBO: 1704.83\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1686.44 .. NELBO: 1691.47\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.54 .. Rec_loss: 1698.0 .. NELBO: 1703.54\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 5.79 .. Rec_loss: 1699.17 .. NELBO: 1704.96\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.99 .. Rec_loss: 1688.41 .. NELBO: 1693.4\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.34 .. Rec_loss: 1699.32 .. NELBO: 1704.66\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 5.61 .. Rec_loss: 1700.34 .. NELBO: 1705.95\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1685.31 .. NELBO: 1690.39\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1696.83 .. NELBO: 1702.41\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1698.16 .. NELBO: 1703.96\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.4 .. Rec_loss: 1685.65 .. NELBO: 1691.05\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1696.86 .. NELBO: 1702.55\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 1698.07 .. NELBO: 1704.07\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.26 .. Rec_loss: 1685.15 .. NELBO: 1690.41\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.43 .. Rec_loss: 1696.8 .. NELBO: 1702.23\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1698.1 .. NELBO: 1703.82\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1684.1 .. NELBO: 1689.57\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.4 .. Rec_loss: 1696.02 .. NELBO: 1701.42\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 5.78 .. Rec_loss: 1697.28 .. NELBO: 1703.06\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1683.9 .. NELBO: 1689.48\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.33 .. Rec_loss: 1695.79 .. NELBO: 1701.12\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 5.69 .. Rec_loss: 1697.07 .. NELBO: 1702.76\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 1682.89 .. NELBO: 1688.64\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.38 .. Rec_loss: 1695.06 .. NELBO: 1700.44\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 5.68 .. Rec_loss: 1696.26 .. NELBO: 1701.94\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.0 .. Rec_loss: 1681.59 .. NELBO: 1687.59\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.45 .. Rec_loss: 1694.31 .. NELBO: 1699.76\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 5.73 .. Rec_loss: 1695.61 .. NELBO: 1701.34\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 1681.25 .. NELBO: 1687.21\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.47 .. Rec_loss: 1693.91 .. NELBO: 1699.38\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 5.75 .. Rec_loss: 1695.16 .. NELBO: 1700.91\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Initialize metric\n",
    "eval_metric = CoherenceDiversityCombination(dataset) # Initialize metric\n",
    "\n",
    "# Initialize odel\n",
    "model = ETM(\n",
    "    use_partitions=False,\n",
    "    device='cuda',\n",
    "    embeddings_path='./data/chilit-19th-century-averaged-embeddings.txt',\n",
    ")\n",
    "\n",
    "# Define the search space.\n",
    "search_space = {\n",
    "    \"num_topics\": Integer(10,50),\n",
    "    \"dropout\" : Real(low=0, high=0.60),\n",
    "    \"t_hidden_size\" : Categorical([50, 100, 200, 300]),\n",
    "    \"activation\" : Categorical([\"softplus\", \"relu\", \"sigmoid\"])\n",
    "}\n",
    "\n",
    "# Initialize an optimizer object and start the optimization.\n",
    "optimizer=Optimizer()\n",
    "optResult=optimizer.optimize(\n",
    "    model, dataset,\n",
    "    eval_metric,\n",
    "    search_space,\n",
    "    save_name='OCTIS_ETM',\n",
    "    save_path=octis_folder,\n",
    "    number_of_call=50, # number of optimization iterations\n",
    "    model_runs=5  # number of runs of the topic model\n",
    ")\n",
    "\n",
    "# Save the results of th optimization in a csv file\n",
    "optResult.save_to_csv(\"OCTIS_ETM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fca698-5e69-4f3a-8cb1-746cb3386c19",
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1755548551034,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "12fca698-5e69-4f3a-8cb1-746cb3386c19"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9D162fu12Sti",
   "metadata": {
    "id": "9D162fu12Sti"
   },
   "source": [
    "### Train ETM model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "S9AxOjzXXa-P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "error",
     "timestamp": 1755549927301,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "S9AxOjzXXa-P",
    "outputId": "1dad4f07-93a0-44f9-a354-f41d00fbd0fc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5920383989283502, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=10, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 1991.97 .. NELBO: 1992.31\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 1878.18 .. NELBO: 1878.68\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 1866.79 .. NELBO: 1867.33\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1723.14 .. NELBO: 1723.2\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1731.0 .. NELBO: 1731.04\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.31 .. NELBO: 1734.35\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1719.95 .. NELBO: 1719.99\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1728.1 .. NELBO: 1728.13\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1731.3 .. NELBO: 1731.33\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1718.55 .. NELBO: 1718.57\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.74 .. NELBO: 1726.75\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.89 .. NELBO: 1729.9\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.76 .. NELBO: 1717.76\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.94 .. NELBO: 1725.94\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.07 .. NELBO: 1729.07\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.17 .. NELBO: 1717.17\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.32 .. NELBO: 1725.32\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.48 .. NELBO: 1728.48\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.6 .. NELBO: 1716.6\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.0 .. NELBO: 1728.0\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.09 .. NELBO: 1716.09\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.58 .. NELBO: 1727.58\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.76 .. NELBO: 1715.76\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.12 .. NELBO: 1724.12\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.25 .. NELBO: 1727.25\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.44 .. NELBO: 1715.44\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.82 .. NELBO: 1723.82\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.94 .. NELBO: 1726.94\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.33 .. NELBO: 1715.33\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.62 .. NELBO: 1723.62\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.76 .. NELBO: 1726.76\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.96 .. NELBO: 1714.96\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.34 .. NELBO: 1723.34\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.44 .. NELBO: 1726.44\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.8 .. NELBO: 1714.8\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.15 .. NELBO: 1723.15\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.26 .. NELBO: 1726.26\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.7 .. NELBO: 1714.7\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.06 .. NELBO: 1723.06\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.17 .. NELBO: 1726.17\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.54 .. NELBO: 1714.54\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.94 .. NELBO: 1722.94\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.02 .. NELBO: 1726.02\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.39 .. NELBO: 1714.39\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.78 .. NELBO: 1722.78\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.86 .. NELBO: 1725.86\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.3 .. NELBO: 1714.3\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.67 .. NELBO: 1722.67\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.75 .. NELBO: 1725.75\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.15 .. NELBO: 1714.15\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.63 .. NELBO: 1725.63\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.12 .. NELBO: 1714.12\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.5 .. NELBO: 1722.5\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.58 .. NELBO: 1725.58\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.01 .. NELBO: 1714.01\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.45 .. NELBO: 1722.45\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.51 .. NELBO: 1725.51\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.0 .. NELBO: 1714.0\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.41 .. NELBO: 1722.41\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.46 .. NELBO: 1725.46\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.97 .. NELBO: 1713.97\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.35 .. NELBO: 1722.35\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.39 .. NELBO: 1725.39\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.96 .. NELBO: 1713.96\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.29 .. NELBO: 1722.29\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.37 .. NELBO: 1725.37\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.77 .. NELBO: 1713.77\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.26 .. NELBO: 1722.26\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.3 .. NELBO: 1725.3\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.75 .. NELBO: 1713.75\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.17 .. NELBO: 1722.17\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.77 .. NELBO: 1713.77\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.7 .. NELBO: 1713.7\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.15 .. NELBO: 1722.15\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.22 .. NELBO: 1725.22\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.57 .. NELBO: 1713.57\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.08 .. NELBO: 1725.08\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.61 .. NELBO: 1713.61\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.0 .. NELBO: 1725.0\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.32 .. NELBO: 1713.32\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.6 .. NELBO: 1724.6\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.33 .. NELBO: 1721.33\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.38 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.85 .. NELBO: 1712.85\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.78 .. NELBO: 1712.78\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.3 .. NELBO: 1721.3\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 51 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.27 .. NELBO: 1721.27\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.68 .. NELBO: 1712.68\n",
      "Epoch: 52 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.27 .. NELBO: 1721.27\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.77 .. NELBO: 1712.77\n",
      "Epoch: 53 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.29 .. NELBO: 1721.29\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 54 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.29 .. NELBO: 1721.29\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.35 .. NELBO: 1724.35\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.79 .. NELBO: 1712.79\n",
      "Epoch: 55 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.78 .. NELBO: 1712.78\n",
      "Epoch: 56 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.82 .. NELBO: 1712.82\n",
      "Epoch: 57 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.25 .. NELBO: 1721.25\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.28 .. NELBO: 1724.28\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.82 .. NELBO: 1712.82\n",
      "Epoch: 58 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.23 .. NELBO: 1721.23\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.65 .. NELBO: 1712.65\n",
      "Epoch: 59 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.27 .. NELBO: 1721.27\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.3 .. NELBO: 1724.3\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.83 .. NELBO: 1712.83\n",
      "Epoch: 60 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.3 .. NELBO: 1724.3\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 61 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.29 .. NELBO: 1721.29\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.67 .. NELBO: 1712.67\n",
      "Epoch: 62 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.29 .. NELBO: 1721.29\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.75 .. NELBO: 1712.75\n",
      "Epoch: 63 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.28 .. NELBO: 1721.28\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 64 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.29 .. NELBO: 1721.29\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.77 .. NELBO: 1712.77\n",
      "Epoch: 65 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.24 .. NELBO: 1721.24\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.3 .. NELBO: 1724.3\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.72 .. NELBO: 1712.72\n",
      "Epoch: 66 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.28 .. NELBO: 1721.28\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.3 .. NELBO: 1724.3\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.8 .. NELBO: 1712.8\n",
      "Epoch: 67 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.25 .. NELBO: 1721.25\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.27 .. NELBO: 1724.27\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.77 .. NELBO: 1712.77\n",
      "Epoch: 68 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.18 .. NELBO: 1721.18\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.23 .. NELBO: 1724.23\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.59 .. NELBO: 1712.59\n",
      "Epoch: 69 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.18 .. NELBO: 1721.18\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.22 .. NELBO: 1724.22\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.67 .. NELBO: 1712.67\n",
      "Epoch: 70 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.16 .. NELBO: 1721.16\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.16 .. NELBO: 1724.16\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.85 .. NELBO: 1712.85\n",
      "Epoch: 71 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.2 .. NELBO: 1721.2\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.26 .. NELBO: 1724.26\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.67 .. NELBO: 1712.67\n",
      "Epoch: 72 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.22 .. NELBO: 1721.22\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.25 .. NELBO: 1724.25\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.6 .. NELBO: 1712.6\n",
      "Epoch: 73 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.2 .. NELBO: 1721.2\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.19 .. NELBO: 1724.19\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 74 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.22 .. NELBO: 1721.22\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.28 .. NELBO: 1724.28\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.7 .. NELBO: 1712.7\n",
      "Epoch: 75 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.24 .. NELBO: 1721.24\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.27 .. NELBO: 1724.27\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.6 .. NELBO: 1712.6\n",
      "Epoch: 76 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.22 .. NELBO: 1721.22\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.19 .. NELBO: 1724.19\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 77 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.26 .. NELBO: 1721.26\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.3 .. NELBO: 1724.3\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.76 .. NELBO: 1712.76\n",
      "Epoch: 78 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.69 .. NELBO: 1712.69\n",
      "Epoch: 79 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.28 .. NELBO: 1724.28\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 80 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.39 .. NELBO: 1724.39\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 81 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.79 .. NELBO: 1712.79\n",
      "Epoch: 82 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 83 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1721.44 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.42 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 84 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.66 .. NELBO: 1712.66\n",
      "Epoch: 85 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.8 .. NELBO: 1712.8\n",
      "Epoch: 86 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.32 .. NELBO: 1721.32\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.29 .. NELBO: 1724.29\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 87 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.75 .. NELBO: 1712.75\n",
      "Epoch: 88 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 89 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.39 .. NELBO: 1724.39\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 90 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.78 .. NELBO: 1712.78\n",
      "Epoch: 91 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.75 .. NELBO: 1712.75\n",
      "Epoch: 92 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 93 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.69 .. NELBO: 1712.69\n",
      "Epoch: 94 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.69 .. NELBO: 1712.69\n",
      "Epoch: 95 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.29 .. NELBO: 1724.29\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 96 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.75 .. NELBO: 1712.75\n",
      "Epoch: 97 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.64 .. NELBO: 1712.64\n",
      "Epoch: 98 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 99 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 100 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "results = json.load(open(f\"{octis_folder}/OCTIS_ETM.json\",'r'))\n",
    "best_iter = results['f_val'].index(max(results['f_val']))\n",
    "model = ETM(\n",
    "    num_topics=results['x_iters']['num_topics'][best_iter],\n",
    "    dropout=results['x_iters']['dropout'][best_iter],\n",
    "    t_hidden_size=results['x_iters']['t_hidden_size'][best_iter],\n",
    "    activation=results['x_iters']['activation'][best_iter],\n",
    "    device = 'cuda',\n",
    "    embeddings_path='./data/chilit-19th-century-averaged-embeddings.txt',\n",
    "    use_partitions=False\n",
    ")\n",
    "\n",
    "output = model.train_model(dataset)\n",
    "pickle.dump(output, open(octis_folder + \"OCTIS_ETM_Output.pkl\", \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PrMEN2KCXgdj",
   "metadata": {
    "executionInfo": {
     "elapsed": 236,
     "status": "aborted",
     "timestamp": 1755549927456,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "PrMEN2KCXgdj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cDSoA2fq2l6E",
   "metadata": {
    "id": "cDSoA2fq2l6E"
   },
   "source": [
    "### Optuna ETM multi-objective optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rsGh1odSH-91",
   "metadata": {
    "id": "rsGh1odSH-91"
   },
   "outputs": [],
   "source": [
    "optuna.delete_study(study_name=\"ETM_Study\", storage=f\"sqlite:///{optuna_folder}ETM_Study.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jyJA4vl5bARN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3745633,
     "status": "ok",
     "timestamp": 1755538506048,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "jyJA4vl5bARN",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7a48a66a-3053-48d7-83cd-9c6380f47831"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:32:42,639] A new study created in RDB with name: ETM_Study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5017876433740717, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=26, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=26, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=26, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 2001.76 .. NELBO: 2002.07\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 1883.95 .. NELBO: 1884.36\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.42 .. Rec_loss: 1871.89 .. NELBO: 1872.31\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.16 .. NELBO: 1723.21\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1730.94 .. NELBO: 1730.98\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.2 .. NELBO: 1734.24\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.81 .. NELBO: 1719.83\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.99 .. NELBO: 1728.0\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.18 .. NELBO: 1731.19\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.43 .. NELBO: 1718.44\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.62 .. NELBO: 1726.62\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.78 .. NELBO: 1729.78\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.66 .. NELBO: 1717.66\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.83 .. NELBO: 1725.83\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.98 .. NELBO: 1728.98\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.01 .. NELBO: 1717.01\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.21 .. NELBO: 1725.21\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.36 .. NELBO: 1728.36\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.48 .. NELBO: 1716.48\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.87 .. NELBO: 1727.87\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.04 .. NELBO: 1716.04\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.47 .. NELBO: 1727.47\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.67 .. NELBO: 1715.67\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.98 .. NELBO: 1723.98\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.13 .. NELBO: 1727.13\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.33 .. NELBO: 1715.33\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.71 .. NELBO: 1723.71\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.85 .. NELBO: 1726.85\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.08 .. NELBO: 1715.08\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.49 .. NELBO: 1723.49\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.6 .. NELBO: 1726.6\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.15 .. NELBO: 1715.15\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.43 .. NELBO: 1723.43\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.58 .. NELBO: 1726.58\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.74 .. NELBO: 1714.74\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.19 .. NELBO: 1723.19\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.29 .. NELBO: 1726.29\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.65 .. NELBO: 1714.65\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.0 .. NELBO: 1723.0\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.1 .. NELBO: 1726.1\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.45 .. NELBO: 1714.45\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.82 .. NELBO: 1722.82\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.92 .. NELBO: 1725.92\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.33 .. NELBO: 1714.33\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.71 .. NELBO: 1722.71\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.79 .. NELBO: 1725.79\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.27 .. NELBO: 1714.27\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.66 .. NELBO: 1722.66\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.73 .. NELBO: 1725.73\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.2 .. NELBO: 1714.2\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.64 .. NELBO: 1725.64\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.15 .. NELBO: 1714.15\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.49 .. NELBO: 1722.49\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.57 .. NELBO: 1725.57\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.02 .. NELBO: 1714.02\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.44 .. NELBO: 1722.44\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.51 .. NELBO: 1725.51\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.88 .. NELBO: 1713.88\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.36 .. NELBO: 1722.36\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.4 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.88 .. NELBO: 1713.88\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.27 .. NELBO: 1722.27\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.32 .. NELBO: 1725.32\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.82 .. NELBO: 1713.82\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.19 .. NELBO: 1722.19\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.24 .. NELBO: 1725.24\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.73 .. NELBO: 1713.73\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.13 .. NELBO: 1722.13\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.63 .. NELBO: 1713.63\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.17 .. NELBO: 1722.17\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.19 .. NELBO: 1725.19\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.72 .. NELBO: 1713.72\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.13 .. NELBO: 1725.13\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.62 .. NELBO: 1713.62\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.58 .. NELBO: 1713.58\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.98 .. NELBO: 1724.98\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.98 .. NELBO: 1721.98\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.03 .. NELBO: 1725.03\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.35 .. NELBO: 1713.35\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.7 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.14 .. NELBO: 1713.14\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.83 .. NELBO: 1712.83\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:34:00,834] Trial 0 finished with values: [-0.00842551799202895, 0.046153846153846156] and parameters: {'num_topics': 26, 'dropout': 0.5017876433740717, 't_hidden_size': 100, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.23344863649595227, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=21, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=21, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=21, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.76 .. Rec_loss: 2015.32 .. NELBO: 2017.08\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.93 .. Rec_loss: 1891.9 .. NELBO: 1892.83\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.83 .. Rec_loss: 1879.11 .. NELBO: 1879.94\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.48 .. NELBO: 1723.48\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1730.98 .. NELBO: 1730.98\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1734.26 .. NELBO: 1734.26\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.65 .. NELBO: 1719.65\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.77 .. NELBO: 1727.77\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1730.99 .. NELBO: 1730.99\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.23 .. NELBO: 1718.23\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.46 .. NELBO: 1726.46\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.63 .. NELBO: 1729.63\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.51 .. NELBO: 1717.51\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.77 .. NELBO: 1725.77\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.9 .. NELBO: 1728.9\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.98 .. NELBO: 1716.98\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.32 .. NELBO: 1728.32\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.51 .. NELBO: 1716.51\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.87 .. NELBO: 1727.87\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.0 .. NELBO: 1716.0\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.46 .. NELBO: 1727.46\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.62 .. NELBO: 1715.62\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.02 .. NELBO: 1724.02\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.18 .. NELBO: 1727.18\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.32 .. NELBO: 1715.32\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.78 .. NELBO: 1723.78\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.88 .. NELBO: 1726.88\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.36 .. NELBO: 1715.36\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.61 .. NELBO: 1723.61\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.75 .. NELBO: 1726.75\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.94 .. NELBO: 1714.94\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.35 .. NELBO: 1723.35\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.47 .. NELBO: 1726.47\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.71 .. NELBO: 1714.71\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.14 .. NELBO: 1723.14\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.22 .. NELBO: 1726.22\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.84 .. NELBO: 1714.84\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.1 .. NELBO: 1723.1\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.24 .. NELBO: 1726.24\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.49 .. NELBO: 1714.49\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.01 .. NELBO: 1723.01\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.09 .. NELBO: 1726.09\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.45 .. NELBO: 1714.45\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.81 .. NELBO: 1722.81\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.9 .. NELBO: 1725.9\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.44 .. NELBO: 1714.44\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.73 .. NELBO: 1722.73\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.85 .. NELBO: 1725.85\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.1 .. NELBO: 1714.1\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.62 .. NELBO: 1722.62\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.67 .. NELBO: 1725.67\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.22 .. NELBO: 1714.22\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.54 .. NELBO: 1722.54\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.64 .. NELBO: 1725.64\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.13 .. NELBO: 1714.13\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.51 .. NELBO: 1722.51\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.6 .. NELBO: 1725.6\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.87 .. NELBO: 1713.87\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.38 .. NELBO: 1722.38\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.45 .. NELBO: 1725.45\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.85 .. NELBO: 1713.85\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.3 .. NELBO: 1722.3\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.33 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.91 .. NELBO: 1713.91\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.21 .. NELBO: 1722.21\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.3 .. NELBO: 1725.3\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.1 .. NELBO: 1722.1\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.58 .. NELBO: 1713.58\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.05 .. NELBO: 1722.05\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.08 .. NELBO: 1725.08\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.77 .. NELBO: 1713.77\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.19 .. NELBO: 1725.19\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.61 .. NELBO: 1713.61\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.04 .. NELBO: 1722.04\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.1 .. NELBO: 1725.1\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.51 .. NELBO: 1713.51\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.95 .. NELBO: 1721.95\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.0 .. NELBO: 1725.0\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.42 .. NELBO: 1713.42\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.81 .. NELBO: 1721.81\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.81 .. NELBO: 1721.81\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.38 .. NELBO: 1713.38\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.41 .. NELBO: 1713.41\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.81 .. NELBO: 1721.81\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1713.3 .. NELBO: 1713.31\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1721.53 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.58 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1712.77 .. NELBO: 1712.78\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:35:15,249] Trial 1 finished with values: [-0.00816951656097918, 0.05238095238095238] and parameters: {'num_topics': 21, 'dropout': 0.23344863649595227, 't_hidden_size': 300, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.37822912279481546, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=34, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=34, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=34, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.98 .. Rec_loss: 2006.28 .. NELBO: 2007.26\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 1886.94 .. NELBO: 1887.49\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 1874.65 .. NELBO: 1875.14\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.43 .. NELBO: 1723.44\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1730.97 .. NELBO: 1730.98\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1734.27 .. NELBO: 1734.28\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.67 .. NELBO: 1719.67\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.82 .. NELBO: 1727.82\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.04 .. NELBO: 1731.04\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.27 .. NELBO: 1718.27\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.5 .. NELBO: 1726.5\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.67 .. NELBO: 1729.67\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.57 .. NELBO: 1717.57\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.77 .. NELBO: 1725.77\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.91 .. NELBO: 1728.91\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.99 .. NELBO: 1716.99\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.33 .. NELBO: 1728.33\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.46 .. NELBO: 1716.46\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.86 .. NELBO: 1727.86\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.01 .. NELBO: 1716.01\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.46 .. NELBO: 1727.46\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.64 .. NELBO: 1715.64\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.01 .. NELBO: 1724.01\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.16 .. NELBO: 1727.16\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.36 .. NELBO: 1715.36\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.73 .. NELBO: 1723.73\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.86 .. NELBO: 1726.86\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.25 .. NELBO: 1715.25\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.55 .. NELBO: 1723.55\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.69 .. NELBO: 1726.69\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.86 .. NELBO: 1714.86\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.27 .. NELBO: 1723.27\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.39 .. NELBO: 1726.39\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.72 .. NELBO: 1714.72\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.08 .. NELBO: 1723.08\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.18 .. NELBO: 1726.18\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.63 .. NELBO: 1714.63\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.95 .. NELBO: 1722.95\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.09 .. NELBO: 1726.09\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.46 .. NELBO: 1714.46\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.93 .. NELBO: 1722.93\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.03 .. NELBO: 1726.03\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.37 .. NELBO: 1714.37\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.79 .. NELBO: 1722.79\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.86 .. NELBO: 1725.86\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.35 .. NELBO: 1714.35\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.67 .. NELBO: 1722.67\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.76 .. NELBO: 1725.76\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.16 .. NELBO: 1714.16\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.62 .. NELBO: 1722.62\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.71 .. NELBO: 1725.71\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.11 .. NELBO: 1714.11\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.61 .. NELBO: 1725.61\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.21 .. NELBO: 1714.21\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.53 .. NELBO: 1722.53\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.6 .. NELBO: 1725.6\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.03 .. NELBO: 1714.03\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.42 .. NELBO: 1722.42\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.48 .. NELBO: 1725.48\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.83 .. NELBO: 1713.83\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.24 .. NELBO: 1722.24\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.31 .. NELBO: 1725.31\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.76 .. NELBO: 1713.76\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.16 .. NELBO: 1722.16\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.21 .. NELBO: 1725.21\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.66 .. NELBO: 1713.66\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.15 .. NELBO: 1725.15\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.63 .. NELBO: 1713.63\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.06 .. NELBO: 1722.06\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.69 .. NELBO: 1713.69\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.08 .. NELBO: 1722.08\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.15 .. NELBO: 1725.15\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.56 .. NELBO: 1713.56\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.05 .. NELBO: 1722.05\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.09 .. NELBO: 1725.09\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.39 .. NELBO: 1713.39\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.77 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.32 .. NELBO: 1713.32\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.63 .. NELBO: 1713.63\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.99 .. NELBO: 1724.99\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.41 .. NELBO: 1713.41\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.38 .. NELBO: 1713.38\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.68 .. NELBO: 1721.68\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.85 .. NELBO: 1712.85\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.35 .. NELBO: 1721.35\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.39 .. NELBO: 1724.39\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:36:30,867] Trial 2 finished with values: [-0.008843022643195624, 0.03235294117647059] and parameters: {'num_topics': 34, 'dropout': 0.37822912279481546, 't_hidden_size': 300, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.39586080414106467, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=31, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=31, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=31, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.16 .. Rec_loss: 2006.36 .. NELBO: 2007.52\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.66 .. Rec_loss: 1887.0 .. NELBO: 1887.66\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 1874.72 .. NELBO: 1875.31\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.48 .. NELBO: 1723.49\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.02 .. NELBO: 1731.03\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1734.33 .. NELBO: 1734.34\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.64 .. NELBO: 1719.64\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.81 .. NELBO: 1727.81\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.0 .. NELBO: 1731.0\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.3 .. NELBO: 1718.3\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.51 .. NELBO: 1726.51\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.67 .. NELBO: 1729.67\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.59 .. NELBO: 1717.59\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.77 .. NELBO: 1725.77\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.92 .. NELBO: 1728.92\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.98 .. NELBO: 1716.98\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.34 .. NELBO: 1728.34\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.48 .. NELBO: 1716.48\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.86 .. NELBO: 1727.86\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.06 .. NELBO: 1716.06\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.5 .. NELBO: 1727.5\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.63 .. NELBO: 1715.63\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.99 .. NELBO: 1723.99\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.14 .. NELBO: 1727.14\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.34 .. NELBO: 1715.34\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.69 .. NELBO: 1723.69\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.82 .. NELBO: 1726.82\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.09 .. NELBO: 1715.09\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.44 .. NELBO: 1723.44\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.57 .. NELBO: 1726.57\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.85 .. NELBO: 1714.85\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.22 .. NELBO: 1723.22\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.35 .. NELBO: 1726.35\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.68 .. NELBO: 1714.68\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.06 .. NELBO: 1723.06\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.18 .. NELBO: 1726.18\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.55 .. NELBO: 1714.55\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.93 .. NELBO: 1722.93\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.04 .. NELBO: 1726.04\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.45 .. NELBO: 1714.45\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.81 .. NELBO: 1722.81\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.91 .. NELBO: 1725.91\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.38 .. NELBO: 1714.38\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.74 .. NELBO: 1722.74\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.84 .. NELBO: 1725.84\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.19 .. NELBO: 1714.19\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.63 .. NELBO: 1722.63\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.7 .. NELBO: 1725.7\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.16 .. NELBO: 1714.16\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.55 .. NELBO: 1722.55\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.62 .. NELBO: 1725.62\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.09 .. NELBO: 1714.09\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.46 .. NELBO: 1722.46\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.54 .. NELBO: 1725.54\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.03 .. NELBO: 1714.03\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.46 .. NELBO: 1722.46\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.55 .. NELBO: 1725.55\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.95 .. NELBO: 1713.95\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.43 .. NELBO: 1722.43\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.47 .. NELBO: 1725.47\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.09 .. NELBO: 1714.09\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.39 .. NELBO: 1722.39\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.48 .. NELBO: 1725.48\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.89 .. NELBO: 1713.89\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.38 .. NELBO: 1722.38\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.42 .. NELBO: 1725.42\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.89 .. NELBO: 1713.89\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.28 .. NELBO: 1722.28\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.33 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.71 .. NELBO: 1713.71\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.14 .. NELBO: 1725.14\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.95 .. NELBO: 1721.95\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.0 .. NELBO: 1725.0\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.37 .. NELBO: 1713.37\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.75 .. NELBO: 1721.75\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.79 .. NELBO: 1724.79\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.75 .. NELBO: 1721.75\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.83 .. NELBO: 1712.83\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.32 .. NELBO: 1721.32\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.8 .. NELBO: 1712.8\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.34 .. NELBO: 1721.34\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.38 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.3 .. NELBO: 1721.3\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.84 .. NELBO: 1712.84\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.28 .. NELBO: 1721.28\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:37:47,480] Trial 3 finished with values: [-0.008319798632324162, 0.035483870967741936] and parameters: {'num_topics': 31, 'dropout': 0.39586080414106467, 't_hidden_size': 200, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.4078696512690512, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=50, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=50, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.29 .. Rec_loss: 2008.51 .. NELBO: 2008.8\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 1887.96 .. NELBO: 1888.23\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 1875.49 .. NELBO: 1875.76\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.42 .. NELBO: 1723.47\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1730.95 .. NELBO: 1730.99\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1734.24 .. NELBO: 1734.27\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1719.7 .. NELBO: 1719.71\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.89 .. NELBO: 1727.9\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.08 .. NELBO: 1731.09\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.35 .. NELBO: 1718.36\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.56 .. NELBO: 1726.57\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.71 .. NELBO: 1729.72\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.58 .. NELBO: 1717.59\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.75 .. NELBO: 1725.76\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1728.89 .. NELBO: 1728.91\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1716.89 .. NELBO: 1716.94\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1725.04 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1728.16 .. NELBO: 1728.24\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 1716.04 .. NELBO: 1716.27\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 1724.13 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 1727.27 .. NELBO: 1727.61\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.42 .. Rec_loss: 1715.18 .. NELBO: 1715.6\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.43 .. Rec_loss: 1723.44 .. NELBO: 1723.87\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.45 .. Rec_loss: 1726.6 .. NELBO: 1727.05\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 1714.93 .. NELBO: 1715.46\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 1723.29 .. NELBO: 1723.75\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 1726.43 .. NELBO: 1726.89\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.42 .. Rec_loss: 1715.02 .. NELBO: 1715.44\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.44 .. Rec_loss: 1723.09 .. NELBO: 1723.53\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.45 .. Rec_loss: 1726.24 .. NELBO: 1726.69\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 1714.19 .. NELBO: 1714.78\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 1722.36 .. NELBO: 1722.96\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 1725.5 .. NELBO: 1726.11\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.7 .. Rec_loss: 1713.52 .. NELBO: 1714.22\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 1721.87 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 1724.99 .. NELBO: 1725.68\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.8 .. Rec_loss: 1713.03 .. NELBO: 1713.83\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.82 .. Rec_loss: 1721.27 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.85 .. Rec_loss: 1724.36 .. NELBO: 1725.21\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.04 .. Rec_loss: 1712.11 .. NELBO: 1713.15\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.04 .. Rec_loss: 1720.55 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 1.13 .. Rec_loss: 1723.4 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.54 .. Rec_loss: 1710.98 .. NELBO: 1712.52\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.33 .. Rec_loss: 1719.77 .. NELBO: 1721.1\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 1.46 .. Rec_loss: 1722.37 .. NELBO: 1723.83\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.09 .. Rec_loss: 1709.54 .. NELBO: 1711.63\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.63 .. Rec_loss: 1718.7 .. NELBO: 1720.33\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 1.89 .. Rec_loss: 1720.91 .. NELBO: 1722.8\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1707.79 .. NELBO: 1710.23\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.8 .. Rec_loss: 1717.58 .. NELBO: 1719.38\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.3 .. Rec_loss: 1719.45 .. NELBO: 1721.75\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.46 .. Rec_loss: 1706.84 .. NELBO: 1709.3\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.81 .. Rec_loss: 1717.03 .. NELBO: 1718.84\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1718.74 .. NELBO: 1720.98\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1706.74 .. NELBO: 1709.52\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.05 .. Rec_loss: 1717.11 .. NELBO: 1719.16\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.25 .. Rec_loss: 1718.83 .. NELBO: 1721.08\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.44 .. Rec_loss: 1707.4 .. NELBO: 1710.84\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.45 .. Rec_loss: 1717.91 .. NELBO: 1720.36\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1720.14 .. NELBO: 1722.63\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.26 .. Rec_loss: 1705.46 .. NELBO: 1708.72\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1716.27 .. NELBO: 1718.63\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1717.81 .. NELBO: 1720.52\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1705.47 .. NELBO: 1708.47\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.39 .. Rec_loss: 1716.08 .. NELBO: 1718.47\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1718.37 .. NELBO: 1720.87\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1706.03 .. NELBO: 1709.01\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.28 .. Rec_loss: 1716.2 .. NELBO: 1718.48\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1717.85 .. NELBO: 1720.5\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1703.67 .. NELBO: 1706.77\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1714.4 .. NELBO: 1717.05\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1716.25 .. NELBO: 1719.08\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.25 .. Rec_loss: 1702.35 .. NELBO: 1705.6\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1713.42 .. NELBO: 1716.18\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1715.2 .. NELBO: 1718.3\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1701.94 .. NELBO: 1705.09\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1712.91 .. NELBO: 1715.66\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 3.11 .. Rec_loss: 1714.53 .. NELBO: 1717.64\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1701.75 .. NELBO: 1704.84\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.81 .. Rec_loss: 1712.76 .. NELBO: 1715.57\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 3.07 .. Rec_loss: 1714.41 .. NELBO: 1717.48\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1701.84 .. NELBO: 1704.86\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1712.95 .. NELBO: 1715.77\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 3.08 .. Rec_loss: 1714.63 .. NELBO: 1717.71\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1701.42 .. NELBO: 1704.35\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1712.8 .. NELBO: 1715.55\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.14 .. Rec_loss: 1714.25 .. NELBO: 1717.39\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1700.8 .. NELBO: 1703.63\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1712.04 .. NELBO: 1714.71\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1713.51 .. NELBO: 1716.56\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.11 .. Rec_loss: 1700.28 .. NELBO: 1703.39\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1711.6 .. NELBO: 1714.44\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.27 .. Rec_loss: 1713.1 .. NELBO: 1716.37\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1699.37 .. NELBO: 1702.28\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1711.25 .. NELBO: 1713.88\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.08 .. Rec_loss: 1712.9 .. NELBO: 1715.98\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1699.78 .. NELBO: 1702.8\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1711.38 .. NELBO: 1714.07\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.11 .. Rec_loss: 1712.96 .. NELBO: 1716.07\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1699.8 .. NELBO: 1702.57\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.45 .. Rec_loss: 1711.43 .. NELBO: 1713.88\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1713.11 .. NELBO: 1716.07\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1699.41 .. NELBO: 1702.56\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.57 .. Rec_loss: 1711.3 .. NELBO: 1713.87\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1712.75 .. NELBO: 1715.79\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.99 .. Rec_loss: 1697.14 .. NELBO: 1701.13\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1709.8 .. NELBO: 1712.79\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.41 .. Rec_loss: 1711.48 .. NELBO: 1714.89\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.01 .. Rec_loss: 1697.07 .. NELBO: 1701.08\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1709.8 .. NELBO: 1712.77\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.36 .. Rec_loss: 1711.52 .. NELBO: 1714.88\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.13 .. Rec_loss: 1697.51 .. NELBO: 1701.64\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1710.09 .. NELBO: 1713.06\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 1711.59 .. NELBO: 1714.98\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.31 .. Rec_loss: 1696.92 .. NELBO: 1701.23\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.24 .. Rec_loss: 1709.47 .. NELBO: 1712.71\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 3.67 .. Rec_loss: 1711.03 .. NELBO: 1714.7\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.29 .. Rec_loss: 1696.61 .. NELBO: 1700.9\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.31 .. Rec_loss: 1709.09 .. NELBO: 1712.4\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 3.77 .. Rec_loss: 1710.71 .. NELBO: 1714.48\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.1 .. Rec_loss: 1696.29 .. NELBO: 1700.39\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.4 .. Rec_loss: 1708.52 .. NELBO: 1711.92\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 3.86 .. Rec_loss: 1710.22 .. NELBO: 1714.08\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1696.23 .. NELBO: 1700.37\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.54 .. Rec_loss: 1708.57 .. NELBO: 1712.11\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 3.84 .. Rec_loss: 1710.33 .. NELBO: 1714.17\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.25 .. Rec_loss: 1696.03 .. NELBO: 1700.28\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.87 .. Rec_loss: 1708.26 .. NELBO: 1712.13\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 4.13 .. Rec_loss: 1710.09 .. NELBO: 1714.22\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.1 .. Rec_loss: 1696.13 .. NELBO: 1700.23\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.77 .. Rec_loss: 1708.52 .. NELBO: 1712.29\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 4.1 .. Rec_loss: 1710.32 .. NELBO: 1714.42\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.87 .. Rec_loss: 1696.61 .. NELBO: 1700.48\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.62 .. Rec_loss: 1708.84 .. NELBO: 1712.46\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1710.23 .. NELBO: 1714.37\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.51 .. Rec_loss: 1697.65 .. NELBO: 1701.16\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.35 .. Rec_loss: 1709.58 .. NELBO: 1712.93\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 3.83 .. Rec_loss: 1710.91 .. NELBO: 1714.74\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.5 .. Rec_loss: 1696.97 .. NELBO: 1700.47\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.52 .. Rec_loss: 1708.81 .. NELBO: 1712.33\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 3.89 .. Rec_loss: 1710.29 .. NELBO: 1714.18\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.79 .. Rec_loss: 1696.03 .. NELBO: 1699.82\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.62 .. Rec_loss: 1708.3 .. NELBO: 1711.92\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 4.08 .. Rec_loss: 1709.72 .. NELBO: 1713.8\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.99 .. Rec_loss: 1695.98 .. NELBO: 1699.97\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.63 .. Rec_loss: 1708.15 .. NELBO: 1711.78\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1709.52 .. NELBO: 1713.66\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.2 .. Rec_loss: 1695.32 .. NELBO: 1699.52\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.77 .. Rec_loss: 1707.87 .. NELBO: 1711.64\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 4.27 .. Rec_loss: 1709.27 .. NELBO: 1713.54\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:39:12,344] Trial 4 finished with values: [0.0045602776711707645, 0.134] and parameters: {'num_topics': 50, 'dropout': 0.4078696512690512, 't_hidden_size': 300, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.4640083858922023, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=22, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=22, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=22, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.95 .. Rec_loss: 1999.37 .. NELBO: 2000.32\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 1883.03 .. NELBO: 1883.6\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 1871.14 .. NELBO: 1871.65\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1723.38 .. NELBO: 1723.41\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1730.91 .. NELBO: 1730.93\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1734.22 .. NELBO: 1734.24\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1719.66 .. NELBO: 1719.67\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.86 .. NELBO: 1727.87\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.06 .. NELBO: 1731.07\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.34 .. NELBO: 1718.34\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.56 .. NELBO: 1726.56\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.72 .. NELBO: 1729.72\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.63 .. NELBO: 1717.63\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.82 .. NELBO: 1725.82\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.96 .. NELBO: 1728.96\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.03 .. NELBO: 1717.03\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.24 .. NELBO: 1725.24\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.38 .. NELBO: 1728.38\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.48 .. NELBO: 1716.48\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.88 .. NELBO: 1727.88\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.01 .. NELBO: 1716.01\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.47 .. NELBO: 1727.47\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.66 .. NELBO: 1715.66\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.99 .. NELBO: 1723.99\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.13 .. NELBO: 1727.13\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.38 .. NELBO: 1715.38\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.7 .. NELBO: 1723.7\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.85 .. NELBO: 1726.85\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.16 .. NELBO: 1715.16\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.55 .. NELBO: 1723.55\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.69 .. NELBO: 1726.69\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.01 .. NELBO: 1715.01\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.47 .. NELBO: 1723.47\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.56 .. NELBO: 1726.56\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.92 .. NELBO: 1714.92\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.25 .. NELBO: 1723.25\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.36 .. NELBO: 1726.36\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.7 .. NELBO: 1714.7\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.08 .. NELBO: 1723.08\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.17 .. NELBO: 1726.17\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.57 .. NELBO: 1714.57\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.91 .. NELBO: 1722.91\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.0 .. NELBO: 1726.0\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.37 .. NELBO: 1714.37\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.77 .. NELBO: 1722.77\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.86 .. NELBO: 1725.86\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.26 .. NELBO: 1714.26\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.66 .. NELBO: 1722.66\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.73 .. NELBO: 1725.73\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.23 .. NELBO: 1714.23\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.59 .. NELBO: 1722.59\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.66 .. NELBO: 1725.66\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.19 .. NELBO: 1714.19\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.63 .. NELBO: 1725.63\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.0 .. NELBO: 1714.0\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.43 .. NELBO: 1722.43\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.48 .. NELBO: 1725.48\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.98 .. NELBO: 1713.98\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.35 .. NELBO: 1722.35\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.4 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.85 .. NELBO: 1713.85\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.23 .. NELBO: 1722.23\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.29 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.76 .. NELBO: 1713.76\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.17 .. NELBO: 1722.17\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.23 .. NELBO: 1725.23\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.67 .. NELBO: 1713.67\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.14 .. NELBO: 1725.14\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.03 .. NELBO: 1722.03\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.09 .. NELBO: 1725.09\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.56 .. NELBO: 1713.56\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.04 .. NELBO: 1722.04\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.09 .. NELBO: 1725.09\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.55 .. NELBO: 1713.55\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.02 .. NELBO: 1722.02\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.04 .. NELBO: 1725.04\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.63 .. NELBO: 1713.63\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.0 .. NELBO: 1722.0\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.51 .. NELBO: 1713.51\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.98 .. NELBO: 1724.98\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.03 .. NELBO: 1725.03\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.0 .. NELBO: 1722.0\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.04 .. NELBO: 1725.04\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.42 .. NELBO: 1713.42\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.79 .. NELBO: 1724.79\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.68 .. NELBO: 1721.68\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.14 .. NELBO: 1713.14\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.7 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.85 .. NELBO: 1712.85\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.34 .. NELBO: 1721.34\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.38 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.34 .. NELBO: 1721.34\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.38 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.37 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.8 .. NELBO: 1712.8\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.39 .. NELBO: 1724.39\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:40:22,109] Trial 5 finished with values: [-0.007972072099875022, 0.045454545454545456] and parameters: {'num_topics': 22, 'dropout': 0.4640083858922023, 't_hidden_size': 100, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.48463792990665217, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=18, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=18, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=18, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 1996.26 .. NELBO: 1996.91\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 1880.68 .. NELBO: 1881.25\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 1868.94 .. NELBO: 1869.49\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.12 .. NELBO: 1723.17\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1730.89 .. NELBO: 1730.93\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.19 .. NELBO: 1734.23\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1719.73 .. NELBO: 1719.76\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1727.96 .. NELBO: 1727.99\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1731.16 .. NELBO: 1731.19\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1718.56 .. NELBO: 1718.59\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1726.72 .. NELBO: 1726.75\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1729.87 .. NELBO: 1729.9\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1717.78 .. NELBO: 1717.8\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1725.94 .. NELBO: 1725.96\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1729.08 .. NELBO: 1729.1\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.12 .. NELBO: 1717.13\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.31 .. NELBO: 1725.32\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1728.46 .. NELBO: 1728.47\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.51 .. NELBO: 1716.51\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.91 .. NELBO: 1727.91\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.07 .. NELBO: 1716.07\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.35 .. NELBO: 1724.35\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.5 .. NELBO: 1727.5\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.72 .. NELBO: 1715.72\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.02 .. NELBO: 1724.03\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.18 .. NELBO: 1727.19\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1715.23 .. NELBO: 1715.25\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1723.66 .. NELBO: 1723.7\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1726.75 .. NELBO: 1726.79\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.18 .. Rec_loss: 1714.76 .. NELBO: 1714.94\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 1722.83 .. NELBO: 1723.09\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 1725.91 .. NELBO: 1726.21\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 1713.48 .. NELBO: 1714.13\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 1721.83 .. NELBO: 1722.48\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 1724.94 .. NELBO: 1725.59\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.87 .. Rec_loss: 1712.89 .. NELBO: 1713.76\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.82 .. Rec_loss: 1721.44 .. NELBO: 1722.26\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1724.52 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.88 .. Rec_loss: 1712.51 .. NELBO: 1713.39\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.97 .. Rec_loss: 1720.8 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.99 .. Rec_loss: 1723.75 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.21 .. Rec_loss: 1711.43 .. NELBO: 1712.64\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.27 .. Rec_loss: 1719.85 .. NELBO: 1721.12\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 1.35 .. Rec_loss: 1722.59 .. NELBO: 1723.94\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.74 .. Rec_loss: 1709.87 .. NELBO: 1711.61\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.52 .. Rec_loss: 1719.11 .. NELBO: 1720.63\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 1.66 .. Rec_loss: 1721.58 .. NELBO: 1723.24\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.98 .. Rec_loss: 1708.09 .. NELBO: 1710.07\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.69 .. Rec_loss: 1717.78 .. NELBO: 1719.47\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 1.92 .. Rec_loss: 1720.0 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.23 .. Rec_loss: 1706.83 .. NELBO: 1709.06\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.82 .. Rec_loss: 1716.82 .. NELBO: 1718.64\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 1.98 .. Rec_loss: 1718.96 .. NELBO: 1720.94\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.46 .. Rec_loss: 1706.0 .. NELBO: 1708.46\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.96 .. Rec_loss: 1716.06 .. NELBO: 1718.02\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.13 .. Rec_loss: 1718.17 .. NELBO: 1720.3\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1705.45 .. NELBO: 1708.09\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.11 .. Rec_loss: 1715.55 .. NELBO: 1717.66\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.28 .. Rec_loss: 1717.54 .. NELBO: 1719.82\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1704.48 .. NELBO: 1707.16\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.1 .. Rec_loss: 1714.9 .. NELBO: 1717.0\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 1716.77 .. NELBO: 1719.1\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1703.78 .. NELBO: 1706.34\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.09 .. Rec_loss: 1714.34 .. NELBO: 1716.43\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.3 .. Rec_loss: 1716.3 .. NELBO: 1718.6\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1703.1 .. NELBO: 1705.8\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.15 .. Rec_loss: 1713.73 .. NELBO: 1715.88\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1715.58 .. NELBO: 1717.98\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1702.39 .. NELBO: 1705.16\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1713.53 .. NELBO: 1715.7\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.38 .. Rec_loss: 1715.41 .. NELBO: 1717.79\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1702.05 .. NELBO: 1704.76\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.16 .. Rec_loss: 1713.13 .. NELBO: 1715.29\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.42 .. Rec_loss: 1714.99 .. NELBO: 1717.41\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1702.03 .. NELBO: 1704.82\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.22 .. Rec_loss: 1713.13 .. NELBO: 1715.35\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 1714.9 .. NELBO: 1717.33\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1702.23 .. NELBO: 1704.96\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1713.4 .. NELBO: 1715.64\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1715.06 .. NELBO: 1717.56\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1702.23 .. NELBO: 1704.84\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.18 .. Rec_loss: 1712.95 .. NELBO: 1715.13\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1714.73 .. NELBO: 1717.13\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1701.49 .. NELBO: 1704.16\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.25 .. Rec_loss: 1712.69 .. NELBO: 1714.94\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1714.38 .. NELBO: 1716.82\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1701.09 .. NELBO: 1703.72\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.22 .. Rec_loss: 1712.58 .. NELBO: 1714.8\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1714.32 .. NELBO: 1716.76\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1701.41 .. NELBO: 1703.94\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.16 .. Rec_loss: 1712.87 .. NELBO: 1715.03\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 2.39 .. Rec_loss: 1714.48 .. NELBO: 1716.87\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1701.66 .. NELBO: 1704.25\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.22 .. Rec_loss: 1713.08 .. NELBO: 1715.3\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1714.51 .. NELBO: 1717.01\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1700.95 .. NELBO: 1703.51\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1712.46 .. NELBO: 1714.63\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1713.84 .. NELBO: 1716.32\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1700.32 .. NELBO: 1703.0\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1712.18 .. NELBO: 1714.35\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1713.49 .. NELBO: 1715.97\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1701.11 .. NELBO: 1703.7\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1712.5 .. NELBO: 1714.71\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 2.45 .. Rec_loss: 1713.78 .. NELBO: 1716.23\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1700.19 .. NELBO: 1702.98\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.31 .. Rec_loss: 1711.98 .. NELBO: 1714.29\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1713.2 .. NELBO: 1715.78\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1700.16 .. NELBO: 1702.89\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.28 .. Rec_loss: 1712.07 .. NELBO: 1714.35\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1713.16 .. NELBO: 1715.7\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1700.6 .. NELBO: 1703.33\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1712.43 .. NELBO: 1714.67\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1713.41 .. NELBO: 1715.94\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1701.79 .. NELBO: 1704.4\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.2 .. Rec_loss: 1713.65 .. NELBO: 1715.85\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 2.42 .. Rec_loss: 1715.02 .. NELBO: 1717.44\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1703.77 .. NELBO: 1706.44\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.27 .. Rec_loss: 1714.65 .. NELBO: 1716.92\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1716.87 .. NELBO: 1719.41\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1703.24 .. NELBO: 1705.61\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.97 .. Rec_loss: 1714.44 .. NELBO: 1716.41\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 2.31 .. Rec_loss: 1715.69 .. NELBO: 1718.0\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1702.18 .. NELBO: 1705.02\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.26 .. Rec_loss: 1713.13 .. NELBO: 1715.39\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1714.65 .. NELBO: 1717.19\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1702.2 .. NELBO: 1704.94\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.31 .. Rec_loss: 1713.17 .. NELBO: 1715.48\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1714.61 .. NELBO: 1717.25\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1701.99 .. NELBO: 1704.6\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.32 .. Rec_loss: 1713.02 .. NELBO: 1715.34\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1714.62 .. NELBO: 1717.25\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 1701.52 .. NELBO: 1703.95\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1712.82 .. NELBO: 1715.06\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1714.66 .. NELBO: 1717.17\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1700.11 .. NELBO: 1703.12\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.41 .. Rec_loss: 1711.86 .. NELBO: 1714.27\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1713.42 .. NELBO: 1716.16\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1700.5 .. NELBO: 1703.1\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 1711.89 .. NELBO: 1714.23\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1713.59 .. NELBO: 1716.24\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.35 .. Rec_loss: 1701.18 .. NELBO: 1703.53\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1712.41 .. NELBO: 1714.65\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 2.57 .. Rec_loss: 1713.87 .. NELBO: 1716.44\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 1700.39 .. NELBO: 1702.86\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.38 .. Rec_loss: 1712.07 .. NELBO: 1714.45\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1713.57 .. NELBO: 1716.23\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.31 .. Rec_loss: 1700.81 .. NELBO: 1703.12\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.28 .. Rec_loss: 1711.94 .. NELBO: 1714.22\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1713.25 .. NELBO: 1715.89\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:41:35,803] Trial 6 finished with values: [-0.006370235112611755, 0.17222222222222222] and parameters: {'num_topics': 18, 'dropout': 0.48463792990665217, 't_hidden_size': 200, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.3755583179318846, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=22, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=22, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=22, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.75 .. Rec_loss: 2009.72 .. NELBO: 2011.47\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.91 .. Rec_loss: 1888.86 .. NELBO: 1889.77\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1876.4 .. NELBO: 1877.21\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.43 .. NELBO: 1723.44\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1730.88 .. NELBO: 1730.89\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1734.19 .. NELBO: 1734.19\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.74 .. NELBO: 1719.74\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.88 .. NELBO: 1727.88\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.08 .. NELBO: 1731.08\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.21 .. NELBO: 1718.21\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.47 .. NELBO: 1726.47\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.63 .. NELBO: 1729.63\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.55 .. NELBO: 1717.55\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.75 .. NELBO: 1725.75\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.89 .. NELBO: 1728.89\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.96 .. NELBO: 1716.96\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.31 .. NELBO: 1728.31\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.45 .. NELBO: 1716.45\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.84 .. NELBO: 1727.84\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.99 .. NELBO: 1715.99\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.51 .. NELBO: 1727.51\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.68 .. NELBO: 1715.68\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.05 .. NELBO: 1724.05\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.2 .. NELBO: 1727.2\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.4 .. NELBO: 1715.4\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.77 .. NELBO: 1723.77\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.89 .. NELBO: 1726.89\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.3 .. NELBO: 1715.3\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.58 .. NELBO: 1723.58\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.74 .. NELBO: 1726.74\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.96 .. NELBO: 1714.96\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.42 .. NELBO: 1723.42\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.53 .. NELBO: 1726.53\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.74 .. NELBO: 1714.74\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.14 .. NELBO: 1723.14\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.23 .. NELBO: 1726.23\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.79 .. NELBO: 1714.79\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.03 .. NELBO: 1723.03\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.17 .. NELBO: 1726.17\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.48 .. NELBO: 1714.48\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.96 .. NELBO: 1722.96\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.06 .. NELBO: 1726.06\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.33 .. NELBO: 1714.33\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.81 .. NELBO: 1722.81\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.87 .. NELBO: 1725.87\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.4 .. NELBO: 1714.4\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.7 .. NELBO: 1722.7\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.79 .. NELBO: 1725.79\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.13 .. NELBO: 1714.13\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.57 .. NELBO: 1722.57\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.63 .. NELBO: 1725.63\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.02 .. NELBO: 1714.02\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.43 .. NELBO: 1722.43\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.5 .. NELBO: 1725.5\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.05 .. NELBO: 1714.05\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.38 .. NELBO: 1722.38\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.49 .. NELBO: 1725.49\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.89 .. NELBO: 1713.89\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.44 .. NELBO: 1722.44\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.48 .. NELBO: 1725.48\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.97 .. NELBO: 1713.97\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.37 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.43 .. NELBO: 1725.43\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.96 .. NELBO: 1713.96\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.34 .. NELBO: 1722.34\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.41 .. NELBO: 1725.41\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.84 .. NELBO: 1713.84\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.29 .. NELBO: 1722.29\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.34 .. NELBO: 1725.34\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.8 .. NELBO: 1713.8\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.23 .. NELBO: 1722.23\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.29 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.66 .. NELBO: 1713.66\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.13 .. NELBO: 1725.13\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.51 .. NELBO: 1713.51\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.99 .. NELBO: 1724.99\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.96 .. NELBO: 1724.96\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.35 .. NELBO: 1713.35\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.82 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.37 .. NELBO: 1713.37\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.68 .. NELBO: 1721.68\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.8 .. NELBO: 1712.8\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:42:48,239] Trial 7 finished with values: [-0.00827759561610265, 0.05] and parameters: {'num_topics': 22, 'dropout': 0.3755583179318846, 't_hidden_size': 300, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.4173045599935797, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=47, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=47, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=47, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.33 .. Rec_loss: 2004.64 .. NELBO: 2005.97\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.78 .. Rec_loss: 1886.01 .. NELBO: 1886.79\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.7 .. Rec_loss: 1873.82 .. NELBO: 1874.52\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1723.45 .. NELBO: 1723.48\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.01 .. NELBO: 1731.03\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1734.31 .. NELBO: 1734.33\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1719.66 .. NELBO: 1719.67\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.85 .. NELBO: 1727.85\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.05 .. NELBO: 1731.05\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.31 .. NELBO: 1718.31\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.53 .. NELBO: 1726.53\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.69 .. NELBO: 1729.69\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.6 .. NELBO: 1717.6\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.8 .. NELBO: 1725.8\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.93 .. NELBO: 1728.93\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.01 .. NELBO: 1717.01\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.21 .. NELBO: 1725.21\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.35 .. NELBO: 1728.35\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.52 .. NELBO: 1716.52\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.9 .. NELBO: 1727.9\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.05 .. NELBO: 1716.05\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.49 .. NELBO: 1727.49\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.65 .. NELBO: 1715.65\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.97 .. NELBO: 1723.97\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.12 .. NELBO: 1727.12\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.36 .. NELBO: 1715.36\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.69 .. NELBO: 1723.69\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.83 .. NELBO: 1726.83\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.09 .. NELBO: 1715.09\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.45 .. NELBO: 1723.45\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.58 .. NELBO: 1726.58\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.89 .. NELBO: 1714.89\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.28 .. NELBO: 1723.28\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.39 .. NELBO: 1726.39\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.78 .. NELBO: 1714.78\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.13 .. NELBO: 1723.13\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.25 .. NELBO: 1726.25\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.62 .. NELBO: 1714.62\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.0 .. NELBO: 1723.0\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.1 .. NELBO: 1726.1\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.51 .. NELBO: 1714.51\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.89 .. NELBO: 1722.89\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.97 .. NELBO: 1725.97\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.45 .. NELBO: 1714.45\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.79 .. NELBO: 1722.79\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.88 .. NELBO: 1725.88\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.29 .. NELBO: 1714.29\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.67 .. NELBO: 1722.67\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.76 .. NELBO: 1725.76\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.18 .. NELBO: 1714.18\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.63 .. NELBO: 1725.63\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.14 .. NELBO: 1714.14\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.52 .. NELBO: 1722.52\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.6 .. NELBO: 1725.6\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.99 .. NELBO: 1713.99\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.4 .. NELBO: 1722.4\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.47 .. NELBO: 1725.47\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.95 .. NELBO: 1713.95\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.33 .. NELBO: 1722.33\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.4 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.86 .. NELBO: 1713.86\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.28 .. NELBO: 1722.28\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.33 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.82 .. NELBO: 1713.82\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.19 .. NELBO: 1722.19\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.77 .. NELBO: 1713.77\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.15 .. NELBO: 1722.15\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.08 .. NELBO: 1722.08\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.13 .. NELBO: 1725.13\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.56 .. NELBO: 1713.56\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.98 .. NELBO: 1721.98\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.04 .. NELBO: 1725.04\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.97 .. NELBO: 1721.97\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.02 .. NELBO: 1725.02\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.48 .. NELBO: 1713.48\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.96 .. NELBO: 1724.96\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.96 .. NELBO: 1724.96\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.57 .. NELBO: 1713.57\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.82 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.89 .. NELBO: 1724.89\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.91 .. NELBO: 1712.91\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:44:01,900] Trial 8 finished with values: [-0.008356155852233012, 0.023404255319148935] and parameters: {'num_topics': 47, 'dropout': 0.4173045599935797, 't_hidden_size': 100, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.18875913947455583, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=45, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=45, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=45, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 2010.73 .. NELBO: 2011.04\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 1888.94 .. NELBO: 1889.31\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 1876.33 .. NELBO: 1876.71\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1723.53 .. NELBO: 1723.62\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1731.05 .. NELBO: 1731.11\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1734.32 .. NELBO: 1734.38\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.82 .. NELBO: 1719.84\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.96 .. NELBO: 1727.97\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.15 .. NELBO: 1731.16\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.37 .. NELBO: 1718.38\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.58 .. NELBO: 1726.58\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.73 .. NELBO: 1729.73\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.59 .. NELBO: 1717.59\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.78 .. NELBO: 1725.79\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1728.92 .. NELBO: 1728.93\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1716.9 .. NELBO: 1716.95\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1725.04 .. NELBO: 1725.1\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1728.17 .. NELBO: 1728.24\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 1715.78 .. NELBO: 1716.15\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.38 .. Rec_loss: 1723.92 .. NELBO: 1724.3\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 1727.03 .. NELBO: 1727.43\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 1714.7 .. NELBO: 1715.39\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 1723.05 .. NELBO: 1723.68\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 1726.16 .. NELBO: 1726.81\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.76 .. Rec_loss: 1714.14 .. NELBO: 1714.9\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1722.72 .. NELBO: 1723.36\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1725.82 .. NELBO: 1726.46\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.79 .. Rec_loss: 1714.16 .. NELBO: 1714.95\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 1722.66 .. NELBO: 1723.28\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 1725.74 .. NELBO: 1726.37\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.85 .. Rec_loss: 1713.8 .. NELBO: 1714.65\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 1722.22 .. NELBO: 1722.99\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 1725.3 .. NELBO: 1726.07\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.08 .. Rec_loss: 1713.02 .. NELBO: 1714.1\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.94 .. Rec_loss: 1721.71 .. NELBO: 1722.65\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.97 .. Rec_loss: 1724.7 .. NELBO: 1725.67\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.28 .. Rec_loss: 1712.04 .. NELBO: 1713.32\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.24 .. Rec_loss: 1720.59 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 1.3 .. Rec_loss: 1723.52 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.8 .. Rec_loss: 1710.41 .. NELBO: 1712.21\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.64 .. Rec_loss: 1719.18 .. NELBO: 1720.82\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 1.89 .. Rec_loss: 1721.68 .. NELBO: 1723.57\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.22 .. Rec_loss: 1708.73 .. NELBO: 1710.95\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.79 .. Rec_loss: 1717.92 .. NELBO: 1719.71\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.19 .. Rec_loss: 1720.09 .. NELBO: 1722.28\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1706.68 .. NELBO: 1709.37\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.14 .. Rec_loss: 1716.49 .. NELBO: 1718.63\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1718.35 .. NELBO: 1720.86\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.03 .. Rec_loss: 1705.32 .. NELBO: 1708.35\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1715.54 .. NELBO: 1718.12\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1717.58 .. NELBO: 1720.3\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.7 .. Rec_loss: 1705.14 .. NELBO: 1708.84\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1715.56 .. NELBO: 1718.48\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1718.03 .. NELBO: 1721.01\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.35 .. Rec_loss: 1705.33 .. NELBO: 1708.68\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1715.65 .. NELBO: 1718.21\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1717.29 .. NELBO: 1720.14\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.65 .. Rec_loss: 1703.2 .. NELBO: 1706.85\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1714.01 .. NELBO: 1717.0\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1716.23 .. NELBO: 1719.38\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 1702.96 .. NELBO: 1706.35\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1713.83 .. NELBO: 1716.62\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1715.52 .. NELBO: 1718.52\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1701.28 .. NELBO: 1704.7\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.03 .. Rec_loss: 1712.47 .. NELBO: 1715.5\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1714.72 .. NELBO: 1717.93\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1702.58 .. NELBO: 1705.79\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1713.03 .. NELBO: 1715.89\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 3.17 .. Rec_loss: 1714.72 .. NELBO: 1717.89\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.46 .. Rec_loss: 1700.91 .. NELBO: 1704.37\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.25 .. Rec_loss: 1711.91 .. NELBO: 1715.16\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 1713.72 .. NELBO: 1717.11\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.46 .. Rec_loss: 1700.01 .. NELBO: 1703.47\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1711.41 .. NELBO: 1714.69\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 3.47 .. Rec_loss: 1713.23 .. NELBO: 1716.7\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.34 .. Rec_loss: 1699.44 .. NELBO: 1702.78\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.18 .. Rec_loss: 1711.08 .. NELBO: 1714.26\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 3.44 .. Rec_loss: 1712.71 .. NELBO: 1716.15\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.17 .. Rec_loss: 1700.05 .. NELBO: 1703.22\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.14 .. Rec_loss: 1711.44 .. NELBO: 1714.58\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 3.45 .. Rec_loss: 1713.07 .. NELBO: 1716.52\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1699.33 .. NELBO: 1702.23\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1710.78 .. NELBO: 1713.74\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 3.35 .. Rec_loss: 1712.18 .. NELBO: 1715.53\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1698.86 .. NELBO: 1701.88\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.03 .. Rec_loss: 1710.18 .. NELBO: 1713.21\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.45 .. Rec_loss: 1711.96 .. NELBO: 1715.41\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1699.29 .. NELBO: 1702.11\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1710.34 .. NELBO: 1713.2\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3.3 .. Rec_loss: 1711.62 .. NELBO: 1714.92\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 1697.49 .. NELBO: 1700.82\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.18 .. Rec_loss: 1709.05 .. NELBO: 1712.23\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.58 .. Rec_loss: 1710.44 .. NELBO: 1714.02\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1697.33 .. NELBO: 1700.48\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1709.11 .. NELBO: 1712.16\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.47 .. Rec_loss: 1711.03 .. NELBO: 1714.5\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1698.99 .. NELBO: 1701.98\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1710.39 .. NELBO: 1713.22\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1711.69 .. NELBO: 1714.9\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.63 .. Rec_loss: 1696.56 .. NELBO: 1700.19\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.2 .. Rec_loss: 1708.56 .. NELBO: 1711.76\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.58 .. Rec_loss: 1709.76 .. NELBO: 1713.34\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.7 .. Rec_loss: 1695.47 .. NELBO: 1699.17\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 1707.71 .. NELBO: 1711.1\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.79 .. Rec_loss: 1708.93 .. NELBO: 1712.72\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.64 .. Rec_loss: 1695.85 .. NELBO: 1699.49\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 1708.08 .. NELBO: 1711.41\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.72 .. Rec_loss: 1709.83 .. NELBO: 1713.55\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1696.54 .. NELBO: 1699.96\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1708.42 .. NELBO: 1711.65\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.65 .. Rec_loss: 1710.0 .. NELBO: 1713.65\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.65 .. Rec_loss: 1695.69 .. NELBO: 1699.34\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.3 .. Rec_loss: 1707.86 .. NELBO: 1711.16\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.73 .. Rec_loss: 1709.16 .. NELBO: 1712.89\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.89 .. Rec_loss: 1694.98 .. NELBO: 1698.87\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1707.28 .. NELBO: 1710.7\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 3.82 .. Rec_loss: 1708.71 .. NELBO: 1712.53\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.69 .. Rec_loss: 1694.87 .. NELBO: 1698.56\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 1707.25 .. NELBO: 1710.58\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 3.77 .. Rec_loss: 1708.6 .. NELBO: 1712.37\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.93 .. Rec_loss: 1694.43 .. NELBO: 1698.36\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.44 .. Rec_loss: 1706.94 .. NELBO: 1710.38\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 3.82 .. Rec_loss: 1708.35 .. NELBO: 1712.17\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.06 .. Rec_loss: 1694.15 .. NELBO: 1698.21\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.55 .. Rec_loss: 1706.81 .. NELBO: 1710.36\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 3.98 .. Rec_loss: 1708.13 .. NELBO: 1712.11\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.08 .. Rec_loss: 1694.13 .. NELBO: 1698.21\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.53 .. Rec_loss: 1706.71 .. NELBO: 1710.24\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 3.92 .. Rec_loss: 1708.05 .. NELBO: 1711.97\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.1 .. Rec_loss: 1693.75 .. NELBO: 1697.85\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1706.33 .. NELBO: 1709.89\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 3.96 .. Rec_loss: 1707.61 .. NELBO: 1711.57\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.25 .. Rec_loss: 1693.45 .. NELBO: 1697.7\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.63 .. Rec_loss: 1706.23 .. NELBO: 1709.86\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.1 .. Rec_loss: 1707.34 .. NELBO: 1711.44\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.08 .. Rec_loss: 1694.21 .. NELBO: 1698.29\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.54 .. Rec_loss: 1706.52 .. NELBO: 1710.06\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 4.06 .. Rec_loss: 1707.65 .. NELBO: 1711.71\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.77 .. Rec_loss: 1694.3 .. NELBO: 1698.07\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.46 .. Rec_loss: 1706.45 .. NELBO: 1709.91\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 3.87 .. Rec_loss: 1707.57 .. NELBO: 1711.44\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.08 .. Rec_loss: 1693.59 .. NELBO: 1697.67\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.68 .. Rec_loss: 1705.97 .. NELBO: 1709.65\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 4.02 .. Rec_loss: 1707.29 .. NELBO: 1711.31\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.29 .. Rec_loss: 1692.52 .. NELBO: 1696.81\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.88 .. Rec_loss: 1705.21 .. NELBO: 1709.09\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 4.27 .. Rec_loss: 1706.35 .. NELBO: 1710.62\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.29 .. Rec_loss: 1692.71 .. NELBO: 1697.0\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.78 .. Rec_loss: 1705.47 .. NELBO: 1709.25\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 4.3 .. Rec_loss: 1706.42 .. NELBO: 1710.72\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:45:19,162] Trial 9 finished with values: [0.002552780807302627, 0.12444444444444444] and parameters: {'num_topics': 45, 'dropout': 0.18875913947455583, 't_hidden_size': 100, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.24738805161804528, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=20, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=20, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.0 .. Rec_loss: 2001.65 .. NELBO: 2002.65\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 1883.82 .. NELBO: 1884.41\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 1871.75 .. NELBO: 1872.32\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1723.35 .. NELBO: 1723.43\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1730.91 .. NELBO: 1730.97\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.23 .. NELBO: 1734.28\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1719.72 .. NELBO: 1719.76\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1727.9 .. NELBO: 1727.94\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1731.09 .. NELBO: 1731.12\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1718.53 .. NELBO: 1718.56\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1726.7 .. NELBO: 1726.73\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1729.84 .. NELBO: 1729.86\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.71 .. NELBO: 1717.72\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.89 .. NELBO: 1725.9\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.03 .. NELBO: 1729.04\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.09 .. NELBO: 1717.09\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.27 .. NELBO: 1725.27\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.42 .. NELBO: 1728.42\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.53 .. NELBO: 1716.53\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.92 .. NELBO: 1727.92\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.06 .. NELBO: 1716.06\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.49 .. NELBO: 1727.49\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.74 .. NELBO: 1715.74\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.04 .. NELBO: 1724.04\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.19 .. NELBO: 1727.19\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.42 .. NELBO: 1715.42\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.78 .. NELBO: 1723.78\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.92 .. NELBO: 1726.92\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.14 .. NELBO: 1715.14\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.5 .. NELBO: 1723.5\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.65 .. NELBO: 1726.65\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.87 .. NELBO: 1714.87\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.33 .. NELBO: 1723.33\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.44 .. NELBO: 1726.44\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.85 .. NELBO: 1714.85\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.16 .. NELBO: 1723.16\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.27 .. NELBO: 1726.27\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.68 .. NELBO: 1714.68\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.03 .. NELBO: 1723.03\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.15 .. NELBO: 1726.15\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.47 .. NELBO: 1714.47\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.89 .. NELBO: 1722.89\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.97 .. NELBO: 1725.97\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.47 .. NELBO: 1714.47\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.8 .. NELBO: 1722.8\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.92 .. NELBO: 1725.92\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.3 .. NELBO: 1714.3\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.73 .. NELBO: 1722.73\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.8 .. NELBO: 1725.8\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.23 .. NELBO: 1714.23\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.6 .. NELBO: 1722.6\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.68 .. NELBO: 1725.68\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.08 .. NELBO: 1714.08\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.5 .. NELBO: 1722.5\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.57 .. NELBO: 1725.57\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.96 .. NELBO: 1713.96\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.37 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.42 .. NELBO: 1725.42\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.92 .. NELBO: 1713.92\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.3 .. NELBO: 1722.3\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.36 .. NELBO: 1725.36\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.88 .. NELBO: 1713.88\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.29 .. NELBO: 1722.29\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.37 .. NELBO: 1725.37\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.75 .. NELBO: 1713.75\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.23 .. NELBO: 1722.23\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.27 .. NELBO: 1725.27\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.77 .. NELBO: 1713.77\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.18 .. NELBO: 1722.18\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.83 .. NELBO: 1713.83\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.17 .. NELBO: 1722.17\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.64 .. NELBO: 1713.64\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.15 .. NELBO: 1725.15\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.58 .. NELBO: 1713.58\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.99 .. NELBO: 1721.99\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.03 .. NELBO: 1725.03\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.6 .. NELBO: 1713.6\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.98 .. NELBO: 1721.98\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.42 .. NELBO: 1713.42\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.32 .. NELBO: 1713.32\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.33 .. NELBO: 1713.33\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.75 .. NELBO: 1721.75\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.77 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.85 .. NELBO: 1712.85\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.35 .. NELBO: 1721.35\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.34 .. NELBO: 1721.34\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.34 .. NELBO: 1721.34\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.85 .. NELBO: 1712.85\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.39 .. NELBO: 1724.39\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:46:29,581] Trial 10 finished with values: [-0.007835759647631554, 0.055] and parameters: {'num_topics': 20, 'dropout': 0.24738805161804528, 't_hidden_size': 100, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.35252520908516577, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=12, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=12, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=12, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.29 .. Rec_loss: 1995.2 .. NELBO: 1996.49\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.73 .. Rec_loss: 1879.76 .. NELBO: 1880.49\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.67 .. Rec_loss: 1868.09 .. NELBO: 1868.76\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.37 .. NELBO: 1723.42\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.92 .. NELBO: 1730.97\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.21 .. NELBO: 1734.26\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1719.95 .. NELBO: 1720.03\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1728.08 .. NELBO: 1728.16\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1731.26 .. NELBO: 1731.34\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1718.67 .. NELBO: 1718.74\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1726.79 .. NELBO: 1726.85\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1729.96 .. NELBO: 1730.02\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1717.92 .. NELBO: 1717.96\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1726.04 .. NELBO: 1726.08\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1729.19 .. NELBO: 1729.22\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1717.24 .. NELBO: 1717.26\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1725.39 .. NELBO: 1725.41\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1728.54 .. NELBO: 1728.56\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1716.65 .. NELBO: 1716.66\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.86 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1728.02 .. NELBO: 1728.03\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.11 .. NELBO: 1716.11\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.56 .. NELBO: 1727.56\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.72 .. NELBO: 1715.72\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.05 .. NELBO: 1724.05\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.19 .. NELBO: 1727.19\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.49 .. NELBO: 1715.49\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.79 .. NELBO: 1723.79\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.93 .. NELBO: 1726.93\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.18 .. NELBO: 1715.18\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.53 .. NELBO: 1723.53\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.66 .. NELBO: 1726.66\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.94 .. NELBO: 1714.94\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.34 .. NELBO: 1723.34\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.47 .. NELBO: 1726.47\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.78 .. NELBO: 1714.78\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.18 .. NELBO: 1723.18\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.27 .. NELBO: 1726.27\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.81 .. NELBO: 1714.81\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.08 .. NELBO: 1723.08\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.21 .. NELBO: 1726.21\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.6 .. NELBO: 1714.6\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.03 .. NELBO: 1723.03\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.11 .. NELBO: 1726.11\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.78 .. NELBO: 1722.78\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.87 .. NELBO: 1725.87\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.3 .. NELBO: 1714.3\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.67 .. NELBO: 1722.67\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.76 .. NELBO: 1725.76\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.13 .. NELBO: 1714.13\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.57 .. NELBO: 1722.57\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.65 .. NELBO: 1725.65\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.15 .. NELBO: 1714.15\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.62 .. NELBO: 1725.62\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.17 .. NELBO: 1714.17\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.49 .. NELBO: 1722.49\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.58 .. NELBO: 1725.58\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.04 .. NELBO: 1714.04\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.5 .. NELBO: 1722.5\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.55 .. NELBO: 1725.55\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.94 .. NELBO: 1713.94\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.34 .. NELBO: 1722.34\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.38 .. NELBO: 1725.38\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.83 .. NELBO: 1713.83\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.2 .. NELBO: 1722.2\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.69 .. NELBO: 1713.69\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.62 .. NELBO: 1713.62\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.05 .. NELBO: 1722.05\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.09 .. NELBO: 1725.09\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.07 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.57 .. NELBO: 1713.57\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.0 .. NELBO: 1722.0\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.04 .. NELBO: 1725.04\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.55 .. NELBO: 1713.55\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.99 .. NELBO: 1724.99\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.92 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.99 .. NELBO: 1724.99\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.92 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.96 .. NELBO: 1724.96\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.51 .. NELBO: 1713.51\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.38 .. NELBO: 1713.38\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.32 .. NELBO: 1713.32\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.27 .. NELBO: 1713.27\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:47:39,667] Trial 11 finished with values: [-0.007972072099875023, 0.08333333333333333] and parameters: {'num_topics': 12, 'dropout': 0.35252520908516577, 't_hidden_size': 100, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.28698367901587823, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=31, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=31, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=31, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.94 .. Rec_loss: 2011.46 .. NELBO: 2013.4\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.01 .. Rec_loss: 1889.87 .. NELBO: 1890.88\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.9 .. Rec_loss: 1877.31 .. NELBO: 1878.21\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.56 .. NELBO: 1723.57\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.0 .. NELBO: 1731.0\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1734.31 .. NELBO: 1734.31\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.63 .. NELBO: 1719.63\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.79 .. NELBO: 1727.79\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1730.99 .. NELBO: 1730.99\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.28 .. NELBO: 1718.28\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.49 .. NELBO: 1726.49\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.65 .. NELBO: 1729.65\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.53 .. NELBO: 1717.53\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.74 .. NELBO: 1725.74\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.89 .. NELBO: 1728.89\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.95 .. NELBO: 1716.95\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.31 .. NELBO: 1728.31\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.44 .. NELBO: 1716.44\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.84 .. NELBO: 1727.84\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.99 .. NELBO: 1715.99\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.28 .. NELBO: 1724.28\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.43 .. NELBO: 1727.43\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.64 .. NELBO: 1715.64\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.97 .. NELBO: 1723.97\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.1 .. NELBO: 1727.1\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.48 .. NELBO: 1715.48\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.8 .. NELBO: 1723.8\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.94 .. NELBO: 1726.94\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.2 .. NELBO: 1715.2\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.53 .. NELBO: 1723.53\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.67 .. NELBO: 1726.67\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.87 .. NELBO: 1714.87\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.32 .. NELBO: 1723.32\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.42 .. NELBO: 1726.42\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.75 .. NELBO: 1714.75\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.11 .. NELBO: 1723.11\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.23 .. NELBO: 1726.23\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.58 .. NELBO: 1714.58\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.94 .. NELBO: 1722.94\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.05 .. NELBO: 1726.05\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.41 .. NELBO: 1714.41\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.86 .. NELBO: 1722.86\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.93 .. NELBO: 1725.93\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.78 .. NELBO: 1722.78\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.87 .. NELBO: 1725.87\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.37 .. NELBO: 1714.37\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.7 .. NELBO: 1722.7\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.81 .. NELBO: 1725.81\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.14 .. NELBO: 1714.14\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.58 .. NELBO: 1722.58\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.64 .. NELBO: 1725.64\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.07 .. NELBO: 1714.07\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.45 .. NELBO: 1722.45\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.52 .. NELBO: 1725.52\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.01 .. NELBO: 1714.01\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.37 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.46 .. NELBO: 1725.46\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.86 .. NELBO: 1713.86\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.28 .. NELBO: 1722.28\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.35 .. NELBO: 1725.35\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.78 .. NELBO: 1713.78\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.22 .. NELBO: 1722.22\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.89 .. NELBO: 1713.89\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.25 .. NELBO: 1722.25\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.32 .. NELBO: 1725.32\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.76 .. NELBO: 1713.76\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.22 .. NELBO: 1722.22\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.27 .. NELBO: 1725.27\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.68 .. NELBO: 1713.68\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.14 .. NELBO: 1722.14\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.83 .. NELBO: 1713.83\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.12 .. NELBO: 1722.12\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.58 .. NELBO: 1713.58\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.02 .. NELBO: 1722.02\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.08 .. NELBO: 1725.08\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.95 .. NELBO: 1721.95\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.99 .. NELBO: 1724.99\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.97 .. NELBO: 1721.97\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.01 .. NELBO: 1725.01\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.5 .. NELBO: 1713.5\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.89 .. NELBO: 1724.89\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.39 .. NELBO: 1713.39\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.75 .. NELBO: 1721.75\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.7 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.68 .. NELBO: 1721.68\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.91 .. NELBO: 1712.91\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.14 .. NELBO: 1713.14\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.82 .. NELBO: 1712.82\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1713.08 .. NELBO: 1713.09\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:49:02,168] Trial 12 finished with values: [-0.008319798632324162, 0.035483870967741936] and parameters: {'num_topics': 31, 'dropout': 0.28698367901587823, 't_hidden_size': 300, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.3296475363484476, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=18, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=18, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=18, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.22 .. Rec_loss: 1999.95 .. NELBO: 2003.17\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.08 .. Rec_loss: 1880.82 .. NELBO: 1882.9\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 1.86 .. Rec_loss: 1868.95 .. NELBO: 1870.81\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1723.44 .. NELBO: 1723.5\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.71 .. NELBO: 1730.76\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1733.98 .. NELBO: 1734.03\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.85 .. NELBO: 1719.87\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1727.94 .. NELBO: 1727.97\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1731.15 .. NELBO: 1731.18\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1718.36 .. NELBO: 1718.4\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1726.59 .. NELBO: 1726.63\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1729.75 .. NELBO: 1729.79\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1717.73 .. NELBO: 1717.77\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1725.91 .. NELBO: 1725.95\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1729.04 .. NELBO: 1729.08\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1717.26 .. NELBO: 1717.31\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1725.39 .. NELBO: 1725.44\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1728.56 .. NELBO: 1728.61\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1716.62 .. NELBO: 1716.67\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1724.92 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1728.06 .. NELBO: 1728.1\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1716.22 .. NELBO: 1716.26\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1724.54 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1727.64 .. NELBO: 1727.68\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1716.17 .. NELBO: 1716.2\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1724.32 .. NELBO: 1724.35\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1727.48 .. NELBO: 1727.51\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1715.58 .. NELBO: 1715.6\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1723.95 .. NELBO: 1723.97\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.08 .. NELBO: 1727.1\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1715.29 .. NELBO: 1715.3\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.66 .. NELBO: 1723.67\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.78 .. NELBO: 1726.79\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1715.06 .. NELBO: 1715.07\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.45 .. NELBO: 1723.46\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.57 .. NELBO: 1726.58\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.88 .. NELBO: 1714.88\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.23 .. NELBO: 1723.23\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.34 .. NELBO: 1726.34\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.74 .. NELBO: 1714.74\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.06 .. NELBO: 1723.06\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.18 .. NELBO: 1726.18\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.44 .. NELBO: 1714.44\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.88 .. NELBO: 1722.88\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.97 .. NELBO: 1725.97\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.4 .. NELBO: 1714.4\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.74 .. NELBO: 1722.74\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.84 .. NELBO: 1725.84\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.27 .. NELBO: 1714.27\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.63 .. NELBO: 1722.63\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.73 .. NELBO: 1725.73\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.11 .. NELBO: 1714.11\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.54 .. NELBO: 1722.54\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.61 .. NELBO: 1725.61\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.09 .. NELBO: 1714.09\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.44 .. NELBO: 1722.44\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.54 .. NELBO: 1725.54\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.0 .. NELBO: 1714.0\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.45 .. NELBO: 1722.45\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.52 .. NELBO: 1725.52\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.93 .. NELBO: 1713.93\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.35 .. NELBO: 1722.35\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.41 .. NELBO: 1725.41\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.05 .. NELBO: 1714.05\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.35 .. NELBO: 1722.35\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.44 .. NELBO: 1725.44\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.8 .. NELBO: 1713.8\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.26 .. NELBO: 1722.26\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.3 .. NELBO: 1725.3\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.8 .. NELBO: 1713.8\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.18 .. NELBO: 1722.18\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.73 .. NELBO: 1713.73\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.18 .. NELBO: 1722.18\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.64 .. NELBO: 1713.64\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.16 .. NELBO: 1722.16\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.75 .. NELBO: 1713.75\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.14 .. NELBO: 1722.14\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.76 .. NELBO: 1713.76\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.1 .. NELBO: 1722.1\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.15 .. NELBO: 1725.15\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.57 .. NELBO: 1713.57\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.95 .. NELBO: 1721.95\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.01 .. NELBO: 1725.01\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.89 .. NELBO: 1724.89\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.37 .. NELBO: 1713.37\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.39 .. NELBO: 1713.39\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.32 .. NELBO: 1713.32\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.7 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.0 .. NELBO: 1713.0\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.0 .. NELBO: 1713.0\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:50:13,065] Trial 13 finished with values: [-0.007972072099875022, 0.05555555555555555] and parameters: {'num_topics': 18, 'dropout': 0.3296475363484476, 't_hidden_size': 100, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.14595764585550755, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=43, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=43, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=43, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 2005.7 .. NELBO: 2006.0\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 1886.3 .. NELBO: 1886.57\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 1874.02 .. NELBO: 1874.28\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1723.26 .. NELBO: 1723.33\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1730.87 .. NELBO: 1730.93\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1734.12 .. NELBO: 1734.19\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1719.37 .. NELBO: 1719.46\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.15 .. Rec_loss: 1727.46 .. NELBO: 1727.61\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 1730.59 .. NELBO: 1730.79\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 1717.46 .. NELBO: 1717.83\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.43 .. Rec_loss: 1725.56 .. NELBO: 1725.99\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 1728.64 .. NELBO: 1729.1\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.71 .. Rec_loss: 1716.1 .. NELBO: 1716.81\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.68 .. Rec_loss: 1724.33 .. NELBO: 1725.01\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.71 .. Rec_loss: 1727.4 .. NELBO: 1728.11\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.86 .. Rec_loss: 1715.24 .. NELBO: 1716.1\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.75 .. Rec_loss: 1723.63 .. NELBO: 1724.38\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 1726.71 .. NELBO: 1727.48\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.84 .. Rec_loss: 1714.73 .. NELBO: 1715.57\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.78 .. Rec_loss: 1723.1 .. NELBO: 1723.88\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.83 .. Rec_loss: 1726.13 .. NELBO: 1726.96\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.05 .. Rec_loss: 1713.92 .. NELBO: 1714.97\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.9 .. Rec_loss: 1722.65 .. NELBO: 1723.55\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.89 .. Rec_loss: 1725.71 .. NELBO: 1726.6\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.1 .. Rec_loss: 1713.89 .. NELBO: 1714.99\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.11 .. Rec_loss: 1722.1 .. NELBO: 1723.21\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 1.1 .. Rec_loss: 1725.09 .. NELBO: 1726.19\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.74 .. Rec_loss: 1712.69 .. NELBO: 1714.43\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.3 .. Rec_loss: 1721.68 .. NELBO: 1722.98\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 1.46 .. Rec_loss: 1724.32 .. NELBO: 1725.78\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.41 .. Rec_loss: 1710.28 .. NELBO: 1712.69\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.67 .. Rec_loss: 1719.84 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 2.01 .. Rec_loss: 1722.17 .. NELBO: 1724.18\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1707.93 .. NELBO: 1711.03\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.12 .. Rec_loss: 1718.18 .. NELBO: 1720.3\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 2.22 .. Rec_loss: 1720.56 .. NELBO: 1722.78\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 1706.96 .. NELBO: 1710.29\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1716.75 .. NELBO: 1719.36\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1718.78 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 1705.13 .. NELBO: 1708.52\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1715.28 .. NELBO: 1718.02\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1717.17 .. NELBO: 1720.15\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.49 .. Rec_loss: 1703.98 .. NELBO: 1707.47\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1714.29 .. NELBO: 1717.26\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 3.22 .. Rec_loss: 1716.08 .. NELBO: 1719.3\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1703.37 .. NELBO: 1706.93\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.08 .. Rec_loss: 1713.6 .. NELBO: 1716.68\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 3.26 .. Rec_loss: 1715.28 .. NELBO: 1718.54\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.53 .. Rec_loss: 1702.54 .. NELBO: 1706.07\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 1713.08 .. NELBO: 1716.14\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 3.22 .. Rec_loss: 1714.83 .. NELBO: 1718.05\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.44 .. Rec_loss: 1701.26 .. NELBO: 1704.7\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1712.71 .. NELBO: 1715.72\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 3.14 .. Rec_loss: 1714.98 .. NELBO: 1718.12\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.71 .. Rec_loss: 1704.27 .. NELBO: 1707.98\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.8 .. Rec_loss: 1714.92 .. NELBO: 1717.72\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 1716.18 .. NELBO: 1719.51\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1700.91 .. NELBO: 1703.95\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1711.75 .. NELBO: 1714.6\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1713.33 .. NELBO: 1716.46\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.54 .. Rec_loss: 1699.72 .. NELBO: 1703.26\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1711.07 .. NELBO: 1714.28\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 1713.49 .. NELBO: 1716.88\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1703.76 .. NELBO: 1706.86\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1712.92 .. NELBO: 1716.02\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 3.63 .. Rec_loss: 1714.31 .. NELBO: 1717.94\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.16 .. Rec_loss: 1701.27 .. NELBO: 1704.43\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1711.36 .. NELBO: 1714.46\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 3.49 .. Rec_loss: 1712.75 .. NELBO: 1716.24\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.4 .. Rec_loss: 1698.37 .. NELBO: 1701.77\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.34 .. Rec_loss: 1709.98 .. NELBO: 1713.32\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 3.65 .. Rec_loss: 1711.39 .. NELBO: 1715.04\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.36 .. Rec_loss: 1698.07 .. NELBO: 1701.43\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.43 .. Rec_loss: 1709.54 .. NELBO: 1712.97\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 3.67 .. Rec_loss: 1711.18 .. NELBO: 1714.85\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.4 .. Rec_loss: 1697.61 .. NELBO: 1701.01\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.43 .. Rec_loss: 1709.22 .. NELBO: 1712.65\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 3.77 .. Rec_loss: 1710.78 .. NELBO: 1714.55\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.17 .. Rec_loss: 1697.99 .. NELBO: 1701.16\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.31 .. Rec_loss: 1709.46 .. NELBO: 1712.77\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 3.66 .. Rec_loss: 1710.87 .. NELBO: 1714.53\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.03 .. Rec_loss: 1698.95 .. NELBO: 1701.98\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1710.39 .. NELBO: 1713.62\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 3.43 .. Rec_loss: 1712.7 .. NELBO: 1716.13\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.41 .. Rec_loss: 1700.25 .. NELBO: 1703.66\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.27 .. Rec_loss: 1711.17 .. NELBO: 1714.44\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.51 .. Rec_loss: 1712.79 .. NELBO: 1716.3\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1697.42 .. NELBO: 1700.84\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.41 .. Rec_loss: 1708.62 .. NELBO: 1712.03\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3.73 .. Rec_loss: 1710.26 .. NELBO: 1713.99\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.45 .. Rec_loss: 1697.0 .. NELBO: 1700.45\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1708.35 .. NELBO: 1711.77\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.73 .. Rec_loss: 1710.51 .. NELBO: 1714.24\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.48 .. Rec_loss: 1697.86 .. NELBO: 1701.34\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1709.2 .. NELBO: 1712.41\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.69 .. Rec_loss: 1710.57 .. NELBO: 1714.26\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 1696.57 .. NELBO: 1699.9\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.36 .. Rec_loss: 1708.17 .. NELBO: 1711.53\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.8 .. Rec_loss: 1709.37 .. NELBO: 1713.17\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.22 .. Rec_loss: 1696.26 .. NELBO: 1699.48\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.38 .. Rec_loss: 1707.69 .. NELBO: 1711.07\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.71 .. Rec_loss: 1709.33 .. NELBO: 1713.04\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1695.96 .. NELBO: 1699.19\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.47 .. Rec_loss: 1707.14 .. NELBO: 1710.61\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 4.06 .. Rec_loss: 1708.11 .. NELBO: 1712.17\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.55 .. Rec_loss: 1695.86 .. NELBO: 1699.41\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.53 .. Rec_loss: 1707.47 .. NELBO: 1711.0\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.78 .. Rec_loss: 1710.08 .. NELBO: 1713.86\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1698.89 .. NELBO: 1702.31\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.4 .. Rec_loss: 1709.13 .. NELBO: 1712.53\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.89 .. Rec_loss: 1709.99 .. NELBO: 1713.88\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.74 .. Rec_loss: 1694.53 .. NELBO: 1699.27\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.21 .. Rec_loss: 1706.61 .. NELBO: 1710.82\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1708.87 .. NELBO: 1713.32\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.67 .. Rec_loss: 1696.69 .. NELBO: 1700.36\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1707.75 .. NELBO: 1711.31\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 3.98 .. Rec_loss: 1709.11 .. NELBO: 1713.09\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.72 .. Rec_loss: 1694.63 .. NELBO: 1699.35\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.1 .. Rec_loss: 1706.69 .. NELBO: 1710.79\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 4.3 .. Rec_loss: 1708.92 .. NELBO: 1713.22\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.88 .. Rec_loss: 1699.06 .. NELBO: 1702.94\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.66 .. Rec_loss: 1709.37 .. NELBO: 1713.03\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4.16 .. Rec_loss: 1710.86 .. NELBO: 1715.02\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.75 .. Rec_loss: 1696.96 .. NELBO: 1700.71\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.6 .. Rec_loss: 1707.88 .. NELBO: 1711.48\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 3.83 .. Rec_loss: 1709.99 .. NELBO: 1713.82\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.8 .. Rec_loss: 1702.03 .. NELBO: 1705.83\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.52 .. Rec_loss: 1710.71 .. NELBO: 1714.23\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 3.92 .. Rec_loss: 1712.21 .. NELBO: 1716.13\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.94 .. Rec_loss: 1699.63 .. NELBO: 1703.57\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.66 .. Rec_loss: 1709.42 .. NELBO: 1713.08\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 3.91 .. Rec_loss: 1711.07 .. NELBO: 1714.98\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.07 .. Rec_loss: 1697.78 .. NELBO: 1701.85\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.72 .. Rec_loss: 1708.18 .. NELBO: 1711.9\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.09 .. Rec_loss: 1709.72 .. NELBO: 1713.81\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 1696.66 .. NELBO: 1700.85\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.93 .. Rec_loss: 1707.58 .. NELBO: 1711.51\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 4.29 .. Rec_loss: 1709.16 .. NELBO: 1713.45\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.36 .. Rec_loss: 1695.26 .. NELBO: 1699.62\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.02 .. Rec_loss: 1706.98 .. NELBO: 1711.0\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 1708.46 .. NELBO: 1712.92\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.53 .. Rec_loss: 1695.17 .. NELBO: 1699.7\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.2 .. Rec_loss: 1706.71 .. NELBO: 1710.91\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1708.65 .. NELBO: 1713.07\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.68 .. Rec_loss: 1693.34 .. NELBO: 1698.02\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.31 .. Rec_loss: 1705.81 .. NELBO: 1710.12\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1707.37 .. NELBO: 1712.21\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 1694.34 .. NELBO: 1698.53\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.07 .. Rec_loss: 1706.23 .. NELBO: 1710.3\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1707.69 .. NELBO: 1712.11\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:51:34,329] Trial 14 finished with values: [0.0011476612687747398, 0.13023255813953488] and parameters: {'num_topics': 43, 'dropout': 0.14595764585550755, 't_hidden_size': 200, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.31064109780951843, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=40, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=50, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=50, out_features=40, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=50, out_features=40, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.22 .. Rec_loss: 2005.09 .. NELBO: 2006.31\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 1885.92 .. NELBO: 1886.69\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.71 .. Rec_loss: 1873.65 .. NELBO: 1874.36\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1723.48 .. NELBO: 1723.59\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1730.97 .. NELBO: 1731.04\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1734.25 .. NELBO: 1734.31\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.7 .. NELBO: 1719.72\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.87 .. NELBO: 1727.88\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.06 .. NELBO: 1731.07\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.36 .. NELBO: 1718.37\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.56 .. NELBO: 1726.57\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.72 .. NELBO: 1729.73\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.6 .. NELBO: 1717.6\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.8 .. NELBO: 1725.8\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.94 .. NELBO: 1728.94\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.03 .. NELBO: 1717.03\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.21 .. NELBO: 1725.21\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.35 .. NELBO: 1728.35\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.53 .. NELBO: 1716.53\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.92 .. NELBO: 1727.92\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.02 .. NELBO: 1716.02\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.48 .. NELBO: 1727.48\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.64 .. NELBO: 1715.64\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.96 .. NELBO: 1723.96\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.11 .. NELBO: 1727.11\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.32 .. NELBO: 1715.32\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.65 .. NELBO: 1723.65\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.79 .. NELBO: 1726.79\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.08 .. NELBO: 1715.08\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.44 .. NELBO: 1723.44\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.57 .. NELBO: 1726.57\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.85 .. NELBO: 1714.85\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.33 .. NELBO: 1723.33\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.44 .. NELBO: 1726.44\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.8 .. NELBO: 1714.8\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.2 .. NELBO: 1723.2\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.29 .. NELBO: 1726.29\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.89 .. NELBO: 1714.89\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.12 .. NELBO: 1723.12\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.24 .. NELBO: 1726.24\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.53 .. NELBO: 1714.53\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.98 .. NELBO: 1722.98\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.06 .. NELBO: 1726.06\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.41 .. NELBO: 1714.41\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.77 .. NELBO: 1722.77\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.86 .. NELBO: 1725.86\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.34 .. NELBO: 1714.34\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.65 .. NELBO: 1722.65\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.76 .. NELBO: 1725.76\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.13 .. NELBO: 1714.13\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.6 .. NELBO: 1722.6\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.68 .. NELBO: 1725.68\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.16 .. NELBO: 1714.16\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.59 .. NELBO: 1722.59\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.65 .. NELBO: 1725.65\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.17 .. NELBO: 1714.17\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.48 .. NELBO: 1722.48\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.56 .. NELBO: 1725.56\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.92 .. NELBO: 1713.92\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.33 .. NELBO: 1722.33\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.39 .. NELBO: 1725.39\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.9 .. NELBO: 1713.9\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.3 .. NELBO: 1722.3\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.38 .. NELBO: 1725.38\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.75 .. NELBO: 1713.75\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.2 .. NELBO: 1722.2\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.24 .. NELBO: 1725.24\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.79 .. NELBO: 1713.79\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.13 .. NELBO: 1722.13\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.69 .. NELBO: 1713.69\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.12 .. NELBO: 1722.12\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.6 .. NELBO: 1713.6\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.04 .. NELBO: 1722.04\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.07 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.57 .. NELBO: 1713.57\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.0 .. NELBO: 1725.0\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.01 .. NELBO: 1725.01\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.55 .. NELBO: 1713.55\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.99 .. NELBO: 1721.99\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.98 .. NELBO: 1724.98\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.92 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.63 .. NELBO: 1713.63\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.98 .. NELBO: 1721.98\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.81 .. NELBO: 1721.81\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.32 .. NELBO: 1713.32\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.27 .. NELBO: 1713.27\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.37 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:52:46,837] Trial 15 finished with values: [-0.007863069193821251, 0.0275] and parameters: {'num_topics': 40, 'dropout': 0.31064109780951843, 't_hidden_size': 50, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5171898059281129, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=28, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=28, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=28, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.88 .. Rec_loss: 2004.53 .. NELBO: 2005.41\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 1886.0 .. NELBO: 1886.51\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 1873.8 .. NELBO: 1874.27\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.32 .. NELBO: 1723.33\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1730.95 .. NELBO: 1730.96\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1734.26 .. NELBO: 1734.27\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.62 .. NELBO: 1719.62\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.8 .. NELBO: 1727.8\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.01 .. NELBO: 1731.01\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.31 .. NELBO: 1718.31\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.52 .. NELBO: 1726.52\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.68 .. NELBO: 1729.68\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.6 .. NELBO: 1717.6\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.8 .. NELBO: 1725.8\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.93 .. NELBO: 1728.93\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.98 .. NELBO: 1716.98\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.19 .. NELBO: 1725.19\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.34 .. NELBO: 1728.34\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.46 .. NELBO: 1716.46\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.86 .. NELBO: 1727.86\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.0 .. NELBO: 1716.0\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.49 .. NELBO: 1727.49\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.67 .. NELBO: 1715.67\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.01 .. NELBO: 1724.01\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.14 .. NELBO: 1727.14\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.45 .. NELBO: 1715.45\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.73 .. NELBO: 1723.73\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.87 .. NELBO: 1726.87\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.24 .. NELBO: 1715.24\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.56 .. NELBO: 1723.56\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.7 .. NELBO: 1726.7\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.95 .. NELBO: 1714.95\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.34 .. NELBO: 1723.34\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.46 .. NELBO: 1726.46\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.73 .. NELBO: 1714.73\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.09 .. NELBO: 1723.09\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.2 .. NELBO: 1726.2\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.59 .. NELBO: 1714.59\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.96 .. NELBO: 1722.96\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.07 .. NELBO: 1726.07\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.85 .. NELBO: 1722.85\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.93 .. NELBO: 1725.93\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.46 .. NELBO: 1714.46\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.79 .. NELBO: 1722.79\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.88 .. NELBO: 1725.88\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.76 .. NELBO: 1722.76\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.85 .. NELBO: 1725.85\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.21 .. NELBO: 1714.21\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.63 .. NELBO: 1722.63\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.72 .. NELBO: 1725.72\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.06 .. NELBO: 1714.06\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.57 .. NELBO: 1722.57\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.61 .. NELBO: 1725.61\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.19 .. NELBO: 1714.19\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.51 .. NELBO: 1722.51\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.58 .. NELBO: 1725.58\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.01 .. NELBO: 1714.01\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.43 .. NELBO: 1722.43\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.47 .. NELBO: 1725.47\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.91 .. NELBO: 1713.91\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.29 .. NELBO: 1722.29\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.34 .. NELBO: 1725.34\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.89 .. NELBO: 1713.89\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.25 .. NELBO: 1722.25\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.34 .. NELBO: 1725.34\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.7 .. NELBO: 1713.7\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.23 .. NELBO: 1722.23\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.27 .. NELBO: 1725.27\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.74 .. NELBO: 1713.74\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.17 .. NELBO: 1722.17\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.72 .. NELBO: 1713.72\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.08 .. NELBO: 1722.08\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.12 .. NELBO: 1725.12\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.61 .. NELBO: 1713.61\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.02 .. NELBO: 1722.02\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.08 .. NELBO: 1725.08\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.56 .. NELBO: 1713.56\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.99 .. NELBO: 1721.99\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.01 .. NELBO: 1725.01\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.62 .. NELBO: 1713.62\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.97 .. NELBO: 1721.97\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.04 .. NELBO: 1725.04\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.95 .. NELBO: 1721.95\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.99 .. NELBO: 1724.99\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.82 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.35 .. NELBO: 1713.35\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.37 .. NELBO: 1713.37\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.32 .. NELBO: 1713.32\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.77 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.91 .. NELBO: 1712.91\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.0 .. NELBO: 1713.0\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.81 .. NELBO: 1712.81\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:54:02,792] Trial 16 finished with values: [-0.00810621117329217, 0.039285714285714285] and parameters: {'num_topics': 28, 'dropout': 0.5171898059281129, 't_hidden_size': 300, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.07029839529521899, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=16, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=16, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=16, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.7 .. Rec_loss: 2000.59 .. NELBO: 2001.29\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 1883.59 .. NELBO: 1883.99\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.4 .. Rec_loss: 1871.61 .. NELBO: 1872.01\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.23 .. NELBO: 1723.28\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1730.91 .. NELBO: 1730.94\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1734.21 .. NELBO: 1734.24\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.68 .. NELBO: 1719.7\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.88 .. NELBO: 1727.9\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.08 .. NELBO: 1731.1\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.41 .. NELBO: 1718.42\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.6 .. NELBO: 1726.61\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.76 .. NELBO: 1729.77\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.69 .. NELBO: 1717.69\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.86 .. NELBO: 1725.86\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.0 .. NELBO: 1729.0\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.05 .. NELBO: 1717.05\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.39 .. NELBO: 1728.39\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.51 .. NELBO: 1716.51\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.9 .. NELBO: 1727.9\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.05 .. NELBO: 1716.05\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.49 .. NELBO: 1727.49\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.67 .. NELBO: 1715.67\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.99 .. NELBO: 1723.99\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.13 .. NELBO: 1727.13\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.38 .. NELBO: 1715.38\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.72 .. NELBO: 1723.72\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.86 .. NELBO: 1726.86\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.1 .. NELBO: 1715.1\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.46 .. NELBO: 1723.46\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.59 .. NELBO: 1726.59\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.91 .. NELBO: 1714.91\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.27 .. NELBO: 1723.27\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.39 .. NELBO: 1726.39\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.72 .. NELBO: 1714.72\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.1 .. NELBO: 1723.1\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.21 .. NELBO: 1726.21\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.63 .. NELBO: 1714.63\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.97 .. NELBO: 1722.97\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.09 .. NELBO: 1726.09\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.51 .. NELBO: 1714.51\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.91 .. NELBO: 1722.91\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.01 .. NELBO: 1726.01\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.47 .. NELBO: 1714.47\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.91 .. NELBO: 1722.91\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.98 .. NELBO: 1725.98\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.4 .. NELBO: 1714.4\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.76 .. NELBO: 1722.76\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.83 .. NELBO: 1725.83\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.3 .. NELBO: 1714.3\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.61 .. NELBO: 1722.61\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.69 .. NELBO: 1725.69\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.08 .. NELBO: 1714.08\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.48 .. NELBO: 1722.48\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.54 .. NELBO: 1725.54\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.02 .. NELBO: 1714.02\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.41 .. NELBO: 1722.41\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.47 .. NELBO: 1725.47\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.98 .. NELBO: 1713.98\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.37 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.43 .. NELBO: 1725.43\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.94 .. NELBO: 1713.94\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.34 .. NELBO: 1722.34\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.41 .. NELBO: 1725.41\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.83 .. NELBO: 1713.83\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.27 .. NELBO: 1722.27\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.33 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.75 .. NELBO: 1713.75\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.13 .. NELBO: 1722.13\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.06 .. NELBO: 1722.06\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.12 .. NELBO: 1725.12\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.54 .. NELBO: 1713.54\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.0 .. NELBO: 1725.0\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.81 .. NELBO: 1721.81\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.35 .. NELBO: 1713.35\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.77 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.38 .. NELBO: 1713.38\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.33 .. NELBO: 1713.33\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.83 .. NELBO: 1712.83\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.34 .. NELBO: 1721.34\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:55:13,497] Trial 17 finished with values: [-0.007972072099875022, 0.0625] and parameters: {'num_topics': 16, 'dropout': 0.07029839529521899, 't_hidden_size': 200, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.2733758614729633, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=22, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=22, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=22, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.55 .. Rec_loss: 2004.99 .. NELBO: 2006.54\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.87 .. Rec_loss: 1885.33 .. NELBO: 1886.2\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.8 .. Rec_loss: 1873.05 .. NELBO: 1873.85\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 1723.93 .. NELBO: 1724.05\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1731.14 .. NELBO: 1731.23\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1734.39 .. NELBO: 1734.47\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1719.94 .. NELBO: 1719.99\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1728.03 .. NELBO: 1728.07\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1731.25 .. NELBO: 1731.29\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1718.47 .. NELBO: 1718.51\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1726.67 .. NELBO: 1726.7\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1729.83 .. NELBO: 1729.86\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.74 .. NELBO: 1717.75\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.9 .. NELBO: 1725.91\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.04 .. NELBO: 1729.05\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.12 .. NELBO: 1717.13\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.27 .. NELBO: 1725.27\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.42 .. NELBO: 1728.42\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.57 .. NELBO: 1716.57\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.94 .. NELBO: 1727.94\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.05 .. NELBO: 1716.05\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.51 .. NELBO: 1727.51\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.71 .. NELBO: 1715.71\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.05 .. NELBO: 1724.05\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.19 .. NELBO: 1727.19\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.45 .. NELBO: 1715.45\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.79 .. NELBO: 1723.79\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.93 .. NELBO: 1726.93\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.21 .. NELBO: 1715.21\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.56 .. NELBO: 1723.56\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.69 .. NELBO: 1726.69\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.94 .. NELBO: 1714.94\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.3 .. NELBO: 1723.3\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.42 .. NELBO: 1726.42\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.78 .. NELBO: 1714.78\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.11 .. NELBO: 1723.11\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.22 .. NELBO: 1726.22\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.66 .. NELBO: 1714.66\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.01 .. NELBO: 1723.01\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.14 .. NELBO: 1726.14\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.46 .. NELBO: 1714.46\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.95 .. NELBO: 1722.95\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.05 .. NELBO: 1726.05\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.38 .. NELBO: 1714.38\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.83 .. NELBO: 1722.83\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.9 .. NELBO: 1725.9\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.34 .. NELBO: 1714.34\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.72 .. NELBO: 1722.72\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.81 .. NELBO: 1725.81\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.22 .. NELBO: 1714.22\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.65 .. NELBO: 1722.65\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.7 .. NELBO: 1725.7\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.31 .. NELBO: 1714.31\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.57 .. NELBO: 1722.57\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.69 .. NELBO: 1725.69\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.1 .. NELBO: 1714.1\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.58 .. NELBO: 1722.58\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.65 .. NELBO: 1725.65\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.91 .. NELBO: 1713.91\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.39 .. NELBO: 1722.39\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.43 .. NELBO: 1725.43\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.96 .. NELBO: 1713.96\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.3 .. NELBO: 1722.3\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.36 .. NELBO: 1725.36\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.82 .. NELBO: 1713.82\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.21 .. NELBO: 1722.21\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.73 .. NELBO: 1713.73\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.1 .. NELBO: 1722.1\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.19 .. NELBO: 1725.19\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.63 .. NELBO: 1713.63\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.14 .. NELBO: 1722.14\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.19 .. NELBO: 1725.19\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.64 .. NELBO: 1713.64\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.64 .. NELBO: 1713.64\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.97 .. NELBO: 1721.97\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.02 .. NELBO: 1725.02\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.38 .. NELBO: 1713.38\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.41 .. NELBO: 1713.41\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.27 .. NELBO: 1713.27\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.6 .. NELBO: 1724.6\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:56:30,119] Trial 18 finished with values: [-0.008032606468240756, 0.05] and parameters: {'num_topics': 22, 'dropout': 0.2733758614729633, 't_hidden_size': 300, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.13245590924643258, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=44, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=44, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=44, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 2016.94 .. NELBO: 2019.28\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.22 .. Rec_loss: 1892.89 .. NELBO: 1894.11\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 1.09 .. Rec_loss: 1880.01 .. NELBO: 1881.1\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.64 .. NELBO: 1723.65\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.06 .. NELBO: 1731.06\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1734.35 .. NELBO: 1734.35\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.62 .. NELBO: 1719.62\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.77 .. NELBO: 1727.77\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1730.98 .. NELBO: 1730.98\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.19 .. NELBO: 1718.19\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.43 .. NELBO: 1726.43\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.59 .. NELBO: 1729.59\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.5 .. NELBO: 1717.5\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.71 .. NELBO: 1725.71\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.86 .. NELBO: 1728.86\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.89 .. NELBO: 1716.89\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.12 .. NELBO: 1725.12\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.26 .. NELBO: 1728.26\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.48 .. NELBO: 1716.48\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.81 .. NELBO: 1727.81\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.08 .. NELBO: 1716.08\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.52 .. NELBO: 1727.52\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.58 .. NELBO: 1715.58\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.0 .. NELBO: 1724.0\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.14 .. NELBO: 1727.14\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.38 .. NELBO: 1715.38\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.76 .. NELBO: 1723.76\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.87 .. NELBO: 1726.87\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.37 .. NELBO: 1715.37\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.6 .. NELBO: 1723.6\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.73 .. NELBO: 1726.73\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.98 .. NELBO: 1714.98\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.33 .. NELBO: 1723.33\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.47 .. NELBO: 1726.47\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.62 .. NELBO: 1714.62\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.07 .. NELBO: 1723.07\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.18 .. NELBO: 1726.18\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.57 .. NELBO: 1714.57\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.94 .. NELBO: 1722.94\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.04 .. NELBO: 1726.04\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.49 .. NELBO: 1714.49\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.82 .. NELBO: 1722.82\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.94 .. NELBO: 1725.94\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.31 .. NELBO: 1714.31\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.75 .. NELBO: 1722.75\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.84 .. NELBO: 1725.84\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.18 .. NELBO: 1714.18\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.59 .. NELBO: 1722.59\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.67 .. NELBO: 1725.67\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.07 .. NELBO: 1714.07\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.44 .. NELBO: 1722.44\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.52 .. NELBO: 1725.52\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.97 .. NELBO: 1713.97\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.36 .. NELBO: 1722.36\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.45 .. NELBO: 1725.45\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.89 .. NELBO: 1713.89\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.32 .. NELBO: 1722.32\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.39 .. NELBO: 1725.39\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.85 .. NELBO: 1713.85\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.28 .. NELBO: 1722.28\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.36 .. NELBO: 1725.36\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.82 .. NELBO: 1713.82\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.26 .. NELBO: 1722.26\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.33 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.78 .. NELBO: 1713.78\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.24 .. NELBO: 1722.24\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.27 .. NELBO: 1725.27\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.86 .. NELBO: 1713.86\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.18 .. NELBO: 1722.18\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.13 .. NELBO: 1725.13\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.54 .. NELBO: 1713.54\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.0 .. NELBO: 1722.0\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.61 .. NELBO: 1713.61\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.0 .. NELBO: 1722.0\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.56 .. NELBO: 1713.56\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.98 .. NELBO: 1721.98\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.55 .. NELBO: 1713.55\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.99 .. NELBO: 1721.99\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.04 .. NELBO: 1725.04\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.54 .. NELBO: 1713.54\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.39 .. NELBO: 1713.39\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.14 .. NELBO: 1713.14\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.0 .. NELBO: 1713.0\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:57:49,308] Trial 19 finished with values: [-0.008184440322500942, 0.02727272727272727] and parameters: {'num_topics': 44, 'dropout': 0.13245590924643258, 't_hidden_size': 300, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.3348440784817614, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=26, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=50, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=50, out_features=26, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=50, out_features=26, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.12 .. Rec_loss: 1998.41 .. NELBO: 1999.53\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 1882.35 .. NELBO: 1883.04\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 1870.49 .. NELBO: 1871.12\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1723.41 .. NELBO: 1723.49\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.98 .. NELBO: 1731.03\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.27 .. NELBO: 1734.32\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.74 .. NELBO: 1719.76\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.91 .. NELBO: 1727.93\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.11 .. NELBO: 1731.13\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.44 .. NELBO: 1718.45\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.64 .. NELBO: 1726.65\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.79 .. NELBO: 1729.8\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.67 .. NELBO: 1717.67\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.85 .. NELBO: 1725.85\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.99 .. NELBO: 1728.99\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.05 .. NELBO: 1717.05\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.4 .. NELBO: 1728.4\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.5 .. NELBO: 1716.5\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.9 .. NELBO: 1727.9\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.05 .. NELBO: 1716.05\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.49 .. NELBO: 1727.49\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.71 .. NELBO: 1715.71\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.02 .. NELBO: 1724.02\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.16 .. NELBO: 1727.16\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.39 .. NELBO: 1715.39\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.75 .. NELBO: 1723.75\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.88 .. NELBO: 1726.88\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.2 .. NELBO: 1715.2\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.54 .. NELBO: 1723.54\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.68 .. NELBO: 1726.68\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.99 .. NELBO: 1714.99\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.33 .. NELBO: 1723.33\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.45 .. NELBO: 1726.45\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.74 .. NELBO: 1714.74\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.11 .. NELBO: 1723.11\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.22 .. NELBO: 1726.22\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.58 .. NELBO: 1714.58\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.95 .. NELBO: 1722.95\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.07 .. NELBO: 1726.07\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.45 .. NELBO: 1714.45\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.88 .. NELBO: 1722.88\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.96 .. NELBO: 1725.96\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.44 .. NELBO: 1714.44\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.8 .. NELBO: 1722.8\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.88 .. NELBO: 1725.88\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.44 .. NELBO: 1714.44\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.76 .. NELBO: 1722.76\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.84 .. NELBO: 1725.84\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.29 .. NELBO: 1714.29\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.66 .. NELBO: 1722.66\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.74 .. NELBO: 1725.74\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.13 .. NELBO: 1714.13\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.62 .. NELBO: 1725.62\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.08 .. NELBO: 1714.08\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.5 .. NELBO: 1722.5\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.55 .. NELBO: 1725.55\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.01 .. NELBO: 1714.01\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.38 .. NELBO: 1722.38\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.43 .. NELBO: 1725.43\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.89 .. NELBO: 1713.89\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.27 .. NELBO: 1722.27\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.32 .. NELBO: 1725.32\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.8 .. NELBO: 1713.8\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.2 .. NELBO: 1722.2\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.69 .. NELBO: 1713.69\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.06 .. NELBO: 1722.06\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.03 .. NELBO: 1722.03\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.07 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.95 .. NELBO: 1721.95\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.0 .. NELBO: 1725.0\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.99 .. NELBO: 1724.99\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.48 .. NELBO: 1713.48\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.79 .. NELBO: 1724.79\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.83 .. NELBO: 1712.83\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.85 .. NELBO: 1712.85\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.37 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.37 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:59:01,338] Trial 20 finished with values: [-0.008573141269459303, 0.04230769230769231] and parameters: {'num_topics': 26, 'dropout': 0.3348440784817614, 't_hidden_size': 50, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.3594262187553921, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=17, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=17, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=17, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 2000.89 .. NELBO: 2001.58\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.58 .. Rec_loss: 1883.14 .. NELBO: 1883.72\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 1871.12 .. NELBO: 1871.71\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.15 .. NELBO: 1723.2\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1730.88 .. NELBO: 1730.92\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.16 .. NELBO: 1734.2\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1719.83 .. NELBO: 1719.88\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1727.99 .. NELBO: 1728.04\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1731.19 .. NELBO: 1731.24\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1718.52 .. NELBO: 1718.56\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1726.71 .. NELBO: 1726.74\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1729.85 .. NELBO: 1729.88\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1717.74 .. NELBO: 1717.76\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1725.88 .. NELBO: 1725.9\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1729.02 .. NELBO: 1729.04\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1717.01 .. NELBO: 1717.04\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1725.16 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1728.28 .. NELBO: 1728.33\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 1716.01 .. NELBO: 1716.21\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 1724.15 .. NELBO: 1724.39\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 1727.23 .. NELBO: 1727.49\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 1714.72 .. NELBO: 1715.31\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 1722.95 .. NELBO: 1723.52\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 1726.04 .. NELBO: 1726.63\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.82 .. Rec_loss: 1713.84 .. NELBO: 1714.66\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.76 .. Rec_loss: 1722.24 .. NELBO: 1723.0\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.78 .. Rec_loss: 1725.3 .. NELBO: 1726.08\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.0 .. Rec_loss: 1712.83 .. NELBO: 1713.83\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.96 .. Rec_loss: 1721.29 .. NELBO: 1722.25\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 1.0 .. Rec_loss: 1724.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.5 .. Rec_loss: 1711.54 .. NELBO: 1713.04\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.41 .. Rec_loss: 1720.09 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.45 .. Rec_loss: 1722.8 .. NELBO: 1724.25\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.06 .. Rec_loss: 1709.9 .. NELBO: 1711.96\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.72 .. Rec_loss: 1719.18 .. NELBO: 1720.9\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 1.87 .. Rec_loss: 1721.47 .. NELBO: 1723.34\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1707.38 .. NELBO: 1709.92\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.96 .. Rec_loss: 1717.4 .. NELBO: 1719.36\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.13 .. Rec_loss: 1719.49 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1705.24 .. NELBO: 1707.98\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.08 .. Rec_loss: 1715.59 .. NELBO: 1717.67\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1717.51 .. NELBO: 1719.8\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1703.88 .. NELBO: 1706.89\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.26 .. Rec_loss: 1714.6 .. NELBO: 1716.86\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1716.38 .. NELBO: 1718.86\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1702.87 .. NELBO: 1705.91\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1713.69 .. NELBO: 1716.05\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.55 .. Rec_loss: 1715.46 .. NELBO: 1718.01\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.16 .. Rec_loss: 1701.97 .. NELBO: 1705.13\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.42 .. Rec_loss: 1713.03 .. NELBO: 1715.45\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1714.78 .. NELBO: 1717.42\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1700.96 .. NELBO: 1704.01\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1712.23 .. NELBO: 1714.63\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1714.0 .. NELBO: 1716.62\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1700.54 .. NELBO: 1703.56\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 1711.73 .. NELBO: 1714.2\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1713.43 .. NELBO: 1716.07\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1700.0 .. NELBO: 1703.0\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1711.35 .. NELBO: 1713.83\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1713.0 .. NELBO: 1715.7\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1700.05 .. NELBO: 1703.02\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1711.23 .. NELBO: 1713.79\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1712.86 .. NELBO: 1715.61\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1699.7 .. NELBO: 1702.68\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1710.86 .. NELBO: 1713.53\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1712.46 .. NELBO: 1715.32\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1699.16 .. NELBO: 1702.07\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1710.54 .. NELBO: 1713.2\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1712.04 .. NELBO: 1714.94\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1699.18 .. NELBO: 1701.93\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1710.44 .. NELBO: 1713.05\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1711.84 .. NELBO: 1714.75\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1699.22 .. NELBO: 1701.91\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1710.49 .. NELBO: 1713.08\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1711.83 .. NELBO: 1714.69\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1699.17 .. NELBO: 1701.93\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1710.74 .. NELBO: 1713.37\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1711.98 .. NELBO: 1714.89\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1699.14 .. NELBO: 1701.93\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1710.76 .. NELBO: 1713.44\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1711.98 .. NELBO: 1714.95\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1698.62 .. NELBO: 1701.35\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1710.59 .. NELBO: 1713.2\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1711.66 .. NELBO: 1714.62\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1699.28 .. NELBO: 1701.91\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1711.07 .. NELBO: 1713.57\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1711.96 .. NELBO: 1714.8\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1698.99 .. NELBO: 1701.81\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1710.79 .. NELBO: 1713.35\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1711.69 .. NELBO: 1714.57\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1698.09 .. NELBO: 1700.94\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1710.02 .. NELBO: 1712.58\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1710.96 .. NELBO: 1713.83\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1697.34 .. NELBO: 1700.2\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1709.61 .. NELBO: 1712.1\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1710.44 .. NELBO: 1713.31\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1697.0 .. NELBO: 1699.89\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1709.27 .. NELBO: 1711.77\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1710.17 .. NELBO: 1713.02\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1696.4 .. NELBO: 1699.39\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1708.68 .. NELBO: 1711.26\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1709.47 .. NELBO: 1712.4\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1696.24 .. NELBO: 1699.23\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1708.5 .. NELBO: 1711.04\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1709.37 .. NELBO: 1712.27\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.18 .. Rec_loss: 1695.79 .. NELBO: 1698.97\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1708.17 .. NELBO: 1710.83\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1708.97 .. NELBO: 1711.97\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1696.08 .. NELBO: 1699.17\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1708.04 .. NELBO: 1710.72\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1708.79 .. NELBO: 1711.83\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1696.1 .. NELBO: 1699.14\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1708.09 .. NELBO: 1710.75\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1708.81 .. NELBO: 1711.82\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1695.47 .. NELBO: 1698.57\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1707.86 .. NELBO: 1710.55\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1708.6 .. NELBO: 1711.64\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.19 .. Rec_loss: 1695.71 .. NELBO: 1698.9\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1708.03 .. NELBO: 1710.78\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 3.11 .. Rec_loss: 1708.78 .. NELBO: 1711.89\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.24 .. Rec_loss: 1695.32 .. NELBO: 1698.56\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1707.79 .. NELBO: 1710.56\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 3.14 .. Rec_loss: 1708.46 .. NELBO: 1711.6\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.2 .. Rec_loss: 1695.46 .. NELBO: 1698.66\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1707.79 .. NELBO: 1710.57\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 3.12 .. Rec_loss: 1708.37 .. NELBO: 1711.49\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1695.26 .. NELBO: 1698.41\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1707.77 .. NELBO: 1710.51\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1708.29 .. NELBO: 1711.44\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.18 .. Rec_loss: 1695.18 .. NELBO: 1698.36\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.81 .. Rec_loss: 1707.65 .. NELBO: 1710.46\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1708.11 .. NELBO: 1711.32\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1695.68 .. NELBO: 1698.81\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1707.88 .. NELBO: 1710.71\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1708.22 .. NELBO: 1711.45\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.29 .. Rec_loss: 1694.85 .. NELBO: 1698.14\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.94 .. Rec_loss: 1707.17 .. NELBO: 1710.11\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 3.32 .. Rec_loss: 1707.64 .. NELBO: 1710.96\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.14 .. Rec_loss: 1695.27 .. NELBO: 1698.41\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1707.6 .. NELBO: 1710.49\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1708.0 .. NELBO: 1711.28\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.24 .. Rec_loss: 1695.65 .. NELBO: 1698.89\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1708.02 .. NELBO: 1710.97\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 3.32 .. Rec_loss: 1708.36 .. NELBO: 1711.68\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.18 .. Rec_loss: 1695.46 .. NELBO: 1698.64\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1708.0 .. NELBO: 1710.95\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 3.36 .. Rec_loss: 1708.43 .. NELBO: 1711.79\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1696.16 .. NELBO: 1699.25\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1708.44 .. NELBO: 1711.39\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 3.37 .. Rec_loss: 1708.64 .. NELBO: 1712.01\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:00:13,920] Trial 21 finished with values: [-0.0016968190483323267, 0.20588235294117646] and parameters: {'num_topics': 17, 'dropout': 0.3594262187553921, 't_hidden_size': 200, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.38702064203453895, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=45, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=45, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=45, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 2006.69 .. NELBO: 2007.1\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 1886.43 .. NELBO: 1886.93\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 1874.05 .. NELBO: 1874.55\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1723.36 .. NELBO: 1723.46\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1730.97 .. NELBO: 1731.05\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1734.24 .. NELBO: 1734.31\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.84 .. NELBO: 1719.86\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.97 .. NELBO: 1727.99\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.16 .. NELBO: 1731.18\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1718.42 .. NELBO: 1718.45\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1726.53 .. NELBO: 1726.58\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1729.67 .. NELBO: 1729.73\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 1717.2 .. NELBO: 1717.42\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 1725.31 .. NELBO: 1725.55\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 1728.39 .. NELBO: 1728.65\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.58 .. Rec_loss: 1715.91 .. NELBO: 1716.49\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 1724.11 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 1727.21 .. NELBO: 1727.75\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.72 .. Rec_loss: 1715.18 .. NELBO: 1715.9\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 1723.71 .. NELBO: 1724.28\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.56 .. Rec_loss: 1726.82 .. NELBO: 1727.38\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.75 .. Rec_loss: 1714.6 .. NELBO: 1715.35\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 1723.1 .. NELBO: 1723.73\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 1726.18 .. NELBO: 1726.83\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 1714.2 .. NELBO: 1714.97\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.73 .. Rec_loss: 1722.5 .. NELBO: 1723.23\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 1725.6 .. NELBO: 1726.37\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.87 .. Rec_loss: 1713.57 .. NELBO: 1714.44\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.83 .. Rec_loss: 1721.97 .. NELBO: 1722.8\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.86 .. Rec_loss: 1725.06 .. NELBO: 1725.92\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.97 .. Rec_loss: 1713.15 .. NELBO: 1714.12\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.93 .. Rec_loss: 1721.47 .. NELBO: 1722.4\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.99 .. Rec_loss: 1724.55 .. NELBO: 1725.54\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.19 .. Rec_loss: 1712.15 .. NELBO: 1713.34\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.16 .. Rec_loss: 1720.66 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 1.22 .. Rec_loss: 1723.61 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.67 .. Rec_loss: 1710.99 .. NELBO: 1712.66\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.6 .. Rec_loss: 1719.55 .. NELBO: 1721.15\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 1.72 .. Rec_loss: 1722.23 .. NELBO: 1723.95\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1709.76 .. NELBO: 1711.93\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.03 .. Rec_loss: 1718.5 .. NELBO: 1720.53\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.2 .. Rec_loss: 1720.93 .. NELBO: 1723.13\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1709.34 .. NELBO: 1711.92\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1718.39 .. NELBO: 1720.63\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 1720.58 .. NELBO: 1723.05\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1707.74 .. NELBO: 1710.79\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1717.42 .. NELBO: 1719.9\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1719.33 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1706.19 .. NELBO: 1709.24\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.28 .. Rec_loss: 1716.73 .. NELBO: 1719.01\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1718.53 .. NELBO: 1721.07\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.47 .. Rec_loss: 1704.43 .. NELBO: 1707.9\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1714.95 .. NELBO: 1717.66\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1716.89 .. NELBO: 1719.76\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.53 .. Rec_loss: 1703.48 .. NELBO: 1707.01\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1714.2 .. NELBO: 1717.04\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1716.02 .. NELBO: 1719.07\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.51 .. Rec_loss: 1702.64 .. NELBO: 1706.15\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1713.73 .. NELBO: 1716.57\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1715.95 .. NELBO: 1719.0\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.53 .. Rec_loss: 1703.44 .. NELBO: 1706.97\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1714.35 .. NELBO: 1717.03\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1716.01 .. NELBO: 1719.11\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.53 .. Rec_loss: 1701.55 .. NELBO: 1705.08\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1712.72 .. NELBO: 1715.58\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 3.14 .. Rec_loss: 1714.61 .. NELBO: 1717.75\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.51 .. Rec_loss: 1700.58 .. NELBO: 1704.09\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1711.97 .. NELBO: 1715.07\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1713.98 .. NELBO: 1717.21\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1699.36 .. NELBO: 1702.92\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.16 .. Rec_loss: 1711.03 .. NELBO: 1714.19\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1712.89 .. NELBO: 1716.31\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.36 .. Rec_loss: 1699.86 .. NELBO: 1703.22\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1711.27 .. NELBO: 1714.23\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 3.35 .. Rec_loss: 1712.81 .. NELBO: 1716.16\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1700.23 .. NELBO: 1703.36\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1711.39 .. NELBO: 1714.48\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 3.36 .. Rec_loss: 1712.99 .. NELBO: 1716.35\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1699.73 .. NELBO: 1702.75\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.16 .. Rec_loss: 1711.07 .. NELBO: 1714.23\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 3.46 .. Rec_loss: 1712.71 .. NELBO: 1716.17\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1700.03 .. NELBO: 1702.73\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1711.18 .. NELBO: 1714.13\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 3.35 .. Rec_loss: 1712.66 .. NELBO: 1716.01\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1700.24 .. NELBO: 1702.97\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1711.24 .. NELBO: 1714.16\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.3 .. Rec_loss: 1712.5 .. NELBO: 1715.8\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1700.25 .. NELBO: 1703.01\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1711.3 .. NELBO: 1714.07\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3.19 .. Rec_loss: 1712.73 .. NELBO: 1715.92\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1699.83 .. NELBO: 1702.66\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1711.11 .. NELBO: 1713.82\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1713.73 .. NELBO: 1716.77\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1703.07 .. NELBO: 1705.85\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1713.64 .. NELBO: 1716.24\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1715.21 .. NELBO: 1718.16\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 1698.7 .. NELBO: 1701.76\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1709.82 .. NELBO: 1712.78\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.31 .. Rec_loss: 1711.38 .. NELBO: 1714.69\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.45 .. Rec_loss: 1697.41 .. NELBO: 1700.86\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.08 .. Rec_loss: 1709.35 .. NELBO: 1712.43\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.38 .. Rec_loss: 1711.78 .. NELBO: 1715.16\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1699.33 .. NELBO: 1702.48\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1710.91 .. NELBO: 1713.75\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.26 .. Rec_loss: 1712.58 .. NELBO: 1715.84\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.07 .. Rec_loss: 1697.95 .. NELBO: 1701.02\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1709.34 .. NELBO: 1712.22\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.3 .. Rec_loss: 1710.69 .. NELBO: 1713.99\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.72 .. Rec_loss: 1696.67 .. NELBO: 1700.39\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.29 .. Rec_loss: 1708.73 .. NELBO: 1712.02\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.5 .. Rec_loss: 1711.29 .. NELBO: 1714.79\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.26 .. Rec_loss: 1699.56 .. NELBO: 1702.82\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1710.87 .. NELBO: 1713.86\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.4 .. Rec_loss: 1712.65 .. NELBO: 1716.05\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.11 .. Rec_loss: 1697.76 .. NELBO: 1700.87\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1709.07 .. NELBO: 1712.12\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 1711.31 .. NELBO: 1714.64\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.66 .. Rec_loss: 1697.0 .. NELBO: 1700.66\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1709.13 .. NELBO: 1712.41\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 3.57 .. Rec_loss: 1710.77 .. NELBO: 1714.34\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.63 .. Rec_loss: 1695.94 .. NELBO: 1699.57\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1708.19 .. NELBO: 1711.47\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1710.75 .. NELBO: 1714.31\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.43 .. Rec_loss: 1697.97 .. NELBO: 1701.4\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1709.79 .. NELBO: 1712.92\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 3.53 .. Rec_loss: 1711.8 .. NELBO: 1715.33\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1698.76 .. NELBO: 1701.71\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1709.6 .. NELBO: 1712.61\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 3.37 .. Rec_loss: 1711.77 .. NELBO: 1715.14\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.51 .. Rec_loss: 1696.67 .. NELBO: 1700.18\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1708.56 .. NELBO: 1711.84\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 3.64 .. Rec_loss: 1710.28 .. NELBO: 1713.92\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.73 .. Rec_loss: 1695.27 .. NELBO: 1699.0\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 1707.58 .. NELBO: 1710.97\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 3.76 .. Rec_loss: 1709.83 .. NELBO: 1713.59\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.53 .. Rec_loss: 1695.87 .. NELBO: 1699.4\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.35 .. Rec_loss: 1708.28 .. NELBO: 1711.63\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 3.64 .. Rec_loss: 1710.09 .. NELBO: 1713.73\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.37 .. Rec_loss: 1696.48 .. NELBO: 1699.85\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.25 .. Rec_loss: 1708.24 .. NELBO: 1711.49\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 3.63 .. Rec_loss: 1710.43 .. NELBO: 1714.06\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.35 .. Rec_loss: 1696.94 .. NELBO: 1700.29\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.31 .. Rec_loss: 1708.99 .. NELBO: 1712.3\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 3.62 .. Rec_loss: 1710.96 .. NELBO: 1714.58\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1698.94 .. NELBO: 1701.9\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.17 .. Rec_loss: 1709.73 .. NELBO: 1712.9\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 1712.22 .. NELBO: 1715.61\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.3 .. Rec_loss: 1701.4 .. NELBO: 1704.7\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1711.29 .. NELBO: 1714.39\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 3.6 .. Rec_loss: 1712.82 .. NELBO: 1716.42\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:01:34,079] Trial 22 finished with values: [-0.0004960556170698477, 0.11777777777777777] and parameters: {'num_topics': 45, 'dropout': 0.38702064203453895, 't_hidden_size': 200, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.11897931196390538, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=16, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=16, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=16, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1998.79 .. NELBO: 1999.6\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.45 .. Rec_loss: 1882.43 .. NELBO: 1882.88\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.44 .. Rec_loss: 1870.52 .. NELBO: 1870.96\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1723.43 .. NELBO: 1723.51\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1731.01 .. NELBO: 1731.06\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.29 .. NELBO: 1734.34\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1719.85 .. NELBO: 1719.88\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1727.98 .. NELBO: 1728.01\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1731.2 .. NELBO: 1731.23\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.43 .. NELBO: 1718.44\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.64 .. NELBO: 1726.65\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.8 .. NELBO: 1729.81\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.67 .. NELBO: 1717.67\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.87 .. NELBO: 1725.87\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.01 .. NELBO: 1729.01\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.06 .. NELBO: 1717.06\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.4 .. NELBO: 1728.4\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.53 .. NELBO: 1716.53\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.92 .. NELBO: 1727.92\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.06 .. NELBO: 1716.06\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.48 .. NELBO: 1727.48\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.79 .. NELBO: 1715.79\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.09 .. NELBO: 1724.09\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.26 .. NELBO: 1727.26\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.39 .. NELBO: 1715.39\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.79 .. NELBO: 1723.79\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.92 .. NELBO: 1726.92\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.25 .. NELBO: 1715.25\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.6 .. NELBO: 1723.6\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.71 .. NELBO: 1726.71\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.07 .. NELBO: 1715.07\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.42 .. NELBO: 1723.42\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.55 .. NELBO: 1726.55\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.76 .. NELBO: 1714.76\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.17 .. NELBO: 1723.17\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.28 .. NELBO: 1726.28\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.68 .. NELBO: 1714.68\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.01 .. NELBO: 1723.01\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.13 .. NELBO: 1726.13\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.45 .. NELBO: 1714.45\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.89 .. NELBO: 1722.89\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.0 .. NELBO: 1726.0\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.35 .. NELBO: 1714.35\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.84 .. NELBO: 1722.84\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.89 .. NELBO: 1725.89\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.45 .. NELBO: 1714.45\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.78 .. NELBO: 1722.78\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.85 .. NELBO: 1725.85\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.34 .. NELBO: 1714.34\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.66 .. NELBO: 1722.66\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.75 .. NELBO: 1725.75\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.09 .. NELBO: 1714.09\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.49 .. NELBO: 1722.49\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.55 .. NELBO: 1725.55\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.05 .. NELBO: 1714.05\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.42 .. NELBO: 1722.42\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.5 .. NELBO: 1725.5\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.91 .. NELBO: 1713.91\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.38 .. NELBO: 1722.38\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.44 .. NELBO: 1725.44\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.91 .. NELBO: 1713.91\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.3 .. NELBO: 1722.3\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.35 .. NELBO: 1725.35\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.91 .. NELBO: 1713.91\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.24 .. NELBO: 1722.24\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.31 .. NELBO: 1725.31\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.77 .. NELBO: 1713.77\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.27 .. NELBO: 1722.27\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.31 .. NELBO: 1725.31\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.84 .. NELBO: 1713.84\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.21 .. NELBO: 1722.21\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.73 .. NELBO: 1713.73\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.1 .. NELBO: 1725.1\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.6 .. NELBO: 1713.6\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.04 .. NELBO: 1722.04\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.09 .. NELBO: 1725.09\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.02 .. NELBO: 1722.02\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.03 .. NELBO: 1722.03\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.07 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.56 .. NELBO: 1713.56\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.98 .. NELBO: 1721.98\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.02 .. NELBO: 1725.02\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.42 .. NELBO: 1713.42\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:02:44,841] Trial 23 finished with values: [-0.007718446778067803, 0.06875] and parameters: {'num_topics': 16, 'dropout': 0.11897931196390538, 't_hidden_size': 200, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.3229344587670168, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=47, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=47, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=47, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.97 .. Rec_loss: 2010.54 .. NELBO: 2011.51\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.56 .. Rec_loss: 1889.4 .. NELBO: 1889.96\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.5 .. Rec_loss: 1876.87 .. NELBO: 1877.37\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1723.52 .. NELBO: 1723.54\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.01 .. NELBO: 1731.02\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1734.3 .. NELBO: 1734.31\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.65 .. NELBO: 1719.65\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.81 .. NELBO: 1727.81\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.02 .. NELBO: 1731.02\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.26 .. NELBO: 1718.26\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.5 .. NELBO: 1726.5\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.66 .. NELBO: 1729.66\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.56 .. NELBO: 1717.56\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.76 .. NELBO: 1725.76\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.9 .. NELBO: 1728.9\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.97 .. NELBO: 1716.97\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.32 .. NELBO: 1728.32\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.43 .. NELBO: 1716.43\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.84 .. NELBO: 1727.84\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.99 .. NELBO: 1715.99\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.3 .. NELBO: 1724.3\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.44 .. NELBO: 1727.44\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.65 .. NELBO: 1715.65\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.96 .. NELBO: 1723.96\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.11 .. NELBO: 1727.11\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.4 .. NELBO: 1715.4\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.7 .. NELBO: 1723.7\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.85 .. NELBO: 1726.85\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.12 .. NELBO: 1715.12\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.49 .. NELBO: 1723.49\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.64 .. NELBO: 1726.64\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.87 .. NELBO: 1714.87\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.35 .. NELBO: 1723.35\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.45 .. NELBO: 1726.45\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.83 .. NELBO: 1714.83\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.14 .. NELBO: 1723.14\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.26 .. NELBO: 1726.26\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.57 .. NELBO: 1714.57\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.94 .. NELBO: 1722.94\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.04 .. NELBO: 1726.04\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.4 .. NELBO: 1714.4\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.79 .. NELBO: 1722.79\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.89 .. NELBO: 1725.89\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.33 .. NELBO: 1714.33\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.7 .. NELBO: 1722.7\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.81 .. NELBO: 1725.81\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.16 .. NELBO: 1714.16\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.62 .. NELBO: 1722.62\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.71 .. NELBO: 1725.71\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.18 .. NELBO: 1714.18\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.57 .. NELBO: 1722.57\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.63 .. NELBO: 1725.63\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.24 .. NELBO: 1714.24\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.53 .. NELBO: 1722.53\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.62 .. NELBO: 1725.62\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.01 .. NELBO: 1714.01\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.45 .. NELBO: 1722.45\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.53 .. NELBO: 1725.53\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.87 .. NELBO: 1713.87\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.37 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.4 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.03 .. NELBO: 1714.03\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.32 .. NELBO: 1722.32\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.4 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.81 .. NELBO: 1713.81\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.2 .. NELBO: 1722.2\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.68 .. NELBO: 1713.68\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.14 .. NELBO: 1725.14\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.58 .. NELBO: 1713.58\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.02 .. NELBO: 1722.02\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.07 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.01 .. NELBO: 1725.01\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.96 .. NELBO: 1724.96\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.39 .. NELBO: 1713.39\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.35 .. NELBO: 1713.35\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.41 .. NELBO: 1713.41\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.82 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.81 .. NELBO: 1712.81\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:03:57,905] Trial 24 finished with values: [-0.00813832527444089, 0.023404255319148935] and parameters: {'num_topics': 47, 'dropout': 0.3229344587670168, 't_hidden_size': 100, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.08802529452985816, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=32, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=32, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=32, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.12 .. Rec_loss: 2003.74 .. NELBO: 2004.86\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 1885.13 .. NELBO: 1885.75\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.56 .. Rec_loss: 1872.93 .. NELBO: 1873.49\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1723.52 .. NELBO: 1723.58\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1731.0 .. NELBO: 1731.04\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.26 .. NELBO: 1734.3\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1719.74 .. NELBO: 1719.77\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.89 .. NELBO: 1727.91\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.1 .. NELBO: 1731.12\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1718.47 .. NELBO: 1718.49\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.65 .. NELBO: 1726.66\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.8 .. NELBO: 1729.81\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.69 .. NELBO: 1717.7\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.89 .. NELBO: 1725.9\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.02 .. NELBO: 1729.03\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.13 .. NELBO: 1717.14\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.29 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.43 .. NELBO: 1728.43\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.58 .. NELBO: 1716.58\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.96 .. NELBO: 1727.96\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.07 .. NELBO: 1716.07\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.39 .. NELBO: 1724.39\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.53 .. NELBO: 1727.53\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.66 .. NELBO: 1715.66\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.98 .. NELBO: 1723.98\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.12 .. NELBO: 1727.12\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.38 .. NELBO: 1715.38\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.7 .. NELBO: 1723.7\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.85 .. NELBO: 1726.85\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.1 .. NELBO: 1715.1\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.48 .. NELBO: 1723.48\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.61 .. NELBO: 1726.61\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.87 .. NELBO: 1714.87\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.25 .. NELBO: 1723.25\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.37 .. NELBO: 1726.37\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.76 .. NELBO: 1714.76\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.11 .. NELBO: 1723.11\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.23 .. NELBO: 1726.23\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.62 .. NELBO: 1714.62\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.99 .. NELBO: 1722.99\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.11 .. NELBO: 1726.11\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.85 .. NELBO: 1722.85\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.94 .. NELBO: 1725.94\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.44 .. NELBO: 1714.44\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.79 .. NELBO: 1722.79\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.87 .. NELBO: 1725.87\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.35 .. NELBO: 1714.35\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.73 .. NELBO: 1722.73\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.82 .. NELBO: 1725.82\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.27 .. NELBO: 1714.27\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.7 .. NELBO: 1722.7\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.77 .. NELBO: 1725.77\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.17 .. NELBO: 1714.17\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.63 .. NELBO: 1725.63\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.06 .. NELBO: 1714.06\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.49 .. NELBO: 1722.49\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.55 .. NELBO: 1725.55\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.07 .. NELBO: 1714.07\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.42 .. NELBO: 1722.42\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.47 .. NELBO: 1725.47\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.92 .. NELBO: 1713.92\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.28 .. NELBO: 1722.28\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.34 .. NELBO: 1725.34\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.82 .. NELBO: 1713.82\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.2 .. NELBO: 1722.2\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.27 .. NELBO: 1725.27\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.67 .. NELBO: 1713.67\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.16 .. NELBO: 1725.16\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.1 .. NELBO: 1725.1\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.7 .. NELBO: 1713.7\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.04 .. NELBO: 1722.04\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.56 .. NELBO: 1713.56\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.02 .. NELBO: 1722.02\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.09 .. NELBO: 1725.09\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.97 .. NELBO: 1721.97\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.99 .. NELBO: 1724.99\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.58 .. NELBO: 1713.58\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.92 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.56 .. NELBO: 1713.56\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.01 .. NELBO: 1725.01\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.33 .. NELBO: 1713.33\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.75 .. NELBO: 1721.75\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.7 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.37 .. NELBO: 1713.37\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.83 .. NELBO: 1712.83\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:05:16,570] Trial 25 finished with values: [-0.008140502139030073, 0.034375] and parameters: {'num_topics': 32, 'dropout': 0.08802529452985816, 't_hidden_size': 200, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.2200493978790428, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=15, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=50, out_features=15, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=50, out_features=15, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 1995.09 .. NELBO: 1995.45\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 1880.47 .. NELBO: 1880.74\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 1868.79 .. NELBO: 1869.05\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1723.59 .. NELBO: 1723.66\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1731.1 .. NELBO: 1731.14\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.37 .. NELBO: 1734.41\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.76 .. NELBO: 1719.78\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.95 .. NELBO: 1727.97\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.16 .. NELBO: 1731.18\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.43 .. NELBO: 1718.43\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.64 .. NELBO: 1726.64\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.79 .. NELBO: 1729.79\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.69 .. NELBO: 1717.69\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.87 .. NELBO: 1725.87\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.01 .. NELBO: 1729.01\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.09 .. NELBO: 1717.09\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.27 .. NELBO: 1725.27\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.42 .. NELBO: 1728.42\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.52 .. NELBO: 1716.52\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.92 .. NELBO: 1727.92\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.07 .. NELBO: 1716.07\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.51 .. NELBO: 1727.51\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.69 .. NELBO: 1715.69\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.02 .. NELBO: 1724.02\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.16 .. NELBO: 1727.16\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.41 .. NELBO: 1715.41\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.75 .. NELBO: 1723.75\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.88 .. NELBO: 1726.88\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.19 .. NELBO: 1715.19\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.53 .. NELBO: 1723.53\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.67 .. NELBO: 1726.67\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.99 .. NELBO: 1714.99\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.36 .. NELBO: 1723.36\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.48 .. NELBO: 1726.48\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.79 .. NELBO: 1714.79\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.15 .. NELBO: 1723.15\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.26 .. NELBO: 1726.26\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.62 .. NELBO: 1714.62\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.99 .. NELBO: 1722.99\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.09 .. NELBO: 1726.09\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.47 .. NELBO: 1714.47\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.88 .. NELBO: 1722.88\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.97 .. NELBO: 1725.97\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.37 .. NELBO: 1714.37\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.79 .. NELBO: 1722.79\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.85 .. NELBO: 1725.85\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1714.35 .. NELBO: 1714.36\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1722.7 .. NELBO: 1722.71\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.77 .. NELBO: 1725.78\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1714.07 .. NELBO: 1714.11\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1722.31 .. NELBO: 1722.39\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1725.36 .. NELBO: 1725.46\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 1712.85 .. NELBO: 1713.22\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.43 .. Rec_loss: 1721.14 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 1724.24 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.73 .. Rec_loss: 1712.04 .. NELBO: 1712.77\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.71 .. Rec_loss: 1720.78 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.7 .. Rec_loss: 1723.87 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 1712.38 .. NELBO: 1713.12\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1720.88 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.87 .. Rec_loss: 1723.8 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.12 .. Rec_loss: 1711.41 .. NELBO: 1712.53\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.14 .. Rec_loss: 1720.12 .. NELBO: 1721.26\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 1.2 .. Rec_loss: 1722.88 .. NELBO: 1724.08\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.56 .. Rec_loss: 1711.01 .. NELBO: 1712.57\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.36 .. Rec_loss: 1719.71 .. NELBO: 1721.07\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 1.47 .. Rec_loss: 1722.17 .. NELBO: 1723.64\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.83 .. Rec_loss: 1709.03 .. NELBO: 1710.86\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.55 .. Rec_loss: 1718.19 .. NELBO: 1719.74\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 1.73 .. Rec_loss: 1720.37 .. NELBO: 1722.1\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.03 .. Rec_loss: 1707.08 .. NELBO: 1709.11\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.68 .. Rec_loss: 1716.74 .. NELBO: 1718.42\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 1.93 .. Rec_loss: 1718.71 .. NELBO: 1720.64\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.13 .. Rec_loss: 1705.99 .. NELBO: 1708.12\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.75 .. Rec_loss: 1716.05 .. NELBO: 1717.8\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.05 .. Rec_loss: 1717.94 .. NELBO: 1719.99\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1704.54 .. NELBO: 1706.83\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.84 .. Rec_loss: 1715.08 .. NELBO: 1716.92\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1716.95 .. NELBO: 1719.12\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1704.27 .. NELBO: 1706.48\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.76 .. Rec_loss: 1715.07 .. NELBO: 1716.83\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.05 .. Rec_loss: 1716.76 .. NELBO: 1718.81\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 1703.34 .. NELBO: 1705.67\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.81 .. Rec_loss: 1714.62 .. NELBO: 1716.43\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 2.02 .. Rec_loss: 1716.39 .. NELBO: 1718.41\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1704.34 .. NELBO: 1707.03\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.96 .. Rec_loss: 1715.37 .. NELBO: 1717.33\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.13 .. Rec_loss: 1716.95 .. NELBO: 1719.08\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1705.31 .. NELBO: 1707.9\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.93 .. Rec_loss: 1715.9 .. NELBO: 1717.83\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 2.07 .. Rec_loss: 1717.55 .. NELBO: 1719.62\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1706.25 .. NELBO: 1708.75\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.9 .. Rec_loss: 1716.41 .. NELBO: 1718.31\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 2.04 .. Rec_loss: 1718.13 .. NELBO: 1720.17\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1703.59 .. NELBO: 1706.12\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.99 .. Rec_loss: 1714.55 .. NELBO: 1716.54\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1716.16 .. NELBO: 1718.37\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.42 .. Rec_loss: 1702.85 .. NELBO: 1705.27\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.93 .. Rec_loss: 1713.74 .. NELBO: 1715.67\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 2.13 .. Rec_loss: 1715.31 .. NELBO: 1717.44\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1702.05 .. NELBO: 1704.41\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.9 .. Rec_loss: 1713.28 .. NELBO: 1715.18\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 2.12 .. Rec_loss: 1714.89 .. NELBO: 1717.01\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.39 .. Rec_loss: 1701.95 .. NELBO: 1704.34\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.9 .. Rec_loss: 1713.26 .. NELBO: 1715.16\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 2.15 .. Rec_loss: 1714.73 .. NELBO: 1716.88\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.35 .. Rec_loss: 1701.33 .. NELBO: 1703.68\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.85 .. Rec_loss: 1712.94 .. NELBO: 1714.79\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 2.06 .. Rec_loss: 1714.54 .. NELBO: 1716.6\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1701.25 .. NELBO: 1703.65\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.88 .. Rec_loss: 1712.92 .. NELBO: 1714.8\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 2.08 .. Rec_loss: 1714.6 .. NELBO: 1716.68\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.35 .. Rec_loss: 1701.44 .. NELBO: 1703.79\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.87 .. Rec_loss: 1712.96 .. NELBO: 1714.83\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 2.09 .. Rec_loss: 1714.44 .. NELBO: 1716.53\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 1700.98 .. NELBO: 1703.31\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.89 .. Rec_loss: 1712.62 .. NELBO: 1714.51\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 2.09 .. Rec_loss: 1714.06 .. NELBO: 1716.15\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 1701.13 .. NELBO: 1703.47\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.88 .. Rec_loss: 1712.8 .. NELBO: 1714.68\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 2.1 .. Rec_loss: 1714.19 .. NELBO: 1716.29\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.35 .. Rec_loss: 1701.52 .. NELBO: 1703.87\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.91 .. Rec_loss: 1713.01 .. NELBO: 1714.92\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 2.1 .. Rec_loss: 1714.4 .. NELBO: 1716.5\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.38 .. Rec_loss: 1702.23 .. NELBO: 1704.61\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.97 .. Rec_loss: 1713.42 .. NELBO: 1715.39\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1714.7 .. NELBO: 1716.87\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.16 .. Rec_loss: 1702.91 .. NELBO: 1705.07\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.89 .. Rec_loss: 1713.9 .. NELBO: 1715.79\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 2.1 .. Rec_loss: 1715.3 .. NELBO: 1717.4\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.15 .. Rec_loss: 1702.86 .. NELBO: 1705.01\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.92 .. Rec_loss: 1713.53 .. NELBO: 1715.45\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 2.11 .. Rec_loss: 1715.39 .. NELBO: 1717.5\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.39 .. Rec_loss: 1701.79 .. NELBO: 1704.18\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.04 .. Rec_loss: 1712.92 .. NELBO: 1714.96\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 2.25 .. Rec_loss: 1714.97 .. NELBO: 1717.22\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1701.8 .. NELBO: 1704.16\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.03 .. Rec_loss: 1712.93 .. NELBO: 1714.96\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 2.27 .. Rec_loss: 1714.86 .. NELBO: 1717.13\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1701.96 .. NELBO: 1704.33\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.01 .. Rec_loss: 1713.06 .. NELBO: 1715.07\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 2.27 .. Rec_loss: 1714.72 .. NELBO: 1716.99\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.38 .. Rec_loss: 1701.64 .. NELBO: 1704.02\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.01 .. Rec_loss: 1712.88 .. NELBO: 1714.89\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1714.32 .. NELBO: 1716.61\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.2 .. Rec_loss: 1701.44 .. NELBO: 1703.64\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.91 .. Rec_loss: 1712.98 .. NELBO: 1714.89\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 2.18 .. Rec_loss: 1714.46 .. NELBO: 1716.64\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:06:28,677] Trial 26 finished with values: [-0.007643347445374546, 0.17333333333333334] and parameters: {'num_topics': 15, 'dropout': 0.2200493978790428, 't_hidden_size': 50, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5829101244003742, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=30, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=30, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=30, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.32 .. Rec_loss: 2005.54 .. NELBO: 2006.86\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.78 .. Rec_loss: 1886.48 .. NELBO: 1887.26\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.7 .. Rec_loss: 1874.24 .. NELBO: 1874.94\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1723.41 .. NELBO: 1723.44\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1730.95 .. NELBO: 1730.97\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1734.24 .. NELBO: 1734.26\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1719.66 .. NELBO: 1719.67\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.8 .. NELBO: 1727.8\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.02 .. NELBO: 1731.02\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.31 .. NELBO: 1718.31\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.52 .. NELBO: 1726.52\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.69 .. NELBO: 1729.69\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.56 .. NELBO: 1717.56\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.81 .. NELBO: 1725.81\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.94 .. NELBO: 1728.94\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.01 .. NELBO: 1717.01\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.22 .. NELBO: 1725.22\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.35 .. NELBO: 1728.35\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.61 .. NELBO: 1716.61\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.94 .. NELBO: 1727.94\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.06 .. NELBO: 1716.06\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.37 .. NELBO: 1724.37\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.5 .. NELBO: 1727.5\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.77 .. NELBO: 1715.77\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.06 .. NELBO: 1724.06\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.23 .. NELBO: 1727.23\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.39 .. NELBO: 1715.39\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.76 .. NELBO: 1723.76\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.89 .. NELBO: 1726.89\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.25 .. NELBO: 1715.25\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.56 .. NELBO: 1723.56\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.69 .. NELBO: 1726.69\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.92 .. NELBO: 1714.92\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.33 .. NELBO: 1723.33\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.45 .. NELBO: 1726.45\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.76 .. NELBO: 1714.76\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.15 .. NELBO: 1723.15\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.25 .. NELBO: 1726.25\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.65 .. NELBO: 1714.65\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.01 .. NELBO: 1723.01\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.13 .. NELBO: 1726.13\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.41 .. NELBO: 1714.41\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.83 .. NELBO: 1722.83\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.92 .. NELBO: 1725.92\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.34 .. NELBO: 1714.34\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.7 .. NELBO: 1722.7\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.78 .. NELBO: 1725.78\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.29 .. NELBO: 1714.29\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.65 .. NELBO: 1722.65\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.77 .. NELBO: 1725.77\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.11 .. NELBO: 1714.11\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.61 .. NELBO: 1722.61\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.68 .. NELBO: 1725.68\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.11 .. NELBO: 1714.11\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.51 .. NELBO: 1722.51\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.57 .. NELBO: 1725.57\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.1 .. NELBO: 1714.1\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.43 .. NELBO: 1722.43\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.51 .. NELBO: 1725.51\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.91 .. NELBO: 1713.91\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.34 .. NELBO: 1722.34\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.42 .. NELBO: 1725.42\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.8 .. NELBO: 1713.8\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.27 .. NELBO: 1722.27\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.3 .. NELBO: 1725.3\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.84 .. NELBO: 1713.84\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.23 .. NELBO: 1722.23\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.29 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.88 .. NELBO: 1713.88\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.2 .. NELBO: 1722.2\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.28 .. NELBO: 1725.28\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.71 .. NELBO: 1713.71\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.15 .. NELBO: 1722.15\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.22 .. NELBO: 1725.22\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.06 .. NELBO: 1722.06\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.1 .. NELBO: 1725.1\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.98 .. NELBO: 1724.98\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.96 .. NELBO: 1724.96\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.35 .. NELBO: 1713.35\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.55 .. NELBO: 1713.55\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.41 .. NELBO: 1713.41\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.81 .. NELBO: 1721.81\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.33 .. NELBO: 1713.33\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.6 .. NELBO: 1724.6\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1712.87 .. NELBO: 1712.88\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.52 .. NELBO: 1724.52\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:07:40,972] Trial 27 finished with values: [-0.008114284618493513, 0.04] and parameters: {'num_topics': 30, 'dropout': 0.5829101244003742, 't_hidden_size': 100, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5405996787437367, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=35, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=35, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=35, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 2004.13 .. NELBO: 2004.5\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.45 .. Rec_loss: 1885.15 .. NELBO: 1885.6\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.45 .. Rec_loss: 1872.98 .. NELBO: 1873.43\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.24 .. NELBO: 1723.29\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.89 .. NELBO: 1730.94\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.18 .. NELBO: 1734.22\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1719.82 .. NELBO: 1719.85\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.95 .. NELBO: 1727.97\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.16 .. NELBO: 1731.18\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.41 .. NELBO: 1718.42\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.62 .. NELBO: 1726.63\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.78 .. NELBO: 1729.79\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.67 .. NELBO: 1717.68\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.82 .. NELBO: 1725.83\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1728.96 .. NELBO: 1728.97\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1717.0 .. NELBO: 1717.02\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1725.17 .. NELBO: 1725.19\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1728.31 .. NELBO: 1728.33\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1716.28 .. NELBO: 1716.37\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1724.47 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 1727.58 .. NELBO: 1727.71\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.35 .. Rec_loss: 1715.39 .. NELBO: 1715.74\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 1723.6 .. NELBO: 1723.94\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 1726.71 .. NELBO: 1727.07\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.58 .. Rec_loss: 1714.49 .. NELBO: 1715.07\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 1722.91 .. NELBO: 1723.42\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 1726.05 .. NELBO: 1726.57\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 1714.35 .. NELBO: 1714.98\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 1722.71 .. NELBO: 1723.28\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 1725.86 .. NELBO: 1726.41\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.56 .. Rec_loss: 1714.4 .. NELBO: 1714.96\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 1722.76 .. NELBO: 1723.3\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 1725.88 .. NELBO: 1726.43\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 1713.73 .. NELBO: 1714.35\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 1722.2 .. NELBO: 1722.83\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1725.33 .. NELBO: 1725.97\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.76 .. Rec_loss: 1713.09 .. NELBO: 1713.85\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.76 .. Rec_loss: 1721.55 .. NELBO: 1722.31\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1724.57 .. NELBO: 1725.38\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.93 .. Rec_loss: 1712.43 .. NELBO: 1713.36\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.93 .. Rec_loss: 1720.75 .. NELBO: 1721.68\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 1.0 .. Rec_loss: 1723.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.32 .. Rec_loss: 1711.21 .. NELBO: 1712.53\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.21 .. Rec_loss: 1719.99 .. NELBO: 1721.2\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 1.35 .. Rec_loss: 1722.6 .. NELBO: 1723.95\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.96 .. Rec_loss: 1709.8 .. NELBO: 1711.76\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.66 .. Rec_loss: 1718.79 .. NELBO: 1720.45\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.03 .. Rec_loss: 1720.99 .. NELBO: 1723.02\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.26 .. Rec_loss: 1708.5 .. NELBO: 1710.76\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.76 .. Rec_loss: 1718.04 .. NELBO: 1719.8\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.22 .. Rec_loss: 1719.86 .. NELBO: 1722.08\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1706.8 .. NELBO: 1709.29\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.75 .. Rec_loss: 1717.29 .. NELBO: 1719.04\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.1 .. Rec_loss: 1719.06 .. NELBO: 1721.16\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1705.81 .. NELBO: 1708.78\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.18 .. Rec_loss: 1716.38 .. NELBO: 1718.56\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1718.16 .. NELBO: 1720.56\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.12 .. Rec_loss: 1705.44 .. NELBO: 1708.56\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.31 .. Rec_loss: 1716.46 .. NELBO: 1718.77\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.45 .. Rec_loss: 1718.43 .. NELBO: 1720.88\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1706.89 .. NELBO: 1709.81\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.05 .. Rec_loss: 1717.75 .. NELBO: 1719.8\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1720.16 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1707.68 .. NELBO: 1710.35\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.12 .. Rec_loss: 1717.5 .. NELBO: 1719.62\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.39 .. Rec_loss: 1719.31 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1703.8 .. NELBO: 1706.78\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1714.83 .. NELBO: 1717.19\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1716.86 .. NELBO: 1719.4\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.19 .. Rec_loss: 1701.72 .. NELBO: 1704.91\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1713.38 .. NELBO: 1716.0\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1715.26 .. NELBO: 1718.12\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1700.93 .. NELBO: 1703.94\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1712.88 .. NELBO: 1715.32\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1714.77 .. NELBO: 1717.49\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1700.25 .. NELBO: 1703.26\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1712.34 .. NELBO: 1714.84\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1714.22 .. NELBO: 1717.05\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1699.74 .. NELBO: 1702.79\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1711.69 .. NELBO: 1714.38\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1713.49 .. NELBO: 1716.42\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.2 .. Rec_loss: 1699.38 .. NELBO: 1702.58\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1711.42 .. NELBO: 1714.15\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 3.08 .. Rec_loss: 1713.15 .. NELBO: 1716.23\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1699.09 .. NELBO: 1702.1\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1711.18 .. NELBO: 1713.79\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1712.74 .. NELBO: 1715.79\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 1699.3 .. NELBO: 1702.36\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1711.07 .. NELBO: 1713.75\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 1712.69 .. NELBO: 1715.75\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.19 .. Rec_loss: 1698.61 .. NELBO: 1701.8\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1710.51 .. NELBO: 1713.29\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.19 .. Rec_loss: 1712.25 .. NELBO: 1715.44\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1698.75 .. NELBO: 1701.88\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1710.67 .. NELBO: 1713.4\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 1712.77 .. NELBO: 1715.83\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.12 .. Rec_loss: 1701.2 .. NELBO: 1704.32\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1712.33 .. NELBO: 1715.01\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1714.29 .. NELBO: 1717.24\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1698.66 .. NELBO: 1701.94\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1710.34 .. NELBO: 1713.21\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 1711.84 .. NELBO: 1715.23\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.45 .. Rec_loss: 1697.3 .. NELBO: 1700.75\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1709.43 .. NELBO: 1712.34\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.46 .. Rec_loss: 1710.83 .. NELBO: 1714.29\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.51 .. Rec_loss: 1696.58 .. NELBO: 1700.09\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1708.94 .. NELBO: 1711.93\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.3 .. Rec_loss: 1710.53 .. NELBO: 1713.83\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.99 .. Rec_loss: 1695.69 .. NELBO: 1699.68\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.14 .. Rec_loss: 1708.71 .. NELBO: 1711.85\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.43 .. Rec_loss: 1710.47 .. NELBO: 1713.9\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.92 .. Rec_loss: 1695.59 .. NELBO: 1699.51\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1708.32 .. NELBO: 1711.53\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.61 .. Rec_loss: 1709.95 .. NELBO: 1713.56\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.85 .. Rec_loss: 1694.83 .. NELBO: 1698.68\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.27 .. Rec_loss: 1707.67 .. NELBO: 1710.94\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 3.64 .. Rec_loss: 1709.25 .. NELBO: 1712.89\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.32 .. Rec_loss: 1694.04 .. NELBO: 1698.36\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.54 .. Rec_loss: 1707.05 .. NELBO: 1710.59\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 3.83 .. Rec_loss: 1708.85 .. NELBO: 1712.68\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 1694.34 .. NELBO: 1698.53\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1707.05 .. NELBO: 1710.61\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 3.99 .. Rec_loss: 1708.71 .. NELBO: 1712.7\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.32 .. Rec_loss: 1693.85 .. NELBO: 1698.17\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.61 .. Rec_loss: 1706.7 .. NELBO: 1710.31\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4.02 .. Rec_loss: 1708.42 .. NELBO: 1712.44\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.55 .. Rec_loss: 1693.32 .. NELBO: 1697.87\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.82 .. Rec_loss: 1706.21 .. NELBO: 1710.03\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 4.15 .. Rec_loss: 1708.04 .. NELBO: 1712.19\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.8 .. Rec_loss: 1692.42 .. NELBO: 1697.22\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.0 .. Rec_loss: 1705.8 .. NELBO: 1709.8\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 4.27 .. Rec_loss: 1707.87 .. NELBO: 1712.14\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.94 .. Rec_loss: 1692.2 .. NELBO: 1697.14\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.05 .. Rec_loss: 1705.37 .. NELBO: 1709.42\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.39 .. Rec_loss: 1707.56 .. NELBO: 1711.95\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1692.14 .. NELBO: 1697.04\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.05 .. Rec_loss: 1705.28 .. NELBO: 1709.33\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1707.43 .. NELBO: 1711.85\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.98 .. Rec_loss: 1691.91 .. NELBO: 1696.89\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.2 .. Rec_loss: 1705.07 .. NELBO: 1709.27\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 1707.4 .. NELBO: 1711.86\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.19 .. Rec_loss: 1691.08 .. NELBO: 1696.27\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.33 .. Rec_loss: 1704.47 .. NELBO: 1708.8\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 4.59 .. Rec_loss: 1706.8 .. NELBO: 1711.39\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1692.08 .. NELBO: 1697.26\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.36 .. Rec_loss: 1705.01 .. NELBO: 1709.37\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 4.65 .. Rec_loss: 1707.09 .. NELBO: 1711.74\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.28 .. Rec_loss: 1692.45 .. NELBO: 1697.73\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.28 .. Rec_loss: 1705.52 .. NELBO: 1709.8\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1707.35 .. NELBO: 1712.19\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:09:05,514] Trial 28 finished with values: [-0.001033423423210282, 0.14285714285714285] and parameters: {'num_topics': 35, 'dropout': 0.5405996787437367, 't_hidden_size': 300, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.320319944785655, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=15, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=50, out_features=15, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=50, out_features=15, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.67 .. Rec_loss: 1997.13 .. NELBO: 1997.8\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.53 .. Rec_loss: 1881.23 .. NELBO: 1881.76\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 1869.42 .. NELBO: 1869.91\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1723.6 .. NELBO: 1723.7\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1731.09 .. NELBO: 1731.16\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1734.37 .. NELBO: 1734.44\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1719.83 .. NELBO: 1719.88\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1727.97 .. NELBO: 1728.01\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1731.17 .. NELBO: 1731.21\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1718.49 .. NELBO: 1718.51\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.68 .. NELBO: 1726.69\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.83 .. NELBO: 1729.84\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.71 .. NELBO: 1717.71\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.89 .. NELBO: 1725.89\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.02 .. NELBO: 1729.02\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.11 .. NELBO: 1717.11\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.28 .. NELBO: 1725.28\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.43 .. NELBO: 1728.43\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.56 .. NELBO: 1716.56\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.79 .. NELBO: 1724.79\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.95 .. NELBO: 1727.95\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.06 .. NELBO: 1716.06\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.53 .. NELBO: 1727.53\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.74 .. NELBO: 1715.74\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.03 .. NELBO: 1724.03\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.17 .. NELBO: 1727.17\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1715.38 .. NELBO: 1715.39\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.71 .. NELBO: 1723.72\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.85 .. NELBO: 1726.86\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1715.09 .. NELBO: 1715.1\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1723.44 .. NELBO: 1723.46\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1726.55 .. NELBO: 1726.58\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 1714.31 .. NELBO: 1714.54\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 1722.48 .. NELBO: 1722.78\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 1725.55 .. NELBO: 1725.89\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.76 .. Rec_loss: 1713.21 .. NELBO: 1713.97\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 1721.65 .. NELBO: 1722.34\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 1724.73 .. NELBO: 1725.42\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.89 .. Rec_loss: 1712.59 .. NELBO: 1713.48\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.84 .. Rec_loss: 1721.23 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.84 .. Rec_loss: 1724.28 .. NELBO: 1725.12\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.94 .. Rec_loss: 1712.2 .. NELBO: 1713.14\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.04 .. Rec_loss: 1720.56 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 1.09 .. Rec_loss: 1723.41 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.56 .. Rec_loss: 1710.55 .. NELBO: 1712.11\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.55 .. Rec_loss: 1719.52 .. NELBO: 1721.07\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 1.62 .. Rec_loss: 1722.07 .. NELBO: 1723.69\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.79 .. Rec_loss: 1708.9 .. NELBO: 1710.69\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.63 .. Rec_loss: 1718.32 .. NELBO: 1719.95\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 1.83 .. Rec_loss: 1720.54 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.08 .. Rec_loss: 1706.58 .. NELBO: 1708.66\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.84 .. Rec_loss: 1716.26 .. NELBO: 1718.1\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.05 .. Rec_loss: 1718.33 .. NELBO: 1720.38\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.52 .. Rec_loss: 1704.97 .. NELBO: 1707.49\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.1 .. Rec_loss: 1715.26 .. NELBO: 1717.36\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1717.26 .. NELBO: 1719.55\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1703.76 .. NELBO: 1706.5\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.23 .. Rec_loss: 1714.1 .. NELBO: 1716.33\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.42 .. Rec_loss: 1715.99 .. NELBO: 1718.41\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1702.84 .. NELBO: 1705.76\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.38 .. Rec_loss: 1713.35 .. NELBO: 1715.73\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1715.28 .. NELBO: 1717.87\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1702.06 .. NELBO: 1704.85\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.39 .. Rec_loss: 1712.51 .. NELBO: 1714.9\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1714.48 .. NELBO: 1717.06\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1701.11 .. NELBO: 1704.0\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 1711.89 .. NELBO: 1714.36\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1713.87 .. NELBO: 1716.53\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1700.73 .. NELBO: 1703.73\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1711.38 .. NELBO: 1713.87\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1713.42 .. NELBO: 1716.07\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1700.16 .. NELBO: 1703.12\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1710.94 .. NELBO: 1713.61\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1713.0 .. NELBO: 1715.79\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1700.31 .. NELBO: 1703.3\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1711.12 .. NELBO: 1713.84\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1712.96 .. NELBO: 1715.85\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1699.68 .. NELBO: 1702.65\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1710.6 .. NELBO: 1713.35\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1712.54 .. NELBO: 1715.45\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1699.23 .. NELBO: 1702.19\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.8 .. Rec_loss: 1710.38 .. NELBO: 1713.18\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1712.25 .. NELBO: 1715.23\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1698.95 .. NELBO: 1701.8\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1710.33 .. NELBO: 1713.09\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1712.28 .. NELBO: 1715.16\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.8 .. Rec_loss: 1699.8 .. NELBO: 1702.6\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.81 .. Rec_loss: 1710.9 .. NELBO: 1713.71\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1712.68 .. NELBO: 1715.67\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1699.44 .. NELBO: 1702.16\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1710.86 .. NELBO: 1713.59\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1712.43 .. NELBO: 1715.4\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1699.9 .. NELBO: 1702.66\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1710.91 .. NELBO: 1713.67\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1712.45 .. NELBO: 1715.46\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1699.35 .. NELBO: 1702.13\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1710.55 .. NELBO: 1713.28\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.03 .. Rec_loss: 1711.91 .. NELBO: 1714.94\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1698.69 .. NELBO: 1701.45\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1710.24 .. NELBO: 1712.87\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1711.58 .. NELBO: 1714.51\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1698.47 .. NELBO: 1701.31\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1710.02 .. NELBO: 1712.73\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1711.33 .. NELBO: 1714.38\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1697.86 .. NELBO: 1700.72\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1709.46 .. NELBO: 1712.07\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1710.78 .. NELBO: 1713.7\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1698.16 .. NELBO: 1701.01\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1709.61 .. NELBO: 1712.17\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1710.94 .. NELBO: 1713.87\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1697.89 .. NELBO: 1700.74\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1709.38 .. NELBO: 1712.0\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1710.79 .. NELBO: 1713.7\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1697.85 .. NELBO: 1700.78\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1709.26 .. NELBO: 1711.97\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1710.63 .. NELBO: 1713.6\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.07 .. Rec_loss: 1698.15 .. NELBO: 1701.22\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1709.41 .. NELBO: 1712.2\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 3.04 .. Rec_loss: 1710.67 .. NELBO: 1713.71\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1697.71 .. NELBO: 1700.6\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1709.22 .. NELBO: 1711.91\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1710.44 .. NELBO: 1713.37\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1698.43 .. NELBO: 1701.34\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1709.74 .. NELBO: 1712.51\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1710.89 .. NELBO: 1713.9\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1698.21 .. NELBO: 1701.23\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1709.63 .. NELBO: 1712.55\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1710.51 .. NELBO: 1713.64\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1698.57 .. NELBO: 1701.59\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1710.13 .. NELBO: 1713.08\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1711.11 .. NELBO: 1714.26\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.27 .. Rec_loss: 1699.66 .. NELBO: 1702.93\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1711.04 .. NELBO: 1714.19\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 3.29 .. Rec_loss: 1712.14 .. NELBO: 1715.43\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1699.84 .. NELBO: 1702.75\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1711.18 .. NELBO: 1714.19\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 3.2 .. Rec_loss: 1712.94 .. NELBO: 1716.14\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1697.29 .. NELBO: 1700.15\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1708.97 .. NELBO: 1711.95\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 3.29 .. Rec_loss: 1710.31 .. NELBO: 1713.6\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.33 .. Rec_loss: 1696.54 .. NELBO: 1699.87\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.2 .. Rec_loss: 1708.43 .. NELBO: 1711.63\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 3.56 .. Rec_loss: 1709.6 .. NELBO: 1713.16\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.32 .. Rec_loss: 1695.91 .. NELBO: 1699.23\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1707.51 .. NELBO: 1710.74\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 3.64 .. Rec_loss: 1708.67 .. NELBO: 1712.31\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.34 .. Rec_loss: 1695.76 .. NELBO: 1699.1\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.24 .. Rec_loss: 1707.1 .. NELBO: 1710.34\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 3.67 .. Rec_loss: 1708.32 .. NELBO: 1711.99\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:10:17,955] Trial 29 finished with values: [0.00045604524557746296, 0.25333333333333335] and parameters: {'num_topics': 15, 'dropout': 0.320319944785655, 't_hidden_size': 50, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.30614914824238526, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=41, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=41, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=41, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.01 .. Rec_loss: 2006.51 .. NELBO: 2007.52\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 1887.15 .. NELBO: 1887.69\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.49 .. Rec_loss: 1874.87 .. NELBO: 1875.36\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.35 .. NELBO: 1723.35\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1730.91 .. NELBO: 1730.91\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1734.22 .. NELBO: 1734.22\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.59 .. NELBO: 1719.59\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.77 .. NELBO: 1727.77\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1730.97 .. NELBO: 1730.97\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.24 .. NELBO: 1718.24\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.47 .. NELBO: 1726.47\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.64 .. NELBO: 1729.64\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.54 .. NELBO: 1717.54\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.76 .. NELBO: 1725.76\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.89 .. NELBO: 1728.89\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.96 .. NELBO: 1716.96\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.32 .. NELBO: 1728.32\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.46 .. NELBO: 1716.46\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.86 .. NELBO: 1727.86\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.96 .. NELBO: 1715.96\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.33 .. NELBO: 1724.33\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.47 .. NELBO: 1727.47\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.65 .. NELBO: 1715.65\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.05 .. NELBO: 1724.05\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.15 .. NELBO: 1727.15\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.62 .. NELBO: 1715.62\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.85 .. NELBO: 1723.85\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.99 .. NELBO: 1726.99\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.36 .. NELBO: 1715.36\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.63 .. NELBO: 1723.63\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.78 .. NELBO: 1726.78\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.98 .. NELBO: 1714.98\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.41 .. NELBO: 1723.41\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.5 .. NELBO: 1726.5\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.86 .. NELBO: 1714.86\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.17 .. NELBO: 1723.17\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.29 .. NELBO: 1726.29\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.65 .. NELBO: 1714.65\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.0 .. NELBO: 1723.0\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.11 .. NELBO: 1726.11\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.85 .. NELBO: 1722.85\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.96 .. NELBO: 1725.96\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.35 .. NELBO: 1714.35\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.74 .. NELBO: 1722.74\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.83 .. NELBO: 1725.83\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.27 .. NELBO: 1714.27\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.63 .. NELBO: 1722.63\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.73 .. NELBO: 1725.73\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.09 .. NELBO: 1714.09\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.62 .. NELBO: 1725.62\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.06 .. NELBO: 1714.06\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.44 .. NELBO: 1722.44\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.51 .. NELBO: 1725.51\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.98 .. NELBO: 1713.98\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.36 .. NELBO: 1722.36\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.44 .. NELBO: 1725.44\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.86 .. NELBO: 1713.86\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.34 .. NELBO: 1722.34\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.38 .. NELBO: 1725.38\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.96 .. NELBO: 1713.96\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.31 .. NELBO: 1722.31\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.39 .. NELBO: 1725.39\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.85 .. NELBO: 1713.85\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.26 .. NELBO: 1722.26\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.32 .. NELBO: 1725.32\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.74 .. NELBO: 1713.74\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.19 .. NELBO: 1722.19\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.24 .. NELBO: 1725.24\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.7 .. NELBO: 1713.7\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.16 .. NELBO: 1722.16\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.21 .. NELBO: 1725.21\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.71 .. NELBO: 1713.71\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.14 .. NELBO: 1725.14\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.86 .. NELBO: 1713.86\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.14 .. NELBO: 1722.14\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.23 .. NELBO: 1725.23\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.14 .. NELBO: 1722.14\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.6 .. NELBO: 1713.6\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.05 .. NELBO: 1722.05\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.56 .. NELBO: 1713.56\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.38 .. NELBO: 1713.38\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.77 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.14 .. NELBO: 1713.14\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:11:35,470] Trial 30 finished with values: [-0.008162655007304185, 0.026829268292682926] and parameters: {'num_topics': 41, 'dropout': 0.30614914824238526, 't_hidden_size': 300, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.08467938286173438, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=16, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=16, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=16, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.07 .. Rec_loss: 1998.05 .. NELBO: 1999.12\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 1881.68 .. NELBO: 1882.28\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.56 .. Rec_loss: 1869.83 .. NELBO: 1870.39\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1723.56 .. NELBO: 1723.62\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1731.01 .. NELBO: 1731.05\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.31 .. NELBO: 1734.35\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1719.75 .. NELBO: 1719.81\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1727.93 .. NELBO: 1727.98\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1731.15 .. NELBO: 1731.2\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1718.53 .. NELBO: 1718.58\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1726.69 .. NELBO: 1726.73\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1729.85 .. NELBO: 1729.89\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1717.76 .. NELBO: 1717.79\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1725.93 .. NELBO: 1725.96\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1729.07 .. NELBO: 1729.1\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1717.22 .. NELBO: 1717.24\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.34 .. NELBO: 1725.35\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1728.5 .. NELBO: 1728.51\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1716.63 .. NELBO: 1716.64\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.02 .. NELBO: 1728.02\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.11 .. NELBO: 1716.11\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.58 .. NELBO: 1727.58\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.73 .. NELBO: 1715.73\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.06 .. NELBO: 1724.06\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.21 .. NELBO: 1727.21\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.34 .. NELBO: 1715.34\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.74 .. NELBO: 1723.74\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.87 .. NELBO: 1726.87\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.26 .. NELBO: 1715.26\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.55 .. NELBO: 1723.55\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.69 .. NELBO: 1726.69\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.98 .. NELBO: 1714.98\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.38 .. NELBO: 1723.38\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.5 .. NELBO: 1726.5\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.75 .. NELBO: 1714.75\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.17 .. NELBO: 1723.17\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.26 .. NELBO: 1726.26\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.76 .. NELBO: 1714.76\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.06 .. NELBO: 1723.06\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.17 .. NELBO: 1726.17\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.57 .. NELBO: 1714.57\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.98 .. NELBO: 1722.98\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.07 .. NELBO: 1726.07\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.46 .. NELBO: 1714.46\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.83 .. NELBO: 1722.83\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.91 .. NELBO: 1725.91\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.39 .. NELBO: 1714.39\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.74 .. NELBO: 1722.74\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.85 .. NELBO: 1725.85\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.16 .. NELBO: 1714.16\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.65 .. NELBO: 1722.65\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.69 .. NELBO: 1725.69\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.21 .. NELBO: 1714.21\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.53 .. NELBO: 1722.53\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.6 .. NELBO: 1725.6\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.07 .. NELBO: 1714.07\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.48 .. NELBO: 1722.48\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.54 .. NELBO: 1725.54\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.97 .. NELBO: 1713.97\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.4 .. NELBO: 1722.4\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.46 .. NELBO: 1725.46\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.0 .. NELBO: 1714.0\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.34 .. NELBO: 1722.34\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.41 .. NELBO: 1725.41\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.93 .. NELBO: 1713.93\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.26 .. NELBO: 1722.26\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.33 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.72 .. NELBO: 1713.72\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.18 .. NELBO: 1722.18\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.24 .. NELBO: 1725.24\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.6 .. NELBO: 1713.6\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.64 .. NELBO: 1713.64\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.99 .. NELBO: 1721.99\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.97 .. NELBO: 1721.97\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.03 .. NELBO: 1725.03\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.97 .. NELBO: 1721.97\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.01 .. NELBO: 1725.01\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.99 .. NELBO: 1724.99\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.39 .. NELBO: 1713.39\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.33 .. NELBO: 1713.33\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.14 .. NELBO: 1713.14\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.0 .. NELBO: 1713.0\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.91 .. NELBO: 1712.91\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.0 .. NELBO: 1713.0\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.91 .. NELBO: 1712.91\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:12:45,368] Trial 31 finished with values: [-0.00877536518409483, 0.06875] and parameters: {'num_topics': 16, 'dropout': 0.08467938286173438, 't_hidden_size': 200, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.17684923924858745, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=28, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=28, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.93 .. Rec_loss: 2003.99 .. NELBO: 2004.92\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 1885.34 .. NELBO: 1885.91\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 1873.14 .. NELBO: 1873.66\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1723.42 .. NELBO: 1723.48\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1730.99 .. NELBO: 1731.03\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.27 .. NELBO: 1734.31\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.74 .. NELBO: 1719.76\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.92 .. NELBO: 1727.94\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.11 .. NELBO: 1731.13\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.36 .. NELBO: 1718.37\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.59 .. NELBO: 1726.6\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.75 .. NELBO: 1729.76\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.66 .. NELBO: 1717.66\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.84 .. NELBO: 1725.84\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.98 .. NELBO: 1728.98\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.03 .. NELBO: 1717.03\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.22 .. NELBO: 1725.22\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.37 .. NELBO: 1728.37\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.51 .. NELBO: 1716.51\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.91 .. NELBO: 1727.91\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.02 .. NELBO: 1716.02\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.49 .. NELBO: 1727.49\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.72 .. NELBO: 1715.72\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.01 .. NELBO: 1724.01\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.15 .. NELBO: 1727.15\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.38 .. NELBO: 1715.38\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.71 .. NELBO: 1723.71\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.85 .. NELBO: 1726.85\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.09 .. NELBO: 1715.09\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.44 .. NELBO: 1723.44\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.58 .. NELBO: 1726.58\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.88 .. NELBO: 1714.88\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.26 .. NELBO: 1723.26\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.39 .. NELBO: 1726.39\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.71 .. NELBO: 1714.71\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.11 .. NELBO: 1723.11\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.2 .. NELBO: 1726.2\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.7 .. NELBO: 1714.7\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.01 .. NELBO: 1723.01\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.13 .. NELBO: 1726.13\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.52 .. NELBO: 1714.52\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.92 .. NELBO: 1722.92\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.01 .. NELBO: 1726.01\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.37 .. NELBO: 1714.37\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.73 .. NELBO: 1722.73\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.82 .. NELBO: 1725.82\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.26 .. NELBO: 1714.26\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.64 .. NELBO: 1722.64\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.72 .. NELBO: 1725.72\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.18 .. NELBO: 1714.18\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.6 .. NELBO: 1722.6\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.69 .. NELBO: 1725.69\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.1 .. NELBO: 1714.1\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.55 .. NELBO: 1722.55\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.61 .. NELBO: 1725.61\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.03 .. NELBO: 1714.03\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.47 .. NELBO: 1722.47\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.51 .. NELBO: 1725.51\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.99 .. NELBO: 1713.99\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.34 .. NELBO: 1722.34\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.4 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.87 .. NELBO: 1713.87\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.25 .. NELBO: 1722.25\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.34 .. NELBO: 1725.34\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.78 .. NELBO: 1713.78\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.22 .. NELBO: 1722.22\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.74 .. NELBO: 1713.74\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.17 .. NELBO: 1722.17\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.23 .. NELBO: 1725.23\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.72 .. NELBO: 1713.72\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.12 .. NELBO: 1722.12\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.6 .. NELBO: 1713.6\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.07 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.42 .. NELBO: 1713.42\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.32 .. NELBO: 1713.32\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.77 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.38 .. NELBO: 1713.38\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.82 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.68 .. NELBO: 1721.68\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.83 .. NELBO: 1712.83\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.35 .. NELBO: 1721.35\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.77 .. NELBO: 1712.77\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:13:57,718] Trial 32 finished with values: [-0.008214751497585479, 0.04285714285714286] and parameters: {'num_topics': 28, 'dropout': 0.17684923924858745, 't_hidden_size': 100, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5381536241250484, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=26, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=26, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=26, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.1 .. Rec_loss: 2002.3 .. NELBO: 2003.4\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1884.69 .. NELBO: 1885.33\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 1872.64 .. NELBO: 1873.21\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1723.39 .. NELBO: 1723.41\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1730.96 .. NELBO: 1730.97\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1734.26 .. NELBO: 1734.27\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1719.64 .. NELBO: 1719.65\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.83 .. NELBO: 1727.84\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.03 .. NELBO: 1731.04\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.33 .. NELBO: 1718.33\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.54 .. NELBO: 1726.54\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.7 .. NELBO: 1729.7\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.61 .. NELBO: 1717.61\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.79 .. NELBO: 1725.79\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.93 .. NELBO: 1728.93\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.0 .. NELBO: 1717.0\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.21 .. NELBO: 1725.21\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.36 .. NELBO: 1728.36\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.47 .. NELBO: 1716.47\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.87 .. NELBO: 1727.87\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.01 .. NELBO: 1716.01\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.31 .. NELBO: 1724.31\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.45 .. NELBO: 1727.45\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.65 .. NELBO: 1715.65\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.97 .. NELBO: 1723.97\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.13 .. NELBO: 1727.13\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.33 .. NELBO: 1715.33\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.69 .. NELBO: 1723.69\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.82 .. NELBO: 1726.82\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.17 .. NELBO: 1715.17\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.49 .. NELBO: 1723.49\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.63 .. NELBO: 1726.63\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.96 .. NELBO: 1714.96\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.32 .. NELBO: 1723.32\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.43 .. NELBO: 1726.43\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.83 .. NELBO: 1714.83\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.14 .. NELBO: 1723.14\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.27 .. NELBO: 1726.27\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.63 .. NELBO: 1714.63\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.01 .. NELBO: 1723.01\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.13 .. NELBO: 1726.13\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.43 .. NELBO: 1714.43\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.9 .. NELBO: 1722.9\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.98 .. NELBO: 1725.98\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.77 .. NELBO: 1722.77\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.85 .. NELBO: 1725.85\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.23 .. NELBO: 1714.23\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.61 .. NELBO: 1722.61\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.69 .. NELBO: 1725.69\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.08 .. NELBO: 1714.08\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.49 .. NELBO: 1722.49\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.57 .. NELBO: 1725.57\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.05 .. NELBO: 1714.05\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.42 .. NELBO: 1722.42\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.5 .. NELBO: 1725.5\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.04 .. NELBO: 1714.04\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.38 .. NELBO: 1722.38\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.48 .. NELBO: 1725.48\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.92 .. NELBO: 1713.92\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.36 .. NELBO: 1722.36\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.42 .. NELBO: 1725.42\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.84 .. NELBO: 1713.84\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.28 .. NELBO: 1722.28\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.32 .. NELBO: 1725.32\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.8 .. NELBO: 1713.8\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.17 .. NELBO: 1722.17\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.23 .. NELBO: 1725.23\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.72 .. NELBO: 1713.72\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.16 .. NELBO: 1725.16\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.61 .. NELBO: 1713.61\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.06 .. NELBO: 1722.06\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.1 .. NELBO: 1725.1\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.0 .. NELBO: 1722.0\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.04 .. NELBO: 1725.04\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.61 .. NELBO: 1713.61\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.99 .. NELBO: 1721.99\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.51 .. NELBO: 1713.51\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.01 .. NELBO: 1725.01\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.48 .. NELBO: 1713.48\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.0 .. NELBO: 1725.0\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.41 .. NELBO: 1713.41\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.62 .. NELBO: 1713.62\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.92 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.0 .. NELBO: 1725.0\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.92 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.75 .. NELBO: 1721.75\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.33 .. NELBO: 1713.33\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.68 .. NELBO: 1721.68\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:15:12,528] Trial 33 finished with values: [-0.007597076811601464, 0.04230769230769231] and parameters: {'num_topics': 26, 'dropout': 0.5381536241250484, 't_hidden_size': 200, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.027477818419284695, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=50, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=50, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.71 .. Rec_loss: 2006.41 .. NELBO: 2007.12\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.44 .. Rec_loss: 1886.81 .. NELBO: 1887.25\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.44 .. Rec_loss: 1874.46 .. NELBO: 1874.9\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1723.43 .. NELBO: 1723.51\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.98 .. NELBO: 1731.03\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.27 .. NELBO: 1734.31\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1719.72 .. NELBO: 1719.73\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.88 .. NELBO: 1727.89\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.08 .. NELBO: 1731.09\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.36 .. NELBO: 1718.36\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.58 .. NELBO: 1726.58\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.73 .. NELBO: 1729.73\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.64 .. NELBO: 1717.64\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.82 .. NELBO: 1725.82\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.96 .. NELBO: 1728.96\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.02 .. NELBO: 1717.02\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.22 .. NELBO: 1725.22\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.36 .. NELBO: 1728.36\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.46 .. NELBO: 1716.46\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.86 .. NELBO: 1727.86\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.01 .. NELBO: 1716.01\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.46 .. NELBO: 1727.46\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.64 .. NELBO: 1715.64\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.98 .. NELBO: 1723.98\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.13 .. NELBO: 1727.13\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.34 .. NELBO: 1715.34\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.72 .. NELBO: 1723.72\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.85 .. NELBO: 1726.85\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.19 .. NELBO: 1715.19\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.51 .. NELBO: 1723.51\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.65 .. NELBO: 1726.65\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.91 .. NELBO: 1714.91\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.27 .. NELBO: 1723.27\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.39 .. NELBO: 1726.39\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.73 .. NELBO: 1714.73\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.09 .. NELBO: 1723.09\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.2 .. NELBO: 1726.2\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.55 .. NELBO: 1714.55\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.94 .. NELBO: 1722.94\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.04 .. NELBO: 1726.04\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.81 .. NELBO: 1722.81\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.9 .. NELBO: 1725.9\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.33 .. NELBO: 1714.33\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.7 .. NELBO: 1722.7\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.81 .. NELBO: 1725.81\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.22 .. NELBO: 1714.22\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.65 .. NELBO: 1722.65\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.73 .. NELBO: 1725.73\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.24 .. NELBO: 1714.24\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.64 .. NELBO: 1722.64\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.71 .. NELBO: 1725.71\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.2 .. NELBO: 1714.2\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.62 .. NELBO: 1725.62\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.09 .. NELBO: 1714.09\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.45 .. NELBO: 1722.45\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.55 .. NELBO: 1725.55\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.92 .. NELBO: 1713.92\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.41 .. NELBO: 1722.41\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.46 .. NELBO: 1725.46\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.94 .. NELBO: 1713.94\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.32 .. NELBO: 1722.32\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.38 .. NELBO: 1725.38\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.76 .. NELBO: 1713.76\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.18 .. NELBO: 1722.18\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.24 .. NELBO: 1725.24\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.7 .. NELBO: 1713.7\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.63 .. NELBO: 1713.63\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.08 .. NELBO: 1722.08\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.13 .. NELBO: 1725.13\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.64 .. NELBO: 1713.64\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.07 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.63 .. NELBO: 1713.63\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.0 .. NELBO: 1722.0\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.09 .. NELBO: 1725.09\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.41 .. NELBO: 1713.41\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.98 .. NELBO: 1724.98\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.82 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.89 .. NELBO: 1724.89\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.77 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.7 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.68 .. NELBO: 1721.68\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.35 .. NELBO: 1721.35\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:16:28,883] Trial 34 finished with values: [-0.008768270912557233, 0.024] and parameters: {'num_topics': 50, 'dropout': 0.027477818419284695, 't_hidden_size': 100, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5279578289947756, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=38, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=50, out_features=38, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=50, out_features=38, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 2007.44 .. NELBO: 2007.72\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.23 .. Rec_loss: 1887.45 .. NELBO: 1887.68\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 1875.08 .. NELBO: 1875.33\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1723.45 .. NELBO: 1723.51\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1731.02 .. NELBO: 1731.05\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1734.31 .. NELBO: 1734.34\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1719.67 .. NELBO: 1719.68\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.85 .. NELBO: 1727.86\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.06 .. NELBO: 1731.07\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.35 .. NELBO: 1718.35\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.56 .. NELBO: 1726.56\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.72 .. NELBO: 1729.72\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.62 .. NELBO: 1717.62\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.8 .. NELBO: 1725.8\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.94 .. NELBO: 1728.94\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.01 .. NELBO: 1717.01\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.21 .. NELBO: 1725.21\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.35 .. NELBO: 1728.35\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.46 .. NELBO: 1716.46\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.86 .. NELBO: 1727.86\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.01 .. NELBO: 1716.01\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.32 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.46 .. NELBO: 1727.46\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.64 .. NELBO: 1715.64\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.98 .. NELBO: 1723.98\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.12 .. NELBO: 1727.12\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.39 .. NELBO: 1715.39\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.7 .. NELBO: 1723.7\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.84 .. NELBO: 1726.84\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.16 .. NELBO: 1715.16\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.5 .. NELBO: 1723.5\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.65 .. NELBO: 1726.65\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.9 .. NELBO: 1714.9\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.3 .. NELBO: 1723.3\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.41 .. NELBO: 1726.41\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.73 .. NELBO: 1714.73\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.09 .. NELBO: 1723.09\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.21 .. NELBO: 1726.21\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.55 .. NELBO: 1714.55\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.94 .. NELBO: 1722.94\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.04 .. NELBO: 1726.04\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.83 .. NELBO: 1722.83\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.92 .. NELBO: 1725.92\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.3 .. NELBO: 1714.3\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.71 .. NELBO: 1722.71\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.79 .. NELBO: 1725.79\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.22 .. NELBO: 1714.22\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.61 .. NELBO: 1722.61\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.7 .. NELBO: 1725.7\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.13 .. NELBO: 1714.13\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.53 .. NELBO: 1722.53\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.6 .. NELBO: 1725.6\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.06 .. NELBO: 1714.06\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.45 .. NELBO: 1722.45\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.52 .. NELBO: 1725.52\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.02 .. NELBO: 1714.02\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.41 .. NELBO: 1722.41\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.47 .. NELBO: 1725.47\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.0 .. NELBO: 1714.0\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.39 .. NELBO: 1722.39\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.44 .. NELBO: 1725.44\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.03 .. NELBO: 1714.03\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.37 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.43 .. NELBO: 1725.43\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.87 .. NELBO: 1713.87\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.27 .. NELBO: 1722.27\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.33 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.7 .. NELBO: 1713.7\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.13 .. NELBO: 1722.13\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.64 .. NELBO: 1713.64\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.04 .. NELBO: 1722.04\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.08 .. NELBO: 1725.08\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.55 .. NELBO: 1713.55\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.02 .. NELBO: 1725.02\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.51 .. NELBO: 1713.51\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.92 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.98 .. NELBO: 1724.98\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.98 .. NELBO: 1724.98\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.48 .. NELBO: 1713.48\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.98 .. NELBO: 1724.98\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.38 .. NELBO: 1713.38\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.91 .. NELBO: 1724.91\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.5 .. NELBO: 1713.5\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.85 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.39 .. NELBO: 1713.39\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.7 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.82 .. NELBO: 1712.82\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.46 .. NELBO: 1724.46\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.39 .. NELBO: 1721.39\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.0 .. NELBO: 1713.0\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:17:43,919] Trial 35 finished with values: [-0.00853941538966046, 0.02894736842105263] and parameters: {'num_topics': 38, 'dropout': 0.5279578289947756, 't_hidden_size': 50, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.09441093479072561, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=41, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=41, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=41, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.1 .. Rec_loss: 2010.71 .. NELBO: 2011.81\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 1889.4 .. NELBO: 1890.01\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.55 .. Rec_loss: 1876.88 .. NELBO: 1877.43\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.58 .. NELBO: 1723.59\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.04 .. NELBO: 1731.05\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1734.34 .. NELBO: 1734.35\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.65 .. NELBO: 1719.65\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.81 .. NELBO: 1727.81\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.01 .. NELBO: 1731.01\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.27 .. NELBO: 1718.27\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.49 .. NELBO: 1726.49\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.65 .. NELBO: 1729.65\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.56 .. NELBO: 1717.56\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.77 .. NELBO: 1725.77\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.91 .. NELBO: 1728.91\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.95 .. NELBO: 1716.95\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.31 .. NELBO: 1728.31\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.53 .. NELBO: 1716.53\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.88 .. NELBO: 1727.88\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.06 .. NELBO: 1716.06\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.51 .. NELBO: 1727.51\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.62 .. NELBO: 1715.62\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.97 .. NELBO: 1723.97\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.1 .. NELBO: 1727.1\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.36 .. NELBO: 1715.36\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.68 .. NELBO: 1723.68\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.83 .. NELBO: 1726.83\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.13 .. NELBO: 1715.13\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.47 .. NELBO: 1723.47\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.6 .. NELBO: 1726.6\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.84 .. NELBO: 1714.84\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.23 .. NELBO: 1723.23\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.35 .. NELBO: 1726.35\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.68 .. NELBO: 1714.68\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.06 .. NELBO: 1723.06\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.17 .. NELBO: 1726.17\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.52 .. NELBO: 1714.52\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.89 .. NELBO: 1722.89\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.01 .. NELBO: 1726.01\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.36 .. NELBO: 1714.36\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.76 .. NELBO: 1722.76\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.86 .. NELBO: 1725.86\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.33 .. NELBO: 1714.33\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.7 .. NELBO: 1722.7\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.79 .. NELBO: 1725.79\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.23 .. NELBO: 1714.23\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.6 .. NELBO: 1722.6\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.71 .. NELBO: 1725.71\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.1 .. NELBO: 1714.1\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.58 .. NELBO: 1722.58\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.64 .. NELBO: 1725.64\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.19 .. NELBO: 1714.19\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.53 .. NELBO: 1722.53\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.6 .. NELBO: 1725.6\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.14 .. NELBO: 1714.14\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.47 .. NELBO: 1722.47\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.56 .. NELBO: 1725.56\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.91 .. NELBO: 1713.91\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.37 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.43 .. NELBO: 1725.43\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.93 .. NELBO: 1713.93\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.3 .. NELBO: 1722.3\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.36 .. NELBO: 1725.36\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.76 .. NELBO: 1713.76\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.15 .. NELBO: 1722.15\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.21 .. NELBO: 1725.21\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.62 .. NELBO: 1713.62\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.05 .. NELBO: 1722.05\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.55 .. NELBO: 1713.55\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.97 .. NELBO: 1721.97\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.03 .. NELBO: 1725.03\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.92 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.54 .. NELBO: 1713.54\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.98 .. NELBO: 1724.98\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.35 .. NELBO: 1713.35\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.51 .. NELBO: 1713.51\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.89 .. NELBO: 1724.89\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.77 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.75 .. NELBO: 1721.75\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.7 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.88 .. NELBO: 1712.88\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:19:02,621] Trial 36 finished with values: [-0.008188369416399083, 0.02926829268292683] and parameters: {'num_topics': 41, 'dropout': 0.09441093479072561, 't_hidden_size': 300, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.1838537319428485, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=20, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=20, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=20, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.83 .. Rec_loss: 2002.53 .. NELBO: 2003.36\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 1884.4 .. NELBO: 1884.86\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.42 .. Rec_loss: 1872.25 .. NELBO: 1872.67\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1723.61 .. NELBO: 1723.67\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1731.07 .. NELBO: 1731.12\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.32 .. NELBO: 1734.37\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1719.82 .. NELBO: 1719.86\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1727.97 .. NELBO: 1728.01\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1731.18 .. NELBO: 1731.21\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1718.41 .. NELBO: 1718.43\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1726.62 .. NELBO: 1726.64\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1729.78 .. NELBO: 1729.8\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.69 .. NELBO: 1717.7\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.87 .. NELBO: 1725.87\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.0 .. NELBO: 1729.0\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.09 .. NELBO: 1717.09\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.4 .. NELBO: 1728.4\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.52 .. NELBO: 1716.52\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.92 .. NELBO: 1727.92\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.05 .. NELBO: 1716.05\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.52 .. NELBO: 1727.52\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.82 .. NELBO: 1715.82\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.08 .. NELBO: 1724.08\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.25 .. NELBO: 1727.25\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.41 .. NELBO: 1715.41\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.79 .. NELBO: 1723.79\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.92 .. NELBO: 1726.92\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.22 .. NELBO: 1715.22\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.55 .. NELBO: 1723.55\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.68 .. NELBO: 1726.68\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.91 .. NELBO: 1714.91\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.35 .. NELBO: 1723.35\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.46 .. NELBO: 1726.46\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.8 .. NELBO: 1714.8\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.17 .. NELBO: 1723.17\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.27 .. NELBO: 1726.27\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.83 .. NELBO: 1714.83\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.1 .. NELBO: 1723.1\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.22 .. NELBO: 1726.22\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.5 .. NELBO: 1714.5\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.94 .. NELBO: 1722.94\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.03 .. NELBO: 1726.03\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.4 .. NELBO: 1714.4\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.77 .. NELBO: 1722.77\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.85 .. NELBO: 1725.85\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.31 .. NELBO: 1714.31\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.68 .. NELBO: 1722.68\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.77 .. NELBO: 1725.77\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.12 .. NELBO: 1714.12\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.61 .. NELBO: 1725.61\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.11 .. NELBO: 1714.11\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.47 .. NELBO: 1722.47\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.54 .. NELBO: 1725.54\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.0 .. NELBO: 1714.0\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.39 .. NELBO: 1722.39\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.48 .. NELBO: 1725.48\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.88 .. NELBO: 1713.88\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.35 .. NELBO: 1722.35\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.4 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.92 .. NELBO: 1713.92\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.29 .. NELBO: 1722.29\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.37 .. NELBO: 1725.37\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.79 .. NELBO: 1713.79\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.27 .. NELBO: 1722.27\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.32 .. NELBO: 1725.32\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.82 .. NELBO: 1713.82\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.21 .. NELBO: 1722.21\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.75 .. NELBO: 1713.75\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.13 .. NELBO: 1722.13\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.64 .. NELBO: 1713.64\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.16 .. NELBO: 1725.16\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.05 .. NELBO: 1722.05\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.08 .. NELBO: 1725.08\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.69 .. NELBO: 1713.69\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.04 .. NELBO: 1722.04\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.07 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.68 .. NELBO: 1713.68\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.99 .. NELBO: 1721.99\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.07 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.59 .. NELBO: 1713.59\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.41 .. NELBO: 1713.41\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.58 .. NELBO: 1721.58\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.6 .. NELBO: 1724.6\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.16 .. NELBO: 1713.16\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.45 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:20:12,955] Trial 37 finished with values: [-0.007972072099875022, 0.05] and parameters: {'num_topics': 20, 'dropout': 0.1838537319428485, 't_hidden_size': 200, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.47553347551975217, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=21, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=21, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=21, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 2007.49 .. NELBO: 2008.26\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.43 .. Rec_loss: 1887.56 .. NELBO: 1887.99\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.39 .. Rec_loss: 1875.21 .. NELBO: 1875.6\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.49 .. NELBO: 1723.5\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1730.96 .. NELBO: 1730.97\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1734.28 .. NELBO: 1734.29\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1719.67 .. NELBO: 1719.68\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.86 .. NELBO: 1727.86\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.05 .. NELBO: 1731.05\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.28 .. NELBO: 1718.28\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.51 .. NELBO: 1726.51\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.67 .. NELBO: 1729.67\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.57 .. NELBO: 1717.57\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.79 .. NELBO: 1725.79\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.93 .. NELBO: 1728.93\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.97 .. NELBO: 1716.97\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.33 .. NELBO: 1728.33\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.53 .. NELBO: 1716.53\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.88 .. NELBO: 1727.88\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.13 .. NELBO: 1716.13\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.59 .. NELBO: 1727.59\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.64 .. NELBO: 1715.64\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.02 .. NELBO: 1724.02\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.15 .. NELBO: 1727.15\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.41 .. NELBO: 1715.41\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.72 .. NELBO: 1723.72\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.84 .. NELBO: 1726.84\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.33 .. NELBO: 1715.33\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.58 .. NELBO: 1723.58\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.74 .. NELBO: 1726.74\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.04 .. NELBO: 1715.04\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.41 .. NELBO: 1723.41\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.55 .. NELBO: 1726.55\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.73 .. NELBO: 1714.73\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.15 .. NELBO: 1723.15\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.26 .. NELBO: 1726.26\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.59 .. NELBO: 1714.59\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.97 .. NELBO: 1722.97\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.08 .. NELBO: 1726.08\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.83 .. NELBO: 1722.83\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.93 .. NELBO: 1725.93\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.33 .. NELBO: 1714.33\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.71 .. NELBO: 1722.71\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.8 .. NELBO: 1725.8\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.23 .. NELBO: 1714.23\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.58 .. NELBO: 1722.58\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.68 .. NELBO: 1725.68\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.08 .. NELBO: 1714.08\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.49 .. NELBO: 1722.49\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.56 .. NELBO: 1725.56\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.07 .. NELBO: 1714.07\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.45 .. NELBO: 1722.45\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.53 .. NELBO: 1725.53\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.98 .. NELBO: 1713.98\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.4 .. NELBO: 1722.4\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.47 .. NELBO: 1725.47\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.9 .. NELBO: 1713.9\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.35 .. NELBO: 1722.35\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.42 .. NELBO: 1725.42\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.9 .. NELBO: 1713.9\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.35 .. NELBO: 1722.35\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.41 .. NELBO: 1725.41\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.8 .. NELBO: 1713.8\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.24 .. NELBO: 1722.24\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.29 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.86 .. NELBO: 1713.86\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.18 .. NELBO: 1722.18\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.62 .. NELBO: 1713.62\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.12 .. NELBO: 1722.12\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.64 .. NELBO: 1713.64\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.06 .. NELBO: 1722.06\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.62 .. NELBO: 1713.62\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.02 .. NELBO: 1722.02\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.08 .. NELBO: 1725.08\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.55 .. NELBO: 1713.55\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.61 .. NELBO: 1713.61\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.02 .. NELBO: 1722.02\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.98 .. NELBO: 1721.98\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.93 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.89 .. NELBO: 1724.89\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.14 .. NELBO: 1713.14\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.6 .. NELBO: 1724.6\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.65 .. NELBO: 1724.65\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:21:26,332] Trial 38 finished with values: [-0.008742037993155257, 0.05238095238095238] and parameters: {'num_topics': 21, 'dropout': 0.47553347551975217, 't_hidden_size': 300, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5283015047924261, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=43, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=43, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=43, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.33 .. Rec_loss: 2009.18 .. NELBO: 2010.51\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.72 .. Rec_loss: 1888.6 .. NELBO: 1889.32\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1876.19 .. NELBO: 1876.83\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1723.48 .. NELBO: 1723.49\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1731.0 .. NELBO: 1731.0\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1734.3 .. NELBO: 1734.3\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1719.61 .. NELBO: 1719.61\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.79 .. NELBO: 1727.79\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1730.99 .. NELBO: 1730.99\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.24 .. NELBO: 1718.24\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.47 .. NELBO: 1726.47\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.63 .. NELBO: 1729.63\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.55 .. NELBO: 1717.55\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.76 .. NELBO: 1725.76\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.9 .. NELBO: 1728.9\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.95 .. NELBO: 1716.95\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.31 .. NELBO: 1728.31\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.51 .. NELBO: 1716.51\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.86 .. NELBO: 1727.86\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.06 .. NELBO: 1716.06\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.34 .. NELBO: 1724.34\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.49 .. NELBO: 1727.49\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.61 .. NELBO: 1715.61\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.0 .. NELBO: 1724.0\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.14 .. NELBO: 1727.14\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.3 .. NELBO: 1715.3\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.71 .. NELBO: 1723.71\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.81 .. NELBO: 1726.81\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.33 .. NELBO: 1715.33\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.57 .. NELBO: 1723.57\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.72 .. NELBO: 1726.72\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.94 .. NELBO: 1714.94\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.33 .. NELBO: 1723.33\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.44 .. NELBO: 1726.44\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.7 .. NELBO: 1714.7\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.08 .. NELBO: 1723.08\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.2 .. NELBO: 1726.2\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.52 .. NELBO: 1714.52\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.9 .. NELBO: 1722.9\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.01 .. NELBO: 1726.01\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.49 .. NELBO: 1714.49\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.82 .. NELBO: 1722.82\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.95 .. NELBO: 1725.95\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.35 .. NELBO: 1714.35\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.8 .. NELBO: 1722.8\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.89 .. NELBO: 1725.89\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.24 .. NELBO: 1714.24\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.65 .. NELBO: 1722.65\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.73 .. NELBO: 1725.73\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.23 .. NELBO: 1714.23\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.6 .. NELBO: 1722.6\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.68 .. NELBO: 1725.68\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.19 .. NELBO: 1714.19\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.6 .. NELBO: 1722.6\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.67 .. NELBO: 1725.67\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.21 .. NELBO: 1714.21\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.56 .. NELBO: 1722.56\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.63 .. NELBO: 1725.63\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.17 .. NELBO: 1714.17\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.55 .. NELBO: 1722.55\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.63 .. NELBO: 1725.63\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.88 .. NELBO: 1713.88\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.37 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.4 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.88 .. NELBO: 1713.88\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.22 .. NELBO: 1722.22\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.28 .. NELBO: 1725.28\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.67 .. NELBO: 1713.67\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.11 .. NELBO: 1722.11\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.16 .. NELBO: 1725.16\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.62 .. NELBO: 1713.62\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.1 .. NELBO: 1725.1\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.74 .. NELBO: 1713.74\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.14 .. NELBO: 1725.14\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.6 .. NELBO: 1713.6\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.01 .. NELBO: 1722.01\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.08 .. NELBO: 1725.08\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.41 .. NELBO: 1713.41\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.5 .. NELBO: 1713.5\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.85 .. NELBO: 1721.85\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.79 .. NELBO: 1724.79\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.38 .. NELBO: 1713.38\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.7 .. NELBO: 1721.7\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.14 .. NELBO: 1713.14\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.44 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.81 .. NELBO: 1712.81\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.89 .. NELBO: 1712.89\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.4 .. NELBO: 1721.4\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.42 .. NELBO: 1721.42\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.48 .. NELBO: 1724.48\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:22:42,806] Trial 39 finished with values: [-0.008414693526731562, 0.02558139534883721] and parameters: {'num_topics': 43, 'dropout': 0.5283015047924261, 't_hidden_size': 200, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.14693900958923492, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=44, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=50, out_features=44, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=50, out_features=44, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.25 .. Rec_loss: 2006.1 .. NELBO: 2006.35\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.22 .. Rec_loss: 1886.75 .. NELBO: 1886.97\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 1874.43 .. NELBO: 1874.67\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.29 .. NELBO: 1723.34\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1730.93 .. NELBO: 1730.96\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1734.21 .. NELBO: 1734.24\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1719.62 .. NELBO: 1719.63\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.83 .. NELBO: 1727.84\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.02 .. NELBO: 1731.03\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.3 .. NELBO: 1718.3\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.52 .. NELBO: 1726.52\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.68 .. NELBO: 1729.68\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.58 .. NELBO: 1717.58\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.76 .. NELBO: 1725.76\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.91 .. NELBO: 1728.91\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.96 .. NELBO: 1716.96\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.32 .. NELBO: 1728.32\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.44 .. NELBO: 1716.44\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.84 .. NELBO: 1727.84\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.01 .. NELBO: 1716.01\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.28 .. NELBO: 1724.28\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.44 .. NELBO: 1727.44\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.6 .. NELBO: 1715.6\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.98 .. NELBO: 1723.98\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.15 .. NELBO: 1727.15\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.31 .. NELBO: 1715.31\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.78 .. NELBO: 1723.78\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.9 .. NELBO: 1726.9\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.27 .. NELBO: 1715.27\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.62 .. NELBO: 1723.62\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.75 .. NELBO: 1726.75\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.0 .. NELBO: 1715.0\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.37 .. NELBO: 1723.37\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.48 .. NELBO: 1726.48\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.76 .. NELBO: 1714.76\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.1 .. NELBO: 1723.1\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.23 .. NELBO: 1726.23\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.55 .. NELBO: 1714.55\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.99 .. NELBO: 1722.99\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.09 .. NELBO: 1726.09\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.49 .. NELBO: 1714.49\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.94 .. NELBO: 1722.94\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.0 .. NELBO: 1726.0\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.44 .. NELBO: 1714.44\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.76 .. NELBO: 1722.76\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.86 .. NELBO: 1725.86\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.22 .. NELBO: 1714.22\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.61 .. NELBO: 1722.61\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.69 .. NELBO: 1725.69\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.11 .. NELBO: 1714.11\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.51 .. NELBO: 1722.51\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.57 .. NELBO: 1725.57\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.12 .. NELBO: 1714.12\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.48 .. NELBO: 1722.48\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.59 .. NELBO: 1725.59\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.96 .. NELBO: 1713.96\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.42 .. NELBO: 1722.42\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.47 .. NELBO: 1725.47\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.95 .. NELBO: 1713.95\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.33 .. NELBO: 1722.33\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.38 .. NELBO: 1725.38\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.94 .. NELBO: 1713.94\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.33 .. NELBO: 1722.33\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.42 .. NELBO: 1725.42\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.78 .. NELBO: 1713.78\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.24 .. NELBO: 1722.24\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.29 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.82 .. NELBO: 1713.82\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.2 .. NELBO: 1722.2\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.81 .. NELBO: 1713.81\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.24 .. NELBO: 1722.24\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.29 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.81 .. NELBO: 1713.81\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.16 .. NELBO: 1722.16\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.24 .. NELBO: 1725.24\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.66 .. NELBO: 1713.66\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.13 .. NELBO: 1722.13\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.05 .. NELBO: 1722.05\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.77 .. NELBO: 1713.77\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.03 .. NELBO: 1722.03\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.1 .. NELBO: 1725.1\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.82 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.47 .. NELBO: 1713.47\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.83 .. NELBO: 1721.83\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.21 .. NELBO: 1713.21\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.15 .. NELBO: 1713.15\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.05 .. NELBO: 1713.05\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.94 .. NELBO: 1712.94\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.55 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.13 .. NELBO: 1713.13\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:23:58,747] Trial 40 finished with values: [-0.008259848806871927, 0.025] and parameters: {'num_topics': 44, 'dropout': 0.14693900958923492, 't_hidden_size': 50, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.11139056710698976, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=44, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=44, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=44, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.11 .. Rec_loss: 2007.1 .. NELBO: 2008.21\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.67 .. Rec_loss: 1887.23 .. NELBO: 1887.9\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 1874.85 .. NELBO: 1875.46\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1723.5 .. NELBO: 1723.56\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1730.99 .. NELBO: 1731.02\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1734.29 .. NELBO: 1734.32\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1719.68 .. NELBO: 1719.69\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.84 .. NELBO: 1727.85\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.04 .. NELBO: 1731.05\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1718.33 .. NELBO: 1718.33\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.53 .. NELBO: 1726.53\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.69 .. NELBO: 1729.69\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.59 .. NELBO: 1717.59\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.78 .. NELBO: 1725.78\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.92 .. NELBO: 1728.92\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.98 .. NELBO: 1716.98\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.33 .. NELBO: 1728.33\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.5 .. NELBO: 1716.5\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.88 .. NELBO: 1727.88\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.02 .. NELBO: 1716.02\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.49 .. NELBO: 1727.49\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.72 .. NELBO: 1715.72\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.01 .. NELBO: 1724.01\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.16 .. NELBO: 1727.16\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.38 .. NELBO: 1715.38\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.73 .. NELBO: 1723.73\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.85 .. NELBO: 1726.85\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.25 .. NELBO: 1715.25\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.53 .. NELBO: 1723.53\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.69 .. NELBO: 1726.69\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.96 .. NELBO: 1714.96\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.4 .. NELBO: 1723.4\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.51 .. NELBO: 1726.51\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.73 .. NELBO: 1714.73\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.11 .. NELBO: 1723.11\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.22 .. NELBO: 1726.22\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.55 .. NELBO: 1714.55\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.91 .. NELBO: 1722.91\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.03 .. NELBO: 1726.03\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.38 .. NELBO: 1714.38\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.79 .. NELBO: 1722.79\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.88 .. NELBO: 1725.88\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.34 .. NELBO: 1714.34\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.71 .. NELBO: 1722.71\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.8 .. NELBO: 1725.8\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.31 .. NELBO: 1714.31\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.67 .. NELBO: 1722.67\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.78 .. NELBO: 1725.78\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.18 .. NELBO: 1714.18\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.66 .. NELBO: 1722.66\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.72 .. NELBO: 1725.72\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.17 .. NELBO: 1714.17\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.54 .. NELBO: 1722.54\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.6 .. NELBO: 1725.6\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.06 .. NELBO: 1714.06\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.4 .. NELBO: 1722.4\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.48 .. NELBO: 1725.48\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.9 .. NELBO: 1713.9\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.31 .. NELBO: 1722.31\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.37 .. NELBO: 1725.37\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.89 .. NELBO: 1713.89\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.31 .. NELBO: 1722.31\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.37 .. NELBO: 1725.37\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.81 .. NELBO: 1713.81\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.22 .. NELBO: 1722.22\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.28 .. NELBO: 1725.28\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.76 .. NELBO: 1713.76\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.17 .. NELBO: 1722.17\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.75 .. NELBO: 1713.75\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.15 .. NELBO: 1722.15\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.21 .. NELBO: 1725.21\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.62 .. NELBO: 1713.62\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.07 .. NELBO: 1722.07\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.12 .. NELBO: 1725.12\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.58 .. NELBO: 1713.58\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.04 .. NELBO: 1722.04\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.06 .. NELBO: 1725.06\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.71 .. NELBO: 1713.71\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.04 .. NELBO: 1722.04\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.52 .. NELBO: 1713.52\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.02 .. NELBO: 1725.02\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.42 .. NELBO: 1713.42\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.5 .. NELBO: 1713.5\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.93 .. NELBO: 1724.93\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.35 .. NELBO: 1713.35\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.34 .. NELBO: 1713.34\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.29 .. NELBO: 1713.29\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.32 .. NELBO: 1713.32\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.17 .. NELBO: 1713.17\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.19 .. NELBO: 1713.19\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.0 .. NELBO: 1713.0\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.47 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.01 .. NELBO: 1713.01\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.03 .. NELBO: 1713.03\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.06 .. NELBO: 1713.06\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.59 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1713.01 .. NELBO: 1713.02\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.48 .. NELBO: 1721.48\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:25:12,941] Trial 41 finished with values: [-0.008219068714424911, 0.025] and parameters: {'num_topics': 44, 'dropout': 0.11139056710698976, 't_hidden_size': 100, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.3104505269586035, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=24, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=24, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=24, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 2000.95 .. NELBO: 2001.25\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 1883.29 .. NELBO: 1883.76\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 1871.25 .. NELBO: 1871.73\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.33 .. NELBO: 1723.38\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1731.07 .. NELBO: 1731.12\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.34 .. NELBO: 1734.39\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1719.93 .. NELBO: 1719.96\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1728.07 .. NELBO: 1728.09\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.27 .. NELBO: 1731.29\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.49 .. NELBO: 1718.5\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.67 .. NELBO: 1726.68\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.83 .. NELBO: 1729.84\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.69 .. NELBO: 1717.69\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.88 .. NELBO: 1725.88\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.02 .. NELBO: 1729.02\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.08 .. NELBO: 1717.08\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.4 .. NELBO: 1728.4\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.5 .. NELBO: 1716.5\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.89 .. NELBO: 1727.89\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1716.02 .. NELBO: 1716.03\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.31 .. NELBO: 1724.32\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.46 .. NELBO: 1727.48\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1715.55 .. NELBO: 1715.59\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1723.83 .. NELBO: 1723.89\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1726.93 .. NELBO: 1727.02\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 1714.72 .. NELBO: 1714.98\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.34 .. Rec_loss: 1722.9 .. NELBO: 1723.24\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 1726.0 .. NELBO: 1726.37\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 1713.89 .. NELBO: 1714.54\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 1722.21 .. NELBO: 1722.83\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 1725.39 .. NELBO: 1726.02\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.8 .. Rec_loss: 1713.33 .. NELBO: 1714.13\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 1721.91 .. NELBO: 1722.68\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 1725.06 .. NELBO: 1725.8\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.63 .. Rec_loss: 1714.12 .. NELBO: 1714.75\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.68 .. Rec_loss: 1722.22 .. NELBO: 1722.9\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.69 .. Rec_loss: 1725.24 .. NELBO: 1725.93\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.9 .. Rec_loss: 1712.6 .. NELBO: 1713.5\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.95 .. Rec_loss: 1720.97 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 1.02 .. Rec_loss: 1723.84 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.43 .. Rec_loss: 1710.73 .. NELBO: 1712.16\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.24 .. Rec_loss: 1719.7 .. NELBO: 1720.94\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 1.47 .. Rec_loss: 1722.18 .. NELBO: 1723.65\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.04 .. Rec_loss: 1708.75 .. NELBO: 1710.79\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.52 .. Rec_loss: 1718.37 .. NELBO: 1719.89\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 1.9 .. Rec_loss: 1720.47 .. NELBO: 1722.37\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.2 .. Rec_loss: 1706.7 .. NELBO: 1708.9\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.64 .. Rec_loss: 1716.91 .. NELBO: 1718.55\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.0 .. Rec_loss: 1718.82 .. NELBO: 1720.82\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.31 .. Rec_loss: 1705.54 .. NELBO: 1707.85\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.65 .. Rec_loss: 1716.28 .. NELBO: 1717.93\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 1.91 .. Rec_loss: 1718.03 .. NELBO: 1719.94\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1704.87 .. NELBO: 1707.58\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.85 .. Rec_loss: 1716.15 .. NELBO: 1718.0\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.04 .. Rec_loss: 1717.75 .. NELBO: 1719.79\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1706.98 .. NELBO: 1709.91\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.01 .. Rec_loss: 1717.62 .. NELBO: 1719.63\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.1 .. Rec_loss: 1719.37 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1707.43 .. NELBO: 1710.38\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.02 .. Rec_loss: 1717.8 .. NELBO: 1719.82\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.14 .. Rec_loss: 1719.6 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1704.43 .. NELBO: 1707.16\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.0 .. Rec_loss: 1715.38 .. NELBO: 1717.38\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1716.87 .. NELBO: 1719.16\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1703.02 .. NELBO: 1705.78\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.06 .. Rec_loss: 1714.15 .. NELBO: 1716.21\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.35 .. Rec_loss: 1715.55 .. NELBO: 1717.9\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1702.77 .. NELBO: 1705.53\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.09 .. Rec_loss: 1714.05 .. NELBO: 1716.14\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1715.23 .. NELBO: 1717.71\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1701.75 .. NELBO: 1704.43\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.02 .. Rec_loss: 1713.26 .. NELBO: 1715.28\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1714.58 .. NELBO: 1716.94\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1701.28 .. NELBO: 1704.05\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.06 .. Rec_loss: 1713.13 .. NELBO: 1715.19\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.38 .. Rec_loss: 1714.41 .. NELBO: 1716.79\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.81 .. Rec_loss: 1701.62 .. NELBO: 1704.43\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.14 .. Rec_loss: 1713.52 .. NELBO: 1715.66\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1714.79 .. NELBO: 1717.19\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 1701.88 .. NELBO: 1704.94\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.32 .. Rec_loss: 1713.69 .. NELBO: 1716.01\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1714.86 .. NELBO: 1717.37\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.07 .. Rec_loss: 1700.93 .. NELBO: 1704.0\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.38 .. Rec_loss: 1712.84 .. NELBO: 1715.22\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1714.35 .. NELBO: 1716.89\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.12 .. Rec_loss: 1700.23 .. NELBO: 1703.35\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.5 .. Rec_loss: 1712.21 .. NELBO: 1714.71\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1713.63 .. NELBO: 1716.33\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.3 .. Rec_loss: 1699.22 .. NELBO: 1702.52\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1711.29 .. NELBO: 1713.93\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1712.84 .. NELBO: 1715.67\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.26 .. Rec_loss: 1698.14 .. NELBO: 1701.4\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1710.36 .. NELBO: 1713.04\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.03 .. Rec_loss: 1711.72 .. NELBO: 1714.75\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.5 .. Rec_loss: 1697.43 .. NELBO: 1700.93\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1709.73 .. NELBO: 1712.55\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.07 .. Rec_loss: 1711.14 .. NELBO: 1714.21\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.6 .. Rec_loss: 1696.75 .. NELBO: 1700.35\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1709.12 .. NELBO: 1712.05\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.26 .. Rec_loss: 1710.42 .. NELBO: 1713.68\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.81 .. Rec_loss: 1695.88 .. NELBO: 1699.69\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.12 .. Rec_loss: 1708.32 .. NELBO: 1711.44\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.49 .. Rec_loss: 1709.64 .. NELBO: 1713.13\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.66 .. Rec_loss: 1695.94 .. NELBO: 1699.6\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.08 .. Rec_loss: 1708.17 .. NELBO: 1711.25\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.48 .. Rec_loss: 1709.37 .. NELBO: 1712.85\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.02 .. Rec_loss: 1694.9 .. NELBO: 1698.92\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.4 .. Rec_loss: 1707.39 .. NELBO: 1710.79\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.77 .. Rec_loss: 1708.66 .. NELBO: 1712.43\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.18 .. Rec_loss: 1694.22 .. NELBO: 1698.4\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.58 .. Rec_loss: 1706.78 .. NELBO: 1710.36\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.82 .. Rec_loss: 1708.2 .. NELBO: 1712.02\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.54 .. Rec_loss: 1693.48 .. NELBO: 1698.02\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.98 .. Rec_loss: 1706.34 .. NELBO: 1710.32\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 4.21 .. Rec_loss: 1707.75 .. NELBO: 1711.96\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 1693.72 .. NELBO: 1697.91\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.88 .. Rec_loss: 1706.88 .. NELBO: 1710.76\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 4.17 .. Rec_loss: 1708.19 .. NELBO: 1712.36\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.17 .. Rec_loss: 1693.66 .. NELBO: 1697.83\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.95 .. Rec_loss: 1706.71 .. NELBO: 1710.66\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4.21 .. Rec_loss: 1708.14 .. NELBO: 1712.35\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 1692.72 .. NELBO: 1696.91\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.74 .. Rec_loss: 1705.47 .. NELBO: 1709.21\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4.12 .. Rec_loss: 1706.75 .. NELBO: 1710.87\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.58 .. Rec_loss: 1691.62 .. NELBO: 1696.2\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.98 .. Rec_loss: 1704.41 .. NELBO: 1708.39\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 4.32 .. Rec_loss: 1705.73 .. NELBO: 1710.05\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.92 .. Rec_loss: 1690.71 .. NELBO: 1695.63\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.17 .. Rec_loss: 1703.81 .. NELBO: 1707.98\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 4.52 .. Rec_loss: 1705.3 .. NELBO: 1709.82\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.76 .. Rec_loss: 1690.5 .. NELBO: 1695.26\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1703.43 .. NELBO: 1707.57\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.39 .. Rec_loss: 1704.92 .. NELBO: 1709.31\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1689.62 .. NELBO: 1695.11\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.59 .. Rec_loss: 1702.81 .. NELBO: 1707.4\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 4.85 .. Rec_loss: 1704.27 .. NELBO: 1709.12\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.29 .. Rec_loss: 1689.57 .. NELBO: 1694.86\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.46 .. Rec_loss: 1702.68 .. NELBO: 1707.14\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 4.79 .. Rec_loss: 1704.11 .. NELBO: 1708.9\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.35 .. Rec_loss: 1689.27 .. NELBO: 1694.62\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.6 .. Rec_loss: 1702.26 .. NELBO: 1706.86\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 4.99 .. Rec_loss: 1703.65 .. NELBO: 1708.64\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.38 .. Rec_loss: 1688.51 .. NELBO: 1693.89\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.64 .. Rec_loss: 1701.5 .. NELBO: 1706.14\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1703.03 .. NELBO: 1707.98\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 1688.6 .. NELBO: 1693.82\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.69 .. Rec_loss: 1701.15 .. NELBO: 1705.84\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 4.96 .. Rec_loss: 1702.74 .. NELBO: 1707.7\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:26:29,992] Trial 42 finished with values: [0.0026266247342269836, 0.2125] and parameters: {'num_topics': 24, 'dropout': 0.3104505269586035, 't_hidden_size': 200, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.2769868853252398, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.16 .. Rec_loss: 1989.99 .. NELBO: 1991.15\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.83 .. Rec_loss: 1876.44 .. NELBO: 1877.27\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.82 .. Rec_loss: 1865.14 .. NELBO: 1865.96\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1722.59 .. NELBO: 1722.63\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.44 .. NELBO: 1730.49\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1733.76 .. NELBO: 1733.81\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1719.7 .. NELBO: 1719.79\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1727.87 .. NELBO: 1727.97\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1731.07 .. NELBO: 1731.18\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 1718.58 .. NELBO: 1718.72\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 1726.7 .. NELBO: 1726.83\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 1729.83 .. NELBO: 1729.97\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 1717.55 .. NELBO: 1717.71\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 1725.67 .. NELBO: 1725.84\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.19 .. Rec_loss: 1728.76 .. NELBO: 1728.95\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 1716.63 .. NELBO: 1716.89\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 1724.77 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 1727.83 .. NELBO: 1728.15\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 1715.36 .. NELBO: 1715.88\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.57 .. Rec_loss: 1723.46 .. NELBO: 1724.03\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.62 .. Rec_loss: 1726.5 .. NELBO: 1727.12\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1714.18 .. NELBO: 1714.99\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.8 .. Rec_loss: 1722.5 .. NELBO: 1723.3\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.85 .. Rec_loss: 1725.54 .. NELBO: 1726.39\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.1 .. Rec_loss: 1712.84 .. NELBO: 1713.94\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.06 .. Rec_loss: 1721.33 .. NELBO: 1722.39\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 1.1 .. Rec_loss: 1724.23 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.62 .. Rec_loss: 1711.24 .. NELBO: 1712.86\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.42 .. Rec_loss: 1720.03 .. NELBO: 1721.45\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 1.54 .. Rec_loss: 1722.3 .. NELBO: 1723.84\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.16 .. Rec_loss: 1709.39 .. NELBO: 1711.55\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.64 .. Rec_loss: 1719.21 .. NELBO: 1720.85\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.89 .. Rec_loss: 1721.09 .. NELBO: 1722.98\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.23 .. Rec_loss: 1706.07 .. NELBO: 1708.3\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.71 .. Rec_loss: 1716.64 .. NELBO: 1718.35\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 1.94 .. Rec_loss: 1718.45 .. NELBO: 1720.39\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.57 .. Rec_loss: 1703.97 .. NELBO: 1706.54\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.96 .. Rec_loss: 1714.6 .. NELBO: 1716.56\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.11 .. Rec_loss: 1716.41 .. NELBO: 1718.52\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.84 .. Rec_loss: 1702.81 .. NELBO: 1705.65\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.15 .. Rec_loss: 1713.68 .. NELBO: 1715.83\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1715.54 .. NELBO: 1717.83\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1701.98 .. NELBO: 1704.81\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1712.98 .. NELBO: 1715.15\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.35 .. Rec_loss: 1714.76 .. NELBO: 1717.11\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1701.3 .. NELBO: 1704.05\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1712.25 .. NELBO: 1714.42\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1714.01 .. NELBO: 1716.37\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1700.55 .. NELBO: 1703.28\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.21 .. Rec_loss: 1711.47 .. NELBO: 1713.68\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1713.29 .. NELBO: 1715.69\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1699.94 .. NELBO: 1702.72\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.25 .. Rec_loss: 1710.84 .. NELBO: 1713.09\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1712.69 .. NELBO: 1715.13\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1699.9 .. NELBO: 1702.81\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1710.61 .. NELBO: 1712.98\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1712.46 .. NELBO: 1714.97\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1699.59 .. NELBO: 1702.61\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.49 .. Rec_loss: 1710.19 .. NELBO: 1712.68\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1712.01 .. NELBO: 1714.64\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.03 .. Rec_loss: 1698.92 .. NELBO: 1701.95\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.57 .. Rec_loss: 1709.93 .. NELBO: 1712.5\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1711.73 .. NELBO: 1714.46\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1698.09 .. NELBO: 1701.24\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1709.21 .. NELBO: 1711.88\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.82 .. Rec_loss: 1710.94 .. NELBO: 1713.76\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.38 .. Rec_loss: 1697.59 .. NELBO: 1700.97\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1708.92 .. NELBO: 1711.84\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1710.74 .. NELBO: 1713.75\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.53 .. Rec_loss: 1697.85 .. NELBO: 1701.38\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.14 .. Rec_loss: 1709.05 .. NELBO: 1712.19\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1710.75 .. NELBO: 1713.98\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.37 .. Rec_loss: 1697.95 .. NELBO: 1701.32\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1709.48 .. NELBO: 1712.57\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1710.95 .. NELBO: 1714.16\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.81 .. Rec_loss: 1697.0 .. NELBO: 1700.81\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.35 .. Rec_loss: 1708.88 .. NELBO: 1712.23\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 3.35 .. Rec_loss: 1710.66 .. NELBO: 1714.01\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.67 .. Rec_loss: 1697.19 .. NELBO: 1701.86\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.76 .. Rec_loss: 1708.17 .. NELBO: 1711.93\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 3.78 .. Rec_loss: 1709.83 .. NELBO: 1713.61\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.69 .. Rec_loss: 1694.15 .. NELBO: 1698.84\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.85 .. Rec_loss: 1705.51 .. NELBO: 1709.36\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 3.97 .. Rec_loss: 1706.86 .. NELBO: 1710.83\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1690.69 .. NELBO: 1695.64\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.02 .. Rec_loss: 1702.85 .. NELBO: 1706.87\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1704.27 .. NELBO: 1708.41\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1688.95 .. NELBO: 1693.95\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.12 .. Rec_loss: 1701.8 .. NELBO: 1705.92\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 1703.21 .. NELBO: 1707.4\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.11 .. Rec_loss: 1688.14 .. NELBO: 1693.25\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.25 .. Rec_loss: 1701.1 .. NELBO: 1705.35\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 4.31 .. Rec_loss: 1702.49 .. NELBO: 1706.8\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.99 .. Rec_loss: 1687.81 .. NELBO: 1692.8\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.27 .. Rec_loss: 1700.59 .. NELBO: 1704.86\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 4.31 .. Rec_loss: 1702.14 .. NELBO: 1706.45\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1687.54 .. NELBO: 1692.51\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.32 .. Rec_loss: 1700.02 .. NELBO: 1704.34\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 4.37 .. Rec_loss: 1701.62 .. NELBO: 1705.99\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.94 .. Rec_loss: 1687.65 .. NELBO: 1692.59\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.35 .. Rec_loss: 1700.04 .. NELBO: 1704.39\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 1701.7 .. NELBO: 1706.12\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.75 .. Rec_loss: 1688.02 .. NELBO: 1692.77\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.36 .. Rec_loss: 1699.89 .. NELBO: 1704.25\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 4.5 .. Rec_loss: 1701.35 .. NELBO: 1705.85\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.75 .. Rec_loss: 1687.11 .. NELBO: 1691.86\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.33 .. Rec_loss: 1699.26 .. NELBO: 1703.59\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 4.44 .. Rec_loss: 1700.82 .. NELBO: 1705.26\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.79 .. Rec_loss: 1687.19 .. NELBO: 1691.98\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.51 .. Rec_loss: 1699.3 .. NELBO: 1703.81\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 4.62 .. Rec_loss: 1700.82 .. NELBO: 1705.44\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.79 .. Rec_loss: 1686.75 .. NELBO: 1691.54\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1698.76 .. NELBO: 1703.21\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 4.58 .. Rec_loss: 1700.24 .. NELBO: 1704.82\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1686.8 .. NELBO: 1691.75\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.68 .. Rec_loss: 1698.65 .. NELBO: 1703.33\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 4.8 .. Rec_loss: 1700.18 .. NELBO: 1704.98\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.85 .. Rec_loss: 1686.05 .. NELBO: 1690.9\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.64 .. Rec_loss: 1698.01 .. NELBO: 1702.65\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 4.8 .. Rec_loss: 1699.5 .. NELBO: 1704.3\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.98 .. Rec_loss: 1686.4 .. NELBO: 1691.38\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.81 .. Rec_loss: 1698.03 .. NELBO: 1702.84\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4.9 .. Rec_loss: 1699.48 .. NELBO: 1704.38\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.85 .. Rec_loss: 1686.29 .. NELBO: 1691.14\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.82 .. Rec_loss: 1698.1 .. NELBO: 1702.92\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4.95 .. Rec_loss: 1699.54 .. NELBO: 1704.49\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.02 .. Rec_loss: 1686.02 .. NELBO: 1691.04\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1697.93 .. NELBO: 1702.9\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1699.34 .. NELBO: 1704.44\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1686.48 .. NELBO: 1691.58\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.02 .. Rec_loss: 1698.31 .. NELBO: 1703.33\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 5.13 .. Rec_loss: 1699.78 .. NELBO: 1704.91\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.21 .. Rec_loss: 1686.06 .. NELBO: 1691.27\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.01 .. Rec_loss: 1698.03 .. NELBO: 1703.04\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 5.19 .. Rec_loss: 1699.49 .. NELBO: 1704.68\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.02 .. Rec_loss: 1685.5 .. NELBO: 1690.52\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 1697.75 .. NELBO: 1702.66\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 5.11 .. Rec_loss: 1699.31 .. NELBO: 1704.42\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 1686.19 .. NELBO: 1691.1\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.7 .. Rec_loss: 1698.27 .. NELBO: 1702.97\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 4.97 .. Rec_loss: 1699.87 .. NELBO: 1704.84\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1685.79 .. NELBO: 1690.82\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1698.24 .. NELBO: 1703.08\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 5.07 .. Rec_loss: 1699.73 .. NELBO: 1704.8\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.53 .. Rec_loss: 1684.25 .. NELBO: 1689.78\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1696.3 .. NELBO: 1701.45\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 5.43 .. Rec_loss: 1697.86 .. NELBO: 1703.29\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.21 .. Rec_loss: 1683.78 .. NELBO: 1688.99\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.88 .. Rec_loss: 1696.28 .. NELBO: 1701.16\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 5.12 .. Rec_loss: 1697.68 .. NELBO: 1702.8\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:27:47,069] Trial 43 finished with values: [0.00755896331352282, 0.45454545454545453] and parameters: {'num_topics': 11, 'dropout': 0.2769868853252398, 't_hidden_size': 300, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.4565340664619044, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=100, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.26 .. Rec_loss: 1992.66 .. NELBO: 1993.92\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.71 .. Rec_loss: 1878.47 .. NELBO: 1879.18\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1866.99 .. NELBO: 1867.63\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1723.95 .. NELBO: 1723.99\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1731.25 .. NELBO: 1731.29\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.55 .. NELBO: 1734.59\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1720.01 .. NELBO: 1720.06\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1728.15 .. NELBO: 1728.2\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1731.35 .. NELBO: 1731.4\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1718.59 .. NELBO: 1718.62\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1726.76 .. NELBO: 1726.79\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1729.92 .. NELBO: 1729.94\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1717.83 .. NELBO: 1717.84\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1725.97 .. NELBO: 1725.98\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.11 .. NELBO: 1729.12\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.22 .. NELBO: 1717.22\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.38 .. NELBO: 1725.38\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.53 .. NELBO: 1728.53\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.58 .. NELBO: 1716.58\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.98 .. NELBO: 1727.98\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.15 .. NELBO: 1716.15\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.42 .. NELBO: 1724.42\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.56 .. NELBO: 1727.56\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.79 .. NELBO: 1715.79\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.09 .. NELBO: 1724.09\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.24 .. NELBO: 1727.24\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.38 .. NELBO: 1715.38\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.77 .. NELBO: 1723.77\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.9 .. NELBO: 1726.9\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.24 .. NELBO: 1715.24\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.54 .. NELBO: 1723.54\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.69 .. NELBO: 1726.69\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.98 .. NELBO: 1714.98\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.41 .. NELBO: 1723.41\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.52 .. NELBO: 1726.52\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.83 .. NELBO: 1714.83\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.21 .. NELBO: 1723.21\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.3 .. NELBO: 1726.3\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.92 .. NELBO: 1714.92\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.15 .. NELBO: 1723.15\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.28 .. NELBO: 1726.28\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.55 .. NELBO: 1714.55\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.02 .. NELBO: 1723.02\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.11 .. NELBO: 1726.11\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.48 .. NELBO: 1714.48\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.85 .. NELBO: 1722.85\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.92 .. NELBO: 1725.92\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.37 .. NELBO: 1714.37\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.75 .. NELBO: 1722.75\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.81 .. NELBO: 1725.81\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.25 .. NELBO: 1714.25\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.61 .. NELBO: 1722.61\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.68 .. NELBO: 1725.68\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.25 .. NELBO: 1714.25\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.57 .. NELBO: 1722.57\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.66 .. NELBO: 1725.66\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.99 .. NELBO: 1713.99\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.47 .. NELBO: 1722.47\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.51 .. NELBO: 1725.51\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.03 .. NELBO: 1714.03\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.39 .. NELBO: 1722.39\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.45 .. NELBO: 1725.45\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.92 .. NELBO: 1713.92\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.29 .. NELBO: 1722.29\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.35 .. NELBO: 1725.35\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.83 .. NELBO: 1713.83\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.24 .. NELBO: 1722.24\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.3 .. NELBO: 1725.3\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.7 .. NELBO: 1713.7\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.16 .. NELBO: 1722.16\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.7 .. NELBO: 1713.7\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.12 .. NELBO: 1722.12\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.63 .. NELBO: 1713.63\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.03 .. NELBO: 1722.03\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.07 .. NELBO: 1725.07\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.57 .. NELBO: 1713.57\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.96 .. NELBO: 1721.96\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.0 .. NELBO: 1725.0\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.97 .. NELBO: 1724.97\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.44 .. NELBO: 1713.44\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.96 .. NELBO: 1724.96\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.39 .. NELBO: 1713.39\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.89 .. NELBO: 1724.89\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.45 .. NELBO: 1713.45\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.81 .. NELBO: 1721.81\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.89 .. NELBO: 1724.89\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.97 .. NELBO: 1721.97\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.03 .. NELBO: 1725.03\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.98 .. NELBO: 1721.98\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.67 .. NELBO: 1713.67\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.96 .. NELBO: 1724.96\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.89 .. NELBO: 1721.89\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.28 .. NELBO: 1713.28\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.82 .. NELBO: 1724.82\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.73 .. NELBO: 1721.73\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.27 .. NELBO: 1713.27\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.69 .. NELBO: 1721.69\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.78 .. NELBO: 1721.78\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.75 .. NELBO: 1721.75\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.76 .. NELBO: 1724.76\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.7 .. NELBO: 1724.7\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.24 .. NELBO: 1713.24\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.98 .. NELBO: 1712.98\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.49 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.51 .. NELBO: 1724.51\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.41 .. NELBO: 1721.41\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.97 .. NELBO: 1712.97\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.38 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.87 .. NELBO: 1712.87\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.37 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.41 .. NELBO: 1724.41\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.86 .. NELBO: 1712.86\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.4 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.93 .. NELBO: 1712.93\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.36 .. NELBO: 1721.36\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.43 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.9 .. NELBO: 1712.9\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:28:57,021] Trial 44 finished with values: [-0.007972072099875023, 0.1] and parameters: {'num_topics': 10, 'dropout': 0.4565340664619044, 't_hidden_size': 100, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.005461920059068204, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=37, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=50, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=50, out_features=37, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=50, out_features=37, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.07 .. Rec_loss: 2002.49 .. NELBO: 2003.56\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.72 .. Rec_loss: 1884.44 .. NELBO: 1885.16\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.68 .. Rec_loss: 1872.32 .. NELBO: 1873.0\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 1723.38 .. NELBO: 1723.51\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1730.9 .. NELBO: 1730.98\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1734.18 .. NELBO: 1734.25\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.76 .. NELBO: 1719.78\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.92 .. NELBO: 1727.94\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1731.12 .. NELBO: 1731.14\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.43 .. NELBO: 1718.44\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.63 .. NELBO: 1726.64\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.79 .. NELBO: 1729.8\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.69 .. NELBO: 1717.69\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.87 .. NELBO: 1725.87\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1729.01 .. NELBO: 1729.01\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1717.08 .. NELBO: 1717.08\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1728.4 .. NELBO: 1728.4\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.52 .. NELBO: 1716.52\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.91 .. NELBO: 1727.91\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1716.06 .. NELBO: 1716.06\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.5 .. NELBO: 1727.5\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.67 .. NELBO: 1715.67\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.0 .. NELBO: 1724.0\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.14 .. NELBO: 1727.14\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.36 .. NELBO: 1715.36\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.71 .. NELBO: 1723.71\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.85 .. NELBO: 1726.85\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.11 .. NELBO: 1715.11\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.49 .. NELBO: 1723.49\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.61 .. NELBO: 1726.61\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.9 .. NELBO: 1714.9\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.27 .. NELBO: 1723.27\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.39 .. NELBO: 1726.39\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.76 .. NELBO: 1714.76\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.11 .. NELBO: 1723.11\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.22 .. NELBO: 1726.22\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.6 .. NELBO: 1714.6\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.99 .. NELBO: 1722.99\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.1 .. NELBO: 1726.1\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.41 .. NELBO: 1714.41\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.83 .. NELBO: 1722.83\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.92 .. NELBO: 1725.92\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.35 .. NELBO: 1714.35\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.7 .. NELBO: 1722.7\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.8 .. NELBO: 1725.8\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.25 .. NELBO: 1714.25\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.63 .. NELBO: 1722.63\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.73 .. NELBO: 1725.73\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.15 .. NELBO: 1714.15\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.54 .. NELBO: 1722.54\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.61 .. NELBO: 1725.61\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.08 .. NELBO: 1714.08\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.46 .. NELBO: 1722.46\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.53 .. NELBO: 1725.53\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.01 .. NELBO: 1714.01\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.4 .. NELBO: 1722.4\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.46 .. NELBO: 1725.46\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.98 .. NELBO: 1713.98\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.35 .. NELBO: 1722.35\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.43 .. NELBO: 1725.43\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.88 .. NELBO: 1713.88\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.29 .. NELBO: 1722.29\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.36 .. NELBO: 1725.36\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.83 .. NELBO: 1713.83\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.27 .. NELBO: 1722.27\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.33 .. NELBO: 1725.33\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.79 .. NELBO: 1713.79\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.21 .. NELBO: 1722.21\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.26 .. NELBO: 1725.26\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.7 .. NELBO: 1713.7\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.12 .. NELBO: 1722.12\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.17 .. NELBO: 1725.17\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.64 .. NELBO: 1713.64\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.06 .. NELBO: 1722.06\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.11 .. NELBO: 1725.11\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.65 .. NELBO: 1713.65\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.04 .. NELBO: 1722.04\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.1 .. NELBO: 1725.1\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.58 .. NELBO: 1713.58\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.0 .. NELBO: 1722.0\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.05 .. NELBO: 1725.05\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.98 .. NELBO: 1724.98\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.46 .. NELBO: 1713.46\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.39 .. NELBO: 1713.39\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.79 .. NELBO: 1721.79\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.83 .. NELBO: 1724.83\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.41 .. NELBO: 1713.41\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.81 .. NELBO: 1721.81\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.79 .. NELBO: 1724.79\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.67 .. NELBO: 1721.67\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.72 .. NELBO: 1724.72\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.53 .. NELBO: 1721.53\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.57 .. NELBO: 1721.57\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.99 .. NELBO: 1712.99\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.49 .. NELBO: 1724.49\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.91 .. NELBO: 1712.91\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.46 .. NELBO: 1721.46\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.5 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.07 .. NELBO: 1713.07\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.43 .. NELBO: 1721.43\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.47 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.95 .. NELBO: 1712.95\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.37 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.44 .. NELBO: 1724.44\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.74 .. NELBO: 1712.74\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.31 .. NELBO: 1721.31\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.36 .. NELBO: 1724.36\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.96 .. NELBO: 1712.96\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.37 .. NELBO: 1721.37\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.45 .. NELBO: 1724.45\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:30:09,863] Trial 45 finished with values: [-0.008765935457076166, 0.02972972972972973] and parameters: {'num_topics': 37, 'dropout': 0.005461920059068204, 't_hidden_size': 50, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5425847953641002, inplace=False)\n",
      "  (theta_act): Sigmoid()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.46 .. Rec_loss: 2001.64 .. NELBO: 2003.1\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1882.97 .. NELBO: 1883.78\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 1870.93 .. NELBO: 1871.67\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1723.72 .. NELBO: 1723.81\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1730.99 .. NELBO: 1731.08\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1734.29 .. NELBO: 1734.38\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1720.0 .. NELBO: 1720.1\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1728.08 .. NELBO: 1728.18\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1731.3 .. NELBO: 1731.4\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1718.64 .. NELBO: 1718.74\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1726.82 .. NELBO: 1726.91\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1729.98 .. NELBO: 1730.07\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1717.88 .. NELBO: 1717.94\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1726.04 .. NELBO: 1726.09\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1729.19 .. NELBO: 1729.24\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1717.24 .. NELBO: 1717.26\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1725.38 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1728.54 .. NELBO: 1728.56\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1716.59 .. NELBO: 1716.6\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.84 .. NELBO: 1724.85\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.99 .. NELBO: 1728.0\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1716.14 .. NELBO: 1716.15\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.42 .. NELBO: 1724.43\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.55 .. NELBO: 1727.56\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.81 .. NELBO: 1715.81\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.09 .. NELBO: 1724.09\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1727.26 .. NELBO: 1727.26\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.43 .. NELBO: 1715.43\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.81 .. NELBO: 1723.81\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.94 .. NELBO: 1726.94\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.28 .. NELBO: 1715.28\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.62 .. NELBO: 1723.62\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.77 .. NELBO: 1726.77\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.03 .. NELBO: 1715.03\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.43 .. NELBO: 1723.43\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.54 .. NELBO: 1726.54\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.81 .. NELBO: 1714.81\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.15 .. NELBO: 1723.15\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.27 .. NELBO: 1726.27\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.6 .. NELBO: 1714.6\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.98 .. NELBO: 1722.98\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.09 .. NELBO: 1726.09\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.43 .. NELBO: 1714.43\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.85 .. NELBO: 1722.85\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.94 .. NELBO: 1725.94\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.38 .. NELBO: 1714.38\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.72 .. NELBO: 1722.72\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.81 .. NELBO: 1725.81\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.3 .. NELBO: 1714.3\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.65 .. NELBO: 1722.65\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.76 .. NELBO: 1725.76\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.15 .. NELBO: 1714.15\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.64 .. NELBO: 1722.64\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.72 .. NELBO: 1725.72\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.1 .. NELBO: 1714.1\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.52 .. NELBO: 1722.52\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.57 .. NELBO: 1725.57\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.17 .. NELBO: 1714.17\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.45 .. NELBO: 1722.45\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.55 .. NELBO: 1725.55\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.99 .. NELBO: 1713.99\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.45 .. NELBO: 1722.45\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.51 .. NELBO: 1725.51\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.9 .. NELBO: 1713.9\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.33 .. NELBO: 1722.33\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.4 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.86 .. NELBO: 1713.86\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.3 .. NELBO: 1722.3\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.35 .. NELBO: 1725.35\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.79 .. NELBO: 1713.79\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.24 .. NELBO: 1722.24\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.28 .. NELBO: 1725.28\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.86 .. NELBO: 1713.86\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.22 .. NELBO: 1722.22\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.29 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.72 .. NELBO: 1713.72\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.13 .. NELBO: 1722.13\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.18 .. NELBO: 1725.18\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.6 .. NELBO: 1713.6\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.02 .. NELBO: 1722.02\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.08 .. NELBO: 1725.08\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.57 .. NELBO: 1713.57\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.97 .. NELBO: 1721.97\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.03 .. NELBO: 1725.03\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.53 .. NELBO: 1713.53\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.91 .. NELBO: 1721.91\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.42 .. NELBO: 1713.42\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.89 .. NELBO: 1724.89\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.38 .. NELBO: 1713.38\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.82 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.87 .. NELBO: 1724.87\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.77 .. NELBO: 1721.77\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.8 .. NELBO: 1724.8\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.77 .. NELBO: 1724.77\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.75 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.71 .. NELBO: 1721.71\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.3 .. NELBO: 1713.3\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.68 .. NELBO: 1721.68\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.66 .. NELBO: 1721.66\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.71 .. NELBO: 1724.71\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.65 .. NELBO: 1721.65\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.67 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.59 .. NELBO: 1721.59\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.12 .. NELBO: 1713.12\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.61 .. NELBO: 1721.61\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.68 .. NELBO: 1724.68\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.04 .. NELBO: 1713.04\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.63 .. NELBO: 1721.63\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1713.35 .. NELBO: 1713.36\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.68 .. NELBO: 1721.68\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.74 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.74 .. NELBO: 1721.74\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.79 .. NELBO: 1724.79\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.09 .. NELBO: 1713.09\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.2 .. NELBO: 1713.2\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.6 .. NELBO: 1724.6\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1713.07 .. NELBO: 1713.08\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.58 .. NELBO: 1724.59\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.91 .. NELBO: 1712.91\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1721.48 .. NELBO: 1721.49\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.49 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1721.43 .. NELBO: 1721.44\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.46 .. NELBO: 1724.47\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1713.11 .. NELBO: 1713.12\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1721.46 .. NELBO: 1721.47\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.53 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1712.76 .. NELBO: 1712.77\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1721.37 .. NELBO: 1721.38\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.39 .. NELBO: 1724.4\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:31:22,012] Trial 46 finished with values: [-0.008952028691322594, 0.1] and parameters: {'num_topics': 11, 'dropout': 0.5425847953641002, 't_hidden_size': 300, 'activation': 'sigmoid'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.15699856688065783, inplace=False)\n",
      "  (theta_act): Softplus(beta=1.0, threshold=20.0)\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=200, bias=True)\n",
      "    (1): Softplus(beta=1.0, threshold=20.0)\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=200, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=200, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.59 .. Rec_loss: 2003.17 .. NELBO: 2004.76\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.91 .. Rec_loss: 1883.83 .. NELBO: 1884.74\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1871.69 .. NELBO: 1872.5\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1723.46 .. NELBO: 1723.48\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1730.83 .. NELBO: 1730.9\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1734.12 .. NELBO: 1734.19\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1719.73 .. NELBO: 1719.83\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1727.9 .. NELBO: 1728.01\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1731.1 .. NELBO: 1731.21\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1718.48 .. NELBO: 1718.58\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1726.67 .. NELBO: 1726.76\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1729.83 .. NELBO: 1729.92\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1717.7 .. NELBO: 1717.8\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1725.89 .. NELBO: 1725.96\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 1729.03 .. NELBO: 1729.1\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1717.2 .. NELBO: 1717.26\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1725.35 .. NELBO: 1725.4\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1728.51 .. NELBO: 1728.56\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1716.64 .. NELBO: 1716.68\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1724.91 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1728.08 .. NELBO: 1728.11\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1716.16 .. NELBO: 1716.18\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1724.53 .. NELBO: 1724.55\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.65 .. NELBO: 1727.66\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1715.9 .. NELBO: 1715.91\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1724.17 .. NELBO: 1724.18\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.31 .. NELBO: 1727.32\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.47 .. NELBO: 1715.47\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.79 .. NELBO: 1723.79\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.92 .. NELBO: 1726.92\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1715.34 .. NELBO: 1715.34\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.66 .. NELBO: 1723.66\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.82 .. NELBO: 1726.82\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.98 .. NELBO: 1714.98\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.47 .. NELBO: 1723.47\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.56 .. NELBO: 1726.56\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.9 .. NELBO: 1714.9\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1723.19 .. NELBO: 1723.19\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.31 .. NELBO: 1726.31\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.65 .. NELBO: 1714.65\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.97 .. NELBO: 1722.97\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.1 .. NELBO: 1726.1\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.47 .. NELBO: 1714.47\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.91 .. NELBO: 1722.91\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1726.0 .. NELBO: 1726.0\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.42 .. NELBO: 1714.42\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.8 .. NELBO: 1722.8\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.9 .. NELBO: 1725.9\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.22 .. NELBO: 1714.22\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.62 .. NELBO: 1722.62\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.7 .. NELBO: 1725.7\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.12 .. NELBO: 1714.12\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.51 .. NELBO: 1722.51\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.59 .. NELBO: 1725.59\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.03 .. NELBO: 1714.03\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.42 .. NELBO: 1722.42\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.49 .. NELBO: 1725.49\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.04 .. NELBO: 1714.04\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.42 .. NELBO: 1722.42\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.51 .. NELBO: 1725.51\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.97 .. NELBO: 1713.97\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.42 .. NELBO: 1722.42\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.49 .. NELBO: 1725.49\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1714.08 .. NELBO: 1714.08\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.46 .. NELBO: 1722.46\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.54 .. NELBO: 1725.54\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.86 .. NELBO: 1713.86\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.34 .. NELBO: 1722.34\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.38 .. NELBO: 1725.38\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.81 .. NELBO: 1713.81\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.24 .. NELBO: 1722.24\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.29 .. NELBO: 1725.29\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.8 .. NELBO: 1713.8\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.2 .. NELBO: 1722.2\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.25 .. NELBO: 1725.25\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.82 .. NELBO: 1713.82\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.13 .. NELBO: 1722.13\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.2 .. NELBO: 1725.2\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.66 .. NELBO: 1713.66\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1722.09 .. NELBO: 1722.09\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1725.15 .. NELBO: 1725.15\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.43 .. NELBO: 1713.43\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.94 .. NELBO: 1721.94\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.96 .. NELBO: 1724.96\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.57 .. NELBO: 1713.57\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.9 .. NELBO: 1721.9\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.94 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.49 .. NELBO: 1713.49\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.88 .. NELBO: 1721.88\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.37 .. NELBO: 1713.37\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.92 .. NELBO: 1721.92\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.95 .. NELBO: 1724.95\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.5 .. NELBO: 1713.5\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.87 .. NELBO: 1721.87\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.92 .. NELBO: 1724.92\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.39 .. NELBO: 1713.39\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.84 .. NELBO: 1721.84\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.88 .. NELBO: 1724.88\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.4 .. NELBO: 1713.4\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.86 .. NELBO: 1721.86\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.9 .. NELBO: 1724.9\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.36 .. NELBO: 1713.36\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.86 .. NELBO: 1724.86\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.31 .. NELBO: 1713.31\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.76 .. NELBO: 1721.76\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.78 .. NELBO: 1724.78\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.37 .. NELBO: 1713.37\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.75 .. NELBO: 1721.75\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.81 .. NELBO: 1724.81\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.27 .. NELBO: 1713.27\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.8 .. NELBO: 1721.8\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.84 .. NELBO: 1724.84\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.26 .. NELBO: 1713.26\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.72 .. NELBO: 1721.72\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.73 .. NELBO: 1724.73\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.69 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.1 .. NELBO: 1713.1\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.64 .. NELBO: 1721.64\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.66 .. NELBO: 1724.66\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.25 .. NELBO: 1713.25\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.62 .. NELBO: 1721.62\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.64 .. NELBO: 1724.64\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.23 .. NELBO: 1713.23\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.54 .. NELBO: 1721.54\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.63 .. NELBO: 1724.63\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.6 .. NELBO: 1721.6\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.62 .. NELBO: 1724.62\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.02 .. NELBO: 1713.02\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.56 .. NELBO: 1721.56\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.58 .. NELBO: 1724.58\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.18 .. NELBO: 1713.18\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.57 .. NELBO: 1724.57\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.11 .. NELBO: 1713.11\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.5 .. NELBO: 1721.5\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.56 .. NELBO: 1724.56\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1712.92 .. NELBO: 1712.92\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.52 .. NELBO: 1721.52\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.54 .. NELBO: 1724.54\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.08 .. NELBO: 1713.08\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.51 .. NELBO: 1721.51\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.53 .. NELBO: 1724.53\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1713.22 .. NELBO: 1713.22\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1721.55 .. NELBO: 1721.55\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 1724.61 .. NELBO: 1724.61\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:32:31,871] Trial 47 finished with values: [-0.007972072099875022, 0.09090909090909091] and parameters: {'num_topics': 11, 'dropout': 0.15699856688065783, 't_hidden_size': 200, 'activation': 'softplus'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.25320981797696485, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=38, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=38, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=38, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.31 .. Rec_loss: 2005.89 .. NELBO: 2006.2\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 1886.5 .. NELBO: 1886.78\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 1874.19 .. NELBO: 1874.47\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1723.27 .. NELBO: 1723.31\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1730.9 .. NELBO: 1730.93\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1734.2 .. NELBO: 1734.23\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.73 .. NELBO: 1719.75\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1727.9 .. NELBO: 1727.91\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.1 .. NELBO: 1731.11\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1718.31 .. NELBO: 1718.33\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1726.51 .. NELBO: 1726.53\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 1729.66 .. NELBO: 1729.69\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1717.41 .. NELBO: 1717.49\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1725.58 .. NELBO: 1725.69\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 1728.67 .. NELBO: 1728.81\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.36 .. Rec_loss: 1716.31 .. NELBO: 1716.67\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.43 .. Rec_loss: 1724.31 .. NELBO: 1724.74\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.47 .. Rec_loss: 1727.42 .. NELBO: 1727.89\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1715.36 .. NELBO: 1716.0\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.6 .. Rec_loss: 1723.67 .. NELBO: 1724.27\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.61 .. Rec_loss: 1726.82 .. NELBO: 1727.43\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.48 .. Rec_loss: 1715.39 .. NELBO: 1715.87\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 1723.56 .. NELBO: 1724.07\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.51 .. Rec_loss: 1726.72 .. NELBO: 1727.23\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.58 .. Rec_loss: 1714.69 .. NELBO: 1715.27\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.66 .. Rec_loss: 1722.78 .. NELBO: 1723.44\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.67 .. Rec_loss: 1725.89 .. NELBO: 1726.56\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1713.51 .. NELBO: 1714.32\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.84 .. Rec_loss: 1721.84 .. NELBO: 1722.68\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.87 .. Rec_loss: 1724.88 .. NELBO: 1725.75\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.14 .. Rec_loss: 1712.27 .. NELBO: 1713.41\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.15 .. Rec_loss: 1720.67 .. NELBO: 1721.82\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.24 .. Rec_loss: 1723.51 .. NELBO: 1724.75\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.89 .. Rec_loss: 1710.16 .. NELBO: 1712.05\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.58 .. Rec_loss: 1719.27 .. NELBO: 1720.85\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 1.87 .. Rec_loss: 1721.58 .. NELBO: 1723.45\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.17 .. Rec_loss: 1709.13 .. NELBO: 1711.3\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.65 .. Rec_loss: 1718.47 .. NELBO: 1720.12\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.16 .. Rec_loss: 1720.53 .. NELBO: 1722.69\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.88 .. Rec_loss: 1709.06 .. NELBO: 1710.94\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.41 .. Rec_loss: 1718.81 .. NELBO: 1720.22\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 1.92 .. Rec_loss: 1720.41 .. NELBO: 1722.33\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 1709.0 .. NELBO: 1711.43\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.65 .. Rec_loss: 1719.28 .. NELBO: 1720.93\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 1.72 .. Rec_loss: 1721.32 .. NELBO: 1723.04\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.73 .. Rec_loss: 1709.49 .. NELBO: 1713.22\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 1719.47 .. NELBO: 1721.81\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.32 .. Rec_loss: 1721.53 .. NELBO: 1723.85\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1706.73 .. NELBO: 1709.73\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.07 .. Rec_loss: 1717.03 .. NELBO: 1719.1\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.26 .. Rec_loss: 1718.8 .. NELBO: 1721.06\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.24 .. Rec_loss: 1704.58 .. NELBO: 1707.82\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.32 .. Rec_loss: 1715.17 .. NELBO: 1717.49\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1716.92 .. NELBO: 1719.52\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.17 .. Rec_loss: 1703.63 .. NELBO: 1706.8\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.27 .. Rec_loss: 1714.41 .. NELBO: 1716.68\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1716.11 .. NELBO: 1718.69\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.12 .. Rec_loss: 1703.11 .. NELBO: 1706.23\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1714.03 .. NELBO: 1716.32\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1715.7 .. NELBO: 1718.36\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.2 .. Rec_loss: 1703.07 .. NELBO: 1706.27\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.31 .. Rec_loss: 1714.05 .. NELBO: 1716.36\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1715.61 .. NELBO: 1718.27\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.27 .. Rec_loss: 1702.8 .. NELBO: 1706.07\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.35 .. Rec_loss: 1714.01 .. NELBO: 1716.36\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1715.54 .. NELBO: 1718.17\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1703.91 .. NELBO: 1707.33\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 1714.66 .. NELBO: 1717.13\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1716.18 .. NELBO: 1718.88\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.4 .. Rec_loss: 1703.93 .. NELBO: 1707.33\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.52 .. Rec_loss: 1714.61 .. NELBO: 1717.13\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1716.45 .. NELBO: 1719.09\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.6 .. Rec_loss: 1703.4 .. NELBO: 1707.0\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1714.04 .. NELBO: 1716.66\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1715.89 .. NELBO: 1718.65\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.48 .. Rec_loss: 1702.75 .. NELBO: 1706.23\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1713.21 .. NELBO: 1715.9\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1715.02 .. NELBO: 1717.88\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.68 .. Rec_loss: 1701.46 .. NELBO: 1705.14\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1712.27 .. NELBO: 1715.05\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1714.04 .. NELBO: 1717.05\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.68 .. Rec_loss: 1701.33 .. NELBO: 1705.01\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.8 .. Rec_loss: 1712.07 .. NELBO: 1714.87\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 3.07 .. Rec_loss: 1713.77 .. NELBO: 1716.84\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.55 .. Rec_loss: 1700.74 .. NELBO: 1704.29\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1711.88 .. NELBO: 1714.58\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1713.58 .. NELBO: 1716.6\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.37 .. Rec_loss: 1701.12 .. NELBO: 1704.49\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.55 .. Rec_loss: 1712.2 .. NELBO: 1714.75\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1713.91 .. NELBO: 1716.78\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.21 .. Rec_loss: 1701.63 .. NELBO: 1704.84\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1712.59 .. NELBO: 1714.95\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1714.21 .. NELBO: 1716.96\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.09 .. Rec_loss: 1703.0 .. NELBO: 1706.09\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1713.84 .. NELBO: 1716.13\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1715.22 .. NELBO: 1717.9\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1702.49 .. NELBO: 1705.46\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 1713.65 .. NELBO: 1715.98\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 2.55 .. Rec_loss: 1715.15 .. NELBO: 1717.7\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.2 .. Rec_loss: 1703.52 .. NELBO: 1706.72\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.52 .. Rec_loss: 1714.53 .. NELBO: 1717.05\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1716.29 .. NELBO: 1718.9\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.23 .. Rec_loss: 1702.51 .. NELBO: 1705.74\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1713.51 .. NELBO: 1716.11\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1715.4 .. NELBO: 1718.14\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.29 .. Rec_loss: 1701.28 .. NELBO: 1704.57\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1712.26 .. NELBO: 1714.95\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1714.2 .. NELBO: 1717.07\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.46 .. Rec_loss: 1700.9 .. NELBO: 1704.36\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1711.82 .. NELBO: 1714.58\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1713.59 .. NELBO: 1716.59\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.42 .. Rec_loss: 1700.53 .. NELBO: 1703.95\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1711.45 .. NELBO: 1714.16\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1712.99 .. NELBO: 1716.09\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.17 .. Rec_loss: 1700.74 .. NELBO: 1703.91\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1711.55 .. NELBO: 1714.13\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1713.1 .. NELBO: 1716.01\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.3 .. Rec_loss: 1700.36 .. NELBO: 1703.66\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1711.34 .. NELBO: 1714.03\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 3.0 .. Rec_loss: 1712.93 .. NELBO: 1715.93\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.26 .. Rec_loss: 1700.48 .. NELBO: 1703.74\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1711.55 .. NELBO: 1714.19\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 1713.03 .. NELBO: 1716.04\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.18 .. Rec_loss: 1700.22 .. NELBO: 1703.4\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1711.57 .. NELBO: 1714.18\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1713.04 .. NELBO: 1715.93\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.18 .. Rec_loss: 1700.63 .. NELBO: 1703.81\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1711.88 .. NELBO: 1714.56\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 1713.31 .. NELBO: 1716.24\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.18 .. Rec_loss: 1700.72 .. NELBO: 1703.9\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1712.12 .. NELBO: 1714.79\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1713.47 .. NELBO: 1716.38\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1700.03 .. NELBO: 1703.18\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1711.67 .. NELBO: 1714.35\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1713.08 .. NELBO: 1715.94\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1700.72 .. NELBO: 1703.82\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1712.02 .. NELBO: 1714.72\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1713.46 .. NELBO: 1716.33\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.98 .. Rec_loss: 1701.09 .. NELBO: 1704.07\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1712.09 .. NELBO: 1714.77\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 2.96 .. Rec_loss: 1713.47 .. NELBO: 1716.43\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1701.11 .. NELBO: 1703.96\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.4 .. Rec_loss: 1712.08 .. NELBO: 1714.48\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1713.36 .. NELBO: 1716.24\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1700.86 .. NELBO: 1703.83\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1711.56 .. NELBO: 1714.16\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1713.05 .. NELBO: 1715.92\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.43 .. Rec_loss: 1700.42 .. NELBO: 1703.85\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1711.18 .. NELBO: 1714.07\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1712.69 .. NELBO: 1715.84\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:33:52,459] Trial 48 finished with values: [-0.00402310855691646, 0.11315789473684211] and parameters: {'num_topics': 38, 'dropout': 0.25320981797696485, 't_hidden_size': 300, 'activation': 'relu'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.447096182596987, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=13, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=13, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=13, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 1993.05 .. NELBO: 1993.17\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.28 .. Rec_loss: 1879.34 .. NELBO: 1879.62\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 1867.82 .. NELBO: 1868.14\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1723.1 .. NELBO: 1723.15\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1730.93 .. NELBO: 1730.97\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1734.22 .. NELBO: 1734.26\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1719.73 .. NELBO: 1719.75\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 1727.94 .. NELBO: 1727.96\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1731.14 .. NELBO: 1731.15\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1718.4 .. NELBO: 1718.41\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1726.6 .. NELBO: 1726.61\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 1729.75 .. NELBO: 1729.76\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 1717.53 .. NELBO: 1717.59\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1725.67 .. NELBO: 1725.75\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1728.79 .. NELBO: 1728.88\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.42 .. Rec_loss: 1716.11 .. NELBO: 1716.53\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.44 .. Rec_loss: 1724.23 .. NELBO: 1724.67\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.46 .. Rec_loss: 1727.27 .. NELBO: 1727.73\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.73 .. Rec_loss: 1714.99 .. NELBO: 1715.72\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.65 .. Rec_loss: 1723.35 .. NELBO: 1724.0\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.66 .. Rec_loss: 1726.41 .. NELBO: 1727.07\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.89 .. Rec_loss: 1714.12 .. NELBO: 1715.01\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.75 .. Rec_loss: 1722.62 .. NELBO: 1723.37\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 1725.65 .. NELBO: 1726.42\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.02 .. Rec_loss: 1713.49 .. NELBO: 1714.51\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.94 .. Rec_loss: 1721.97 .. NELBO: 1722.91\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.97 .. Rec_loss: 1724.86 .. NELBO: 1725.83\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.46 .. Rec_loss: 1711.81 .. NELBO: 1713.27\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.39 .. Rec_loss: 1720.54 .. NELBO: 1721.93\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 1.43 .. Rec_loss: 1723.26 .. NELBO: 1724.69\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.89 .. Rec_loss: 1710.07 .. NELBO: 1711.96\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.7 .. Rec_loss: 1719.07 .. NELBO: 1720.77\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.79 .. Rec_loss: 1721.65 .. NELBO: 1723.44\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.12 .. Rec_loss: 1708.49 .. NELBO: 1710.61\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.88 .. Rec_loss: 1717.7 .. NELBO: 1719.58\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 1.99 .. Rec_loss: 1720.19 .. NELBO: 1722.18\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 1707.12 .. NELBO: 1709.46\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.06 .. Rec_loss: 1716.49 .. NELBO: 1718.55\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.2 .. Rec_loss: 1718.84 .. NELBO: 1721.04\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.57 .. Rec_loss: 1706.38 .. NELBO: 1708.95\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.11 .. Rec_loss: 1716.07 .. NELBO: 1718.18\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.28 .. Rec_loss: 1718.1 .. NELBO: 1720.38\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1705.27 .. NELBO: 1707.96\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.19 .. Rec_loss: 1715.3 .. NELBO: 1717.49\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 1717.34 .. NELBO: 1719.67\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1704.15 .. NELBO: 1706.89\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.24 .. Rec_loss: 1714.51 .. NELBO: 1716.75\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.33 .. Rec_loss: 1716.72 .. NELBO: 1719.05\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1704.14 .. NELBO: 1706.9\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.37 .. Rec_loss: 1714.34 .. NELBO: 1716.71\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.47 .. Rec_loss: 1716.42 .. NELBO: 1718.89\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.64 .. Rec_loss: 1703.31 .. NELBO: 1705.95\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.45 .. Rec_loss: 1713.65 .. NELBO: 1716.1\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1715.72 .. NELBO: 1718.26\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.52 .. Rec_loss: 1703.46 .. NELBO: 1705.98\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.55 .. Rec_loss: 1713.67 .. NELBO: 1716.22\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1715.93 .. NELBO: 1718.53\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1703.22 .. NELBO: 1705.51\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.45 .. Rec_loss: 1713.42 .. NELBO: 1715.87\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1715.51 .. NELBO: 1718.02\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.36 .. Rec_loss: 1702.49 .. NELBO: 1704.85\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1712.79 .. NELBO: 1715.3\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1714.67 .. NELBO: 1717.35\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.38 .. Rec_loss: 1702.55 .. NELBO: 1704.93\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 1712.85 .. NELBO: 1715.28\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1714.5 .. NELBO: 1717.23\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.44 .. Rec_loss: 1701.9 .. NELBO: 1704.34\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 1712.47 .. NELBO: 1714.9\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1714.47 .. NELBO: 1717.07\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.42 .. Rec_loss: 1701.76 .. NELBO: 1704.18\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.39 .. Rec_loss: 1712.21 .. NELBO: 1714.6\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.62 .. Rec_loss: 1714.17 .. NELBO: 1716.79\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.51 .. Rec_loss: 1701.34 .. NELBO: 1703.85\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.48 .. Rec_loss: 1711.87 .. NELBO: 1714.35\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1713.81 .. NELBO: 1716.52\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1701.22 .. NELBO: 1703.8\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1711.72 .. NELBO: 1714.25\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1713.6 .. NELBO: 1716.33\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.55 .. Rec_loss: 1701.04 .. NELBO: 1703.59\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1711.46 .. NELBO: 1714.02\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 2.81 .. Rec_loss: 1713.29 .. NELBO: 1716.1\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 1700.89 .. NELBO: 1703.47\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1711.4 .. NELBO: 1713.93\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1713.36 .. NELBO: 1716.06\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1700.21 .. NELBO: 1702.94\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.67 .. Rec_loss: 1710.96 .. NELBO: 1713.63\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1712.95 .. NELBO: 1715.82\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1699.9 .. NELBO: 1702.63\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1710.75 .. NELBO: 1713.38\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1712.86 .. NELBO: 1715.69\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1700.98 .. NELBO: 1703.68\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.59 .. Rec_loss: 1711.34 .. NELBO: 1713.93\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1713.22 .. NELBO: 1716.05\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1701.2 .. NELBO: 1703.76\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.52 .. Rec_loss: 1711.21 .. NELBO: 1713.73\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 2.76 .. Rec_loss: 1713.18 .. NELBO: 1715.94\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.69 .. Rec_loss: 1699.87 .. NELBO: 1702.56\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.56 .. Rec_loss: 1710.65 .. NELBO: 1713.21\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1712.68 .. NELBO: 1715.47\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.79 .. Rec_loss: 1700.48 .. NELBO: 1703.27\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1710.63 .. NELBO: 1713.29\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1712.48 .. NELBO: 1715.38\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1699.95 .. NELBO: 1702.66\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.63 .. Rec_loss: 1710.49 .. NELBO: 1713.12\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1712.49 .. NELBO: 1715.4\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1700.28 .. NELBO: 1702.94\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.6 .. Rec_loss: 1710.8 .. NELBO: 1713.4\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 2.85 .. Rec_loss: 1712.75 .. NELBO: 1715.6\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.87 .. Rec_loss: 1699.73 .. NELBO: 1702.6\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.75 .. Rec_loss: 1710.33 .. NELBO: 1713.08\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1712.41 .. NELBO: 1715.31\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1699.79 .. NELBO: 1702.51\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.65 .. Rec_loss: 1710.21 .. NELBO: 1712.86\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1712.3 .. NELBO: 1715.22\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1699.77 .. NELBO: 1702.6\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.71 .. Rec_loss: 1710.4 .. NELBO: 1713.11\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1712.54 .. NELBO: 1715.49\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.8 .. Rec_loss: 1699.91 .. NELBO: 1702.71\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.74 .. Rec_loss: 1710.27 .. NELBO: 1713.01\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1712.18 .. NELBO: 1715.17\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.78 .. Rec_loss: 1699.7 .. NELBO: 1702.48\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.7 .. Rec_loss: 1710.19 .. NELBO: 1712.89\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1712.18 .. NELBO: 1715.1\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1699.76 .. NELBO: 1702.65\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1709.87 .. NELBO: 1712.76\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 3.14 .. Rec_loss: 1711.7 .. NELBO: 1714.84\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1698.86 .. NELBO: 1701.74\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.8 .. Rec_loss: 1709.48 .. NELBO: 1712.28\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 1711.28 .. NELBO: 1714.3\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.97 .. Rec_loss: 1698.35 .. NELBO: 1701.32\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1709.02 .. NELBO: 1711.91\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1711.07 .. NELBO: 1714.2\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1698.03 .. NELBO: 1701.08\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1709.0 .. NELBO: 1711.9\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 3.28 .. Rec_loss: 1710.72 .. NELBO: 1714.0\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.81 .. Rec_loss: 1698.78 .. NELBO: 1701.59\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.83 .. Rec_loss: 1709.3 .. NELBO: 1712.13\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 3.05 .. Rec_loss: 1711.17 .. NELBO: 1714.22\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.99 .. Rec_loss: 1697.81 .. NELBO: 1700.8\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1708.78 .. NELBO: 1711.66\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1710.72 .. NELBO: 1713.85\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1697.98 .. NELBO: 1700.89\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.9 .. Rec_loss: 1708.76 .. NELBO: 1711.66\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1710.72 .. NELBO: 1713.85\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1698.19 .. NELBO: 1701.11\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.88 .. Rec_loss: 1708.96 .. NELBO: 1711.84\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 3.13 .. Rec_loss: 1710.92 .. NELBO: 1714.05\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.92 .. Rec_loss: 1697.82 .. NELBO: 1700.74\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.91 .. Rec_loss: 1708.85 .. NELBO: 1711.76\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 3.18 .. Rec_loss: 1710.67 .. NELBO: 1713.85\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 17:35:06,444] Trial 49 finished with values: [-0.009427572215897437, 0.2076923076923077] and parameters: {'num_topics': 13, 'dropout': 0.447096182596987, 't_hidden_size': 300, 'activation': 'relu'}.\n"
     ]
    }
   ],
   "source": [
    "def objectiveETM(trial) -> Tuple[float, float]:\n",
    "\n",
    "    # Define hyperparameters to optimize\n",
    "    num_topics = trial.suggest_int(\"num_topics\", 10, 50)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 0.60)\n",
    "    t_hidden_size = trial.suggest_categorical(\"t_hidden_size\", [50, 100, 200, 300])\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"softplus\", \"relu\", \"sigmoid\"])\n",
    "\n",
    "    # Train ETM model\n",
    "    model = ETM(\n",
    "        num_topics=num_topics,\n",
    "        dropout = dropout,\n",
    "        t_hidden_size=t_hidden_size,\n",
    "        activation=activation,\n",
    "        device = 'cuda',\n",
    "        embeddings_path='./data/chilit-19th-century-averaged-embeddings.txt',\n",
    "        use_partitions=False,\n",
    "        num_epochs = 50\n",
    "    )\n",
    "\n",
    "    output = model.train_model(dataset)\n",
    "\n",
    "    # Compute coherence score (can also use perplexity, but coherence is often better)\n",
    "    coherence_metrics = Coherence(texts=dataset.get_corpus(), #list of our documents\n",
    "                    measure='c_npmi')\n",
    "    coherence = coherence_metrics.score(output)\n",
    "\n",
    "    diverisity_metric = TopicDiversity(topk=10) # Initialize metric\n",
    "    diversity = diverisity_metric.score(output)\n",
    "\n",
    "    return coherence, diversity  # Optuna will maximize these\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(\n",
    "    directions=[\"maximize\",\"maximize\"],\n",
    "    storage=f\"sqlite:///{octis_folder}ETM_Study.db\",\n",
    "    study_name=\"ETM_Study\"\n",
    "  )\n",
    "study.optimize(objectiveETM, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5UVCSZFVTqQL",
   "metadata": {
    "id": "5UVCSZFVTqQL"
   },
   "outputs": [],
   "source": [
    "def train_final_ETM_model(params):\n",
    "    \"\"\"Train final LDA model with selected parameters\"\"\"\n",
    "    print(f\"\\nTraining final model with parameters: {params}\")\n",
    "\n",
    "    model = ETM(\n",
    "        num_topics=params['num_topics'],\n",
    "        dropout = params['dropout'],\n",
    "        t_hidden_size=params['t_hidden_size'],\n",
    "        activation=params['activation'],\n",
    "        device = 'cuda',\n",
    "        embeddings_path='./data/chilit-19th-century-averaged-embeddings.txt',\n",
    "        use_partitions=False,\n",
    "        num_epochs = 50\n",
    "    )\n",
    "\n",
    "    output = model.train_model(dataset)\n",
    "\n",
    "    # Calculate final metrics\n",
    "    coherence_metrics = Coherence(texts=dataset.get_corpus(), #list of our documents\n",
    "                    measure='c_npmi')\n",
    "    coherence = coherence_metrics.score(output)\n",
    "\n",
    "    diverisity_metric = TopicDiversity(topk=10) # Initialize metric\n",
    "    diversity = diverisity_metric.score(output)\n",
    "\n",
    "    print(f\"Final model metrics:\")\n",
    "    print(f\"  Coherence: {coherence:.4f}\")\n",
    "    print(f\"  Diversity: {diversity:.4f}\")\n",
    "\n",
    "    return model, output, coherence, diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6e33dc72-5697-4f0c-9cd0-19d4ccd034fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETM_study = optuna.load_study(\n",
    "    storage=f\"sqlite:///{optuna_folder}ETM_Study.db\",\n",
    "    study_name=\"ETM_Study\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ubvrVjITUIq7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78768,
     "status": "ok",
     "timestamp": 1755540066902,
     "user": {
      "displayName": "NLP Student",
      "userId": "16153972373005500160"
     },
     "user_tz": -120
    },
    "id": "ubvrVjITUIq7",
    "outputId": "74f90b71-cf93-44de-9da5-774f59aa6c71",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model with parameters: {'num_topics': 11, 'dropout': 0.2769868853252398, 't_hidden_size': 300, 'activation': 'relu'}\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.2769868853252398, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=23743, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=23743, out_features=300, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=300, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=300, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 1990.77 .. NELBO: 1991.54\n",
      "Epoch: 1 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.82 .. Rec_loss: 1876.91 .. NELBO: 1877.73\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.8 .. Rec_loss: 1865.67 .. NELBO: 1866.47\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 1722.76 .. NELBO: 1722.8\n",
      "Epoch: 2 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1730.65 .. NELBO: 1730.7\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 1734.0 .. NELBO: 1734.05\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.11 .. Rec_loss: 1719.94 .. NELBO: 1720.05\n",
      "Epoch: 3 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1728.07 .. NELBO: 1728.17\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1731.26 .. NELBO: 1731.36\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 1718.49 .. NELBO: 1718.57\n",
      "Epoch: 4 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 1726.63 .. NELBO: 1726.72\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.1 .. Rec_loss: 1729.77 .. NELBO: 1729.87\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.14 .. Rec_loss: 1717.52 .. NELBO: 1717.66\n",
      "Epoch: 5 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.16 .. Rec_loss: 1725.68 .. NELBO: 1725.84\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.17 .. Rec_loss: 1728.76 .. NELBO: 1728.93\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.24 .. Rec_loss: 1716.58 .. NELBO: 1716.82\n",
      "Epoch: 6 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.27 .. Rec_loss: 1724.67 .. NELBO: 1724.94\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.32 .. Rec_loss: 1727.73 .. NELBO: 1728.05\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.54 .. Rec_loss: 1715.15 .. NELBO: 1715.69\n",
      "Epoch: 7 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 1723.27 .. NELBO: 1723.86\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.64 .. Rec_loss: 1726.33 .. NELBO: 1726.97\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 0.82 .. Rec_loss: 1713.88 .. NELBO: 1714.7\n",
      "Epoch: 8 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 0.81 .. Rec_loss: 1722.22 .. NELBO: 1723.03\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.87 .. Rec_loss: 1725.17 .. NELBO: 1726.04\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 1.52 .. Rec_loss: 1711.75 .. NELBO: 1713.27\n",
      "Epoch: 9 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.3 .. Rec_loss: 1720.65 .. NELBO: 1721.95\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 1.44 .. Rec_loss: 1723.06 .. NELBO: 1724.5\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.16 .. Rec_loss: 1708.56 .. NELBO: 1710.72\n",
      "Epoch: 10 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.53 .. Rec_loss: 1719.29 .. NELBO: 1720.82\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 1.71 .. Rec_loss: 1721.25 .. NELBO: 1722.96\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.3 .. Rec_loss: 1706.41 .. NELBO: 1708.71\n",
      "Epoch: 11 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.65 .. Rec_loss: 1717.31 .. NELBO: 1718.96\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1.83 .. Rec_loss: 1718.97 .. NELBO: 1720.8\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.53 .. Rec_loss: 1705.02 .. NELBO: 1707.55\n",
      "Epoch: 12 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.82 .. Rec_loss: 1716.07 .. NELBO: 1717.89\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 2.0 .. Rec_loss: 1717.63 .. NELBO: 1719.63\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.52 .. Rec_loss: 1703.87 .. NELBO: 1706.39\n",
      "Epoch: 13 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.88 .. Rec_loss: 1715.04 .. NELBO: 1716.92\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2.09 .. Rec_loss: 1716.53 .. NELBO: 1718.62\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1702.85 .. NELBO: 1705.39\n",
      "Epoch: 14 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 1.96 .. Rec_loss: 1714.07 .. NELBO: 1716.03\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 2.18 .. Rec_loss: 1715.56 .. NELBO: 1717.74\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.66 .. Rec_loss: 1701.8 .. NELBO: 1704.46\n",
      "Epoch: 15 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.11 .. Rec_loss: 1712.86 .. NELBO: 1714.97\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 2.34 .. Rec_loss: 1714.32 .. NELBO: 1716.66\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.72 .. Rec_loss: 1701.01 .. NELBO: 1703.73\n",
      "Epoch: 16 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.29 .. Rec_loss: 1711.99 .. NELBO: 1714.28\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 2.46 .. Rec_loss: 1713.55 .. NELBO: 1716.01\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.86 .. Rec_loss: 1700.28 .. NELBO: 1703.14\n",
      "Epoch: 17 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.43 .. Rec_loss: 1711.13 .. NELBO: 1713.56\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2.57 .. Rec_loss: 1712.73 .. NELBO: 1715.3\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1699.75 .. NELBO: 1702.7\n",
      "Epoch: 18 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.54 .. Rec_loss: 1710.38 .. NELBO: 1712.92\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 2.68 .. Rec_loss: 1712.01 .. NELBO: 1714.69\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 2.95 .. Rec_loss: 1699.22 .. NELBO: 1702.17\n",
      "Epoch: 19 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.61 .. Rec_loss: 1709.77 .. NELBO: 1712.38\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2.73 .. Rec_loss: 1711.39 .. NELBO: 1714.12\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.15 .. Rec_loss: 1698.34 .. NELBO: 1701.49\n",
      "Epoch: 20 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.77 .. Rec_loss: 1709.1 .. NELBO: 1711.87\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 2.94 .. Rec_loss: 1710.59 .. NELBO: 1713.53\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.25 .. Rec_loss: 1697.34 .. NELBO: 1700.59\n",
      "Epoch: 21 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 2.89 .. Rec_loss: 1708.31 .. NELBO: 1711.2\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1709.73 .. NELBO: 1712.83\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.58 .. Rec_loss: 1695.98 .. NELBO: 1699.56\n",
      "Epoch: 22 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.1 .. Rec_loss: 1707.11 .. NELBO: 1710.21\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 3.26 .. Rec_loss: 1708.44 .. NELBO: 1711.7\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 3.93 .. Rec_loss: 1694.11 .. NELBO: 1698.04\n",
      "Epoch: 23 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.39 .. Rec_loss: 1705.83 .. NELBO: 1709.22\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 3.61 .. Rec_loss: 1707.06 .. NELBO: 1710.67\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.35 .. Rec_loss: 1692.36 .. NELBO: 1696.71\n",
      "Epoch: 24 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.71 .. Rec_loss: 1704.24 .. NELBO: 1707.95\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 3.89 .. Rec_loss: 1705.5 .. NELBO: 1709.39\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.32 .. Rec_loss: 1692.95 .. NELBO: 1697.27\n",
      "Epoch: 25 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 3.77 .. Rec_loss: 1705.32 .. NELBO: 1709.09\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 3.89 .. Rec_loss: 1706.86 .. NELBO: 1710.75\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.51 .. Rec_loss: 1693.79 .. NELBO: 1698.3\n",
      "Epoch: 26 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.09 .. Rec_loss: 1705.27 .. NELBO: 1709.36\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 4.13 .. Rec_loss: 1706.87 .. NELBO: 1711.0\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.51 .. Rec_loss: 1691.71 .. NELBO: 1696.22\n",
      "Epoch: 27 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.23 .. Rec_loss: 1703.11 .. NELBO: 1707.34\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 4.43 .. Rec_loss: 1704.58 .. NELBO: 1709.01\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.41 .. Rec_loss: 1690.02 .. NELBO: 1694.43\n",
      "Epoch: 28 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.29 .. Rec_loss: 1701.48 .. NELBO: 1705.77\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 4.52 .. Rec_loss: 1702.99 .. NELBO: 1707.51\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.18 .. Rec_loss: 1689.91 .. NELBO: 1694.09\n",
      "Epoch: 29 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.14 .. Rec_loss: 1701.34 .. NELBO: 1705.48\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 4.48 .. Rec_loss: 1702.53 .. NELBO: 1707.01\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.55 .. Rec_loss: 1689.17 .. NELBO: 1693.72\n",
      "Epoch: 30 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.45 .. Rec_loss: 1700.15 .. NELBO: 1704.6\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 4.68 .. Rec_loss: 1701.38 .. NELBO: 1706.06\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1687.59 .. NELBO: 1692.43\n",
      "Epoch: 31 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.65 .. Rec_loss: 1699.25 .. NELBO: 1703.9\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 4.76 .. Rec_loss: 1700.88 .. NELBO: 1705.64\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 1689.2 .. NELBO: 1694.35\n",
      "Epoch: 32 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.91 .. Rec_loss: 1699.79 .. NELBO: 1704.7\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1701.25 .. NELBO: 1706.28\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1687.53 .. NELBO: 1692.59\n",
      "Epoch: 33 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1698.47 .. NELBO: 1703.57\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1700.0 .. NELBO: 1705.18\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.04 .. Rec_loss: 1686.53 .. NELBO: 1691.57\n",
      "Epoch: 34 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.0 .. Rec_loss: 1698.45 .. NELBO: 1703.45\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 5.1 .. Rec_loss: 1699.96 .. NELBO: 1705.06\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.08 .. Rec_loss: 1686.59 .. NELBO: 1691.67\n",
      "Epoch: 35 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1698.84 .. NELBO: 1703.9\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1700.37 .. NELBO: 1705.6\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.06 .. Rec_loss: 1687.35 .. NELBO: 1692.41\n",
      "Epoch: 36 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.84 .. Rec_loss: 1699.3 .. NELBO: 1704.14\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 5.02 .. Rec_loss: 1701.21 .. NELBO: 1706.23\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1687.64 .. NELBO: 1693.08\n",
      "Epoch: 37 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 4.87 .. Rec_loss: 1699.98 .. NELBO: 1704.85\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1701.74 .. NELBO: 1706.77\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.83 .. Rec_loss: 1685.19 .. NELBO: 1691.02\n",
      "Epoch: 38 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.23 .. Rec_loss: 1697.66 .. NELBO: 1702.89\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 5.41 .. Rec_loss: 1699.61 .. NELBO: 1705.02\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.37 .. Rec_loss: 1684.55 .. NELBO: 1689.92\n",
      "Epoch: 39 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.03 .. Rec_loss: 1696.9 .. NELBO: 1701.93\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 5.27 .. Rec_loss: 1698.4 .. NELBO: 1703.67\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.7 .. Rec_loss: 1683.46 .. NELBO: 1689.16\n",
      "Epoch: 40 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.24 .. Rec_loss: 1695.65 .. NELBO: 1700.89\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1697.33 .. NELBO: 1702.77\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.63 .. Rec_loss: 1682.51 .. NELBO: 1688.14\n",
      "Epoch: 41 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.18 .. Rec_loss: 1694.66 .. NELBO: 1699.84\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 5.38 .. Rec_loss: 1696.15 .. NELBO: 1701.53\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.66 .. Rec_loss: 1681.43 .. NELBO: 1687.09\n",
      "Epoch: 42 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.3 .. Rec_loss: 1693.52 .. NELBO: 1698.82\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 5.49 .. Rec_loss: 1695.07 .. NELBO: 1700.56\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1681.0 .. NELBO: 1686.84\n",
      "Epoch: 43 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.44 .. Rec_loss: 1692.87 .. NELBO: 1698.31\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 5.63 .. Rec_loss: 1694.42 .. NELBO: 1700.05\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.84 .. Rec_loss: 1680.21 .. NELBO: 1686.05\n",
      "Epoch: 44 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.43 .. Rec_loss: 1692.25 .. NELBO: 1697.68\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 5.58 .. Rec_loss: 1693.84 .. NELBO: 1699.42\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.03 .. Rec_loss: 1679.28 .. NELBO: 1685.31\n",
      "Epoch: 45 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.61 .. Rec_loss: 1691.4 .. NELBO: 1697.01\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1693.05 .. NELBO: 1698.77\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.98 .. Rec_loss: 1678.96 .. NELBO: 1684.94\n",
      "Epoch: 46 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.64 .. Rec_loss: 1690.98 .. NELBO: 1696.62\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1692.58 .. NELBO: 1698.38\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 5.94 .. Rec_loss: 1678.6 .. NELBO: 1684.54\n",
      "Epoch: 47 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.65 .. Rec_loss: 1690.53 .. NELBO: 1696.18\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 5.8 .. Rec_loss: 1692.13 .. NELBO: 1697.93\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.02 .. Rec_loss: 1678.11 .. NELBO: 1684.13\n",
      "Epoch: 48 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.72 .. Rec_loss: 1690.11 .. NELBO: 1695.83\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 5.88 .. Rec_loss: 1691.72 .. NELBO: 1697.6\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.04 .. Rec_loss: 1677.93 .. NELBO: 1683.97\n",
      "Epoch: 49 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.77 .. Rec_loss: 1689.7 .. NELBO: 1695.47\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 5.92 .. Rec_loss: 1691.31 .. NELBO: 1697.23\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/46 .. LR: 0.005 .. KL_theta: 6.04 .. Rec_loss: 1677.12 .. NELBO: 1683.16\n",
      "Epoch: 50 .. batch: 40/46 .. LR: 0.005 .. KL_theta: 5.85 .. Rec_loss: 1689.06 .. NELBO: 1694.91\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 6.01 .. Rec_loss: 1690.7 .. NELBO: 1696.71\n",
      "****************************************************************************************************\n",
      "Final model metrics:\n",
      "  Coherence: 0.0036\n",
      "  Diversity: 0.4273\n",
      "\n",
      "Final model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "if ETM_study.best_trials:\n",
    "    # Get balanced solution\n",
    "    pareto_trials = ETM_study.best_trials\n",
    "\n",
    "    # Pick the first Pareto optimal solution\n",
    "    selected_params = pareto_trials[0].params\n",
    "    final_model, final_output, final_coherence, final_diversity = train_final_ETM_model(selected_params)\n",
    "\n",
    "    print(f\"\\nFinal model trained successfully!\")\n",
    "\n",
    "    pickle.dump(final_output, open(optuna_folder + \"Optuna_ETM_output.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bHsxhfZAbxal",
   "metadata": {
    "id": "bHsxhfZAbxal"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1kiFnOKOVfAI",
   "metadata": {
    "id": "1kiFnOKOVfAI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "417aca10-8f69-43a6-8cdc-3b923cbc3ed8",
    "cSQ_yBKW2Eru",
    "cDSoA2fq2l6E"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
